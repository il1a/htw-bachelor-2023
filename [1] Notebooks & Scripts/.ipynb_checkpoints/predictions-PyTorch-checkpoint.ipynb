{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d27257b93be61c17",
   "metadata": {},
   "source": [
    "## Bachelor Thesis\n",
    "## \"Exploring Various Classification Techniques In Detection of Disinformation.\"\n",
    "Ilia Sokolovskiy\n",
    "HTW SS23\n",
    "\n",
    "Notebook 3/5 - LSTM & Bi-LSTM Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee2103461e11090",
   "metadata": {},
   "source": [
    "**Installing all necessary dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109df8850fb4855c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install numpy pandas matplotlib torch scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957f454062423f96",
   "metadata": {},
   "source": [
    "**Importing all necessary libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d83df6bb1d3660fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-16T02:04:19.906763700Z",
     "start_time": "2023-08-16T02:04:16.975406700Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "from utils import split_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d9562539afcba9",
   "metadata": {},
   "source": [
    "**Loading the prepared data frame from a pickle**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbbd30f24a5d7d6b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-16T02:04:22.725646300Z",
     "start_time": "2023-08-16T02:04:21.888285Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the pickle with the df\n",
    "base_dir = \"Data\"\n",
    "pickle_folder = \"Pickles\"\n",
    "filename_pickle = \"pickle_lg_df_2\"\n",
    "\n",
    "full_path_pickle = os.path.join(base_dir, pickle_folder, filename_pickle)\n",
    "\n",
    "df = pd.read_pickle(full_path_pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "630129f2c059beaf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-16T02:04:23.447329300Z",
     "start_time": "2023-08-16T02:04:23.382298900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>label_encoded</th>\n",
       "      <th>norp_count</th>\n",
       "      <th>gpe_count</th>\n",
       "      <th>vader_compound</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>original_text_vector</th>\n",
       "      <th>cleaned_text_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Donald Trump just couldn t wish all Americans ...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.8681</td>\n",
       "      <td>donald trump couldn t wish americans happy new...</td>\n",
       "      <td>[-1.6619356, -0.0073223817, -1.6303111, -0.190...</td>\n",
       "      <td>[-0.17023614, 1.1278214, -2.2035916, -1.195557...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>House Intelligence Committee Chairman Devin Nu...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.7141</td>\n",
       "      <td>house intelligence committee chairman devin nu...</td>\n",
       "      <td>[-2.008067, 0.6831929, -1.9811207, 0.52264357,...</td>\n",
       "      <td>[-0.3486856, 0.6266792, -1.7451725, 0.01966631...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>On Friday, it was revealed that former Milwauk...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.9953</td>\n",
       "      <td>friday reveal milwaukee sheriff david clarke c...</td>\n",
       "      <td>[-1.9425699, 0.0044210483, -1.7258451, 0.00323...</td>\n",
       "      <td>[-0.34773135, 0.7257386, -1.7822778, 0.2710289...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>On Christmas day, Donald Trump announced that ...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.9176</td>\n",
       "      <td>christmas day donald trump announce work follo...</td>\n",
       "      <td>[-1.6670086, 0.23368433, -0.6346163, 0.1001595...</td>\n",
       "      <td>[-0.18105617, 0.730818, -0.28500575, -0.608257...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pope Francis used his annual Christmas Day mes...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.3134</td>\n",
       "      <td>pope francis annual christmas day message rebu...</td>\n",
       "      <td>[-2.141846, 1.1239394, -2.4791837, 0.000615673...</td>\n",
       "      <td>[-0.003997393, 1.3095359, -2.2471106, -0.14267...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text label  label_encoded  \\\n",
       "0  Donald Trump just couldn t wish all Americans ...  FAKE              0   \n",
       "1  House Intelligence Committee Chairman Devin Nu...  FAKE              0   \n",
       "2  On Friday, it was revealed that former Milwauk...  FAKE              0   \n",
       "3  On Christmas day, Donald Trump announced that ...  FAKE              0   \n",
       "4  Pope Francis used his annual Christmas Day mes...  FAKE              0   \n",
       "\n",
       "   norp_count  gpe_count  vader_compound  \\\n",
       "0           3          3         -0.8681   \n",
       "1          10          5         -0.7141   \n",
       "2           1          4         -0.9953   \n",
       "3           0          2         -0.9176   \n",
       "4           2          5          0.3134   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  donald trump couldn t wish americans happy new...   \n",
       "1  house intelligence committee chairman devin nu...   \n",
       "2  friday reveal milwaukee sheriff david clarke c...   \n",
       "3  christmas day donald trump announce work follo...   \n",
       "4  pope francis annual christmas day message rebu...   \n",
       "\n",
       "                                original_text_vector  \\\n",
       "0  [-1.6619356, -0.0073223817, -1.6303111, -0.190...   \n",
       "1  [-2.008067, 0.6831929, -1.9811207, 0.52264357,...   \n",
       "2  [-1.9425699, 0.0044210483, -1.7258451, 0.00323...   \n",
       "3  [-1.6670086, 0.23368433, -0.6346163, 0.1001595...   \n",
       "4  [-2.141846, 1.1239394, -2.4791837, 0.000615673...   \n",
       "\n",
       "                                 cleaned_text_vector  \n",
       "0  [-0.17023614, 1.1278214, -2.2035916, -1.195557...  \n",
       "1  [-0.3486856, 0.6266792, -1.7451725, 0.01966631...  \n",
       "2  [-0.34773135, 0.7257386, -1.7822778, 0.2710289...  \n",
       "3  [-0.18105617, 0.730818, -0.28500575, -0.608257...  \n",
       "4  [-0.003997393, 1.3095359, -2.2471106, -0.14267...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83c473a351da70fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-16T02:04:25.053523700Z",
     "start_time": "2023-08-16T02:04:24.996011100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 64429 entries, 0 to 78616\n",
      "Data columns (total 9 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   text                  64429 non-null  object \n",
      " 1   label                 64429 non-null  object \n",
      " 2   label_encoded         64429 non-null  int64  \n",
      " 3   norp_count            64429 non-null  int64  \n",
      " 4   gpe_count             64429 non-null  int64  \n",
      " 5   vader_compound        64429 non-null  float64\n",
      " 6   cleaned_text          64429 non-null  object \n",
      " 7   original_text_vector  64429 non-null  object \n",
      " 8   cleaned_text_vector   64429 non-null  object \n",
      "dtypes: float64(1), int64(3), object(5)\n",
      "memory usage: 4.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3162a8a284a0644",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-16T02:04:26.979125800Z",
     "start_time": "2023-08-16T02:04:26.942608900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original_text_vector:\n",
      "[array([-1.66193557e+00, -7.32238172e-03, -1.63031113e+00, -1.90727830e-01,\n",
      "         2.74318266e+00,  7.56940171e-02,  7.19059646e-01,  3.10060287e+00,\n",
      "         5.63645512e-02, -9.18159544e-01,  3.30311918e+00,  1.01318681e+00,\n",
      "        -2.37784243e+00,  1.09689605e+00,  1.00805140e+00,  9.86939549e-01,\n",
      "         7.12323546e-01, -2.09621027e-01, -1.42275417e+00, -1.22521126e+00,\n",
      "         7.70984590e-01,  4.12203610e-01, -8.28106701e-01, -1.18215770e-01,\n",
      "        -1.68121293e-01, -1.05733287e+00, -2.19249940e+00, -3.05256933e-01,\n",
      "         2.90210135e-02,  9.07058597e-01,  8.54909539e-01, -4.38155919e-01,\n",
      "        -4.96599197e-01, -9.69359338e-01, -1.63529062e+00, -9.06797349e-01,\n",
      "        -3.92651826e-01, -2.68683024e-02,  4.48841333e-01, -2.69704796e-02,\n",
      "         2.16357335e-01,  1.17041871e-01, -6.33049980e-02,  4.43491668e-01,\n",
      "        -1.35239458e+00,  1.27368534e+00, -1.11694761e-01, -1.91329718e+00,\n",
      "        -7.29508519e-01,  1.07032955e+00, -8.88784885e-01,  5.37899613e-01,\n",
      "         2.07946286e-01, -2.98304081e+00, -3.37569922e-01,  7.69427061e-01,\n",
      "        -6.55884087e-01,  6.96408153e-01,  4.68882352e-01, -3.43233436e-01,\n",
      "         6.25461578e-01, -6.98471606e-01,  1.34582356e-01, -7.81436324e-01,\n",
      "         2.37909937e+00,  9.47569728e-01, -1.73600817e+00, -2.20257139e+00,\n",
      "         3.31307143e-01,  9.49213862e-01, -1.08384110e-01, -1.25286388e+00,\n",
      "        -8.07931721e-01, -3.26256067e-01,  8.01019073e-02,  8.55659127e-01,\n",
      "        -2.65750647e+00,  1.38735867e+00, -1.64694989e+00,  9.09301192e-02,\n",
      "        -2.10737610e+00, -4.22937572e-01,  4.05212730e-01,  5.72933495e-01,\n",
      "         1.75564134e+00, -6.75142527e-01, -1.33461213e+00, -1.93335915e+00,\n",
      "         2.98271865e-01, -7.04673588e-01, -3.01556736e-01,  5.68346143e-01,\n",
      "         1.59147549e+00, -2.42703629e+00,  1.42860889e-01, -3.40178549e-01,\n",
      "         1.61494300e-01, -8.32069159e-01,  1.23493326e+00,  1.17538416e+00,\n",
      "         1.50829279e+00,  1.24361181e+00,  1.40914524e+00,  2.07115412e+00,\n",
      "        -7.60951579e-01,  3.00505304e+00, -1.73430562e-01, -1.35142338e+00,\n",
      "        -3.97252470e-01, -2.28876162e+00,  8.06768596e-01,  1.04319133e-01,\n",
      "        -4.26951051e-01,  5.47176301e-01, -3.02147835e-01,  2.61345565e-01,\n",
      "        -2.88605750e-01, -3.68970260e-02, -1.68711692e-01, -1.09926474e+00,\n",
      "        -6.09709024e-01, -1.95561612e+00,  3.72162342e-01,  6.47602379e-01,\n",
      "        -1.50654212e-01, -1.90359378e+00,  6.11368537e-01, -1.67713618e+00,\n",
      "         2.25080752e+00, -7.26091921e-01, -1.67469919e+00, -1.28860459e-01,\n",
      "         2.78164387e+00, -2.89340109e-01,  6.40640855e-01,  1.33787036e-01,\n",
      "        -1.19585490e+00, -8.75051141e-01,  2.17017841e+00, -1.46772897e+00,\n",
      "        -1.40103686e+00,  1.74519926e-01, -1.79291606e-01,  1.46435988e+00,\n",
      "        -1.52300075e-01,  4.67800766e-01, -2.39023733e+00,  6.23838902e-02,\n",
      "         1.08715689e+00,  3.04750741e-01, -4.19174582e-01,  1.23890376e+00,\n",
      "         4.14501876e-01,  1.62059832e+00, -1.13694549e+00,  2.15018854e-01,\n",
      "         2.54646468e+00, -3.35675150e-01, -1.20189130e+00, -9.68454421e-01,\n",
      "        -4.11137789e-01, -2.17241740e+00,  1.28511200e-03,  1.62280118e+00,\n",
      "        -2.14626813e+00, -1.35886502e+00, -3.00083303e+00,  1.24352181e+00,\n",
      "        -6.30314529e-01,  4.18747306e-01,  8.45419884e-01, -1.17566541e-01,\n",
      "         1.35520613e+00,  7.88126171e-01,  1.76451635e+00,  7.60854542e-01,\n",
      "        -2.89514363e-01,  1.86136514e-01, -1.95669913e+00, -8.31047773e-01,\n",
      "        -6.07902586e-01, -3.80930543e-01,  9.89203155e-01, -5.81052303e-01,\n",
      "        -1.15061104e+00,  1.50502753e+00, -6.60263002e-01, -7.09198356e-01,\n",
      "         5.86781502e-01,  1.57544601e+00, -6.84090376e-01, -8.15364540e-01,\n",
      "        -9.48047936e-01, -5.66457510e-01,  5.48577785e-01,  2.03176916e-01,\n",
      "        -2.44672847e+00, -5.25729537e-01, -9.22042429e-02,  7.07108617e-01,\n",
      "        -5.31263232e-01, -1.30364287e+00, -1.57186136e-01, -1.29589021e+00,\n",
      "         1.17161345e+00,  2.84530550e-01, -1.63092899e+00,  6.98065102e-01,\n",
      "         7.08338320e-02, -7.81517208e-01,  1.10013580e+00,  1.17401086e-01,\n",
      "        -6.96550131e-01,  9.09370959e-01,  6.10515833e-01,  2.12450767e+00,\n",
      "        -2.12933615e-01, -1.87208962e+00,  4.29807790e-02,  8.69626030e-02,\n",
      "        -1.27790296e+00,  6.82715297e-01, -3.61863017e-01,  1.32600355e+00,\n",
      "        -1.01764166e+00, -1.33966959e+00, -8.28323066e-02,  1.65407729e+00,\n",
      "         1.33805084e+00,  5.25578022e-01,  9.16313112e-01, -1.82701278e+00,\n",
      "        -6.14299536e-01,  7.52980173e-01,  1.52765381e+00,  9.93892789e-01,\n",
      "        -6.77479863e-01,  6.80557609e-01, -4.65378195e-01, -3.58703971e-01,\n",
      "        -1.79316446e-01,  4.31707054e-01,  1.10931742e+00,  3.72054167e-02,\n",
      "        -2.69901026e-02,  6.43105388e-01, -1.33095896e+00, -5.92227951e-02,\n",
      "         6.24210536e-01,  1.19150424e+00, -8.16059113e-02, -1.51314342e+00,\n",
      "        -3.53771329e+00, -2.36542299e-01,  5.25700808e-01, -1.35523891e+00,\n",
      "         1.14559901e+00, -5.86216688e-01,  5.39616525e-01,  1.17623210e+00,\n",
      "        -3.61649632e-01,  2.19849348e+00,  1.56403720e+00,  1.76430333e+00,\n",
      "         8.53846133e-01,  2.67246634e-01,  7.13754892e-01,  1.39514124e+00,\n",
      "        -2.49083185e+00, -4.20039296e-01,  3.47382456e-01,  1.59823552e-01,\n",
      "        -1.44572720e-01, -7.84732580e-01, -8.83106664e-02, -3.39085460e-01,\n",
      "         1.10222113e+00, -1.62283194e+00, -4.47978556e-01,  1.34253263e+00,\n",
      "         9.76939559e-01,  8.18755925e-02,  8.92627299e-01,  7.34527409e-01,\n",
      "         1.94595635e+00, -4.85903025e-01,  2.80046493e-01,  1.54473579e+00,\n",
      "        -1.81661189e+00, -4.64476794e-01,  8.58601987e-01, -5.54533243e-01,\n",
      "        -1.94275364e-01, -1.14623559e+00, -6.23974741e-01,  8.43465865e-01,\n",
      "         9.52140450e-01, -4.33964461e-01, -2.23771691e+00,  1.13802516e+00],\n",
      "       dtype=float32)\n",
      " array([-2.00806689e+00,  6.83192909e-01, -1.98112071e+00,  5.22643566e-01,\n",
      "         4.12592316e+00, -1.74389303e-01, -3.82635370e-02,  3.88395786e+00,\n",
      "         2.46997163e-01, -7.22308934e-01,  4.94423246e+00,  1.16498387e+00,\n",
      "        -2.72524595e+00,  4.75285888e-01,  2.05770195e-01,  1.92214823e+00,\n",
      "         1.16344404e+00, -2.28740975e-01, -1.24719894e+00, -1.05837429e+00,\n",
      "         8.98300707e-01, -6.01152062e-01, -8.42806160e-01, -2.52719045e-01,\n",
      "         2.02032015e-01, -1.55949664e+00, -2.65064049e+00, -6.92428827e-01,\n",
      "        -3.42696249e-01,  3.76514584e-01,  6.21540606e-01,  2.10145161e-01,\n",
      "        -2.80795395e-01, -1.40520382e+00, -2.55481553e+00, -8.66777182e-01,\n",
      "        -8.21462870e-01,  5.80785573e-01,  2.58024454e-01, -1.30411342e-03,\n",
      "         5.91454357e-02,  1.15603566e-01,  4.50470224e-02,  1.61677867e-01,\n",
      "        -1.38919067e+00,  5.15460789e-01,  1.37410164e-01, -2.26834989e+00,\n",
      "        -7.89022803e-01,  2.05537820e+00, -1.63662982e+00,  1.04221451e+00,\n",
      "         6.88941121e-01, -4.11648464e+00, -2.43470445e-01,  5.48511326e-01,\n",
      "         4.79137190e-02,  5.17636180e-01,  6.35730565e-01, -5.25225520e-01,\n",
      "         4.32876319e-01, -9.58149076e-01,  6.03530295e-02, -1.16442549e+00,\n",
      "         2.26836705e+00,  9.17555809e-01, -2.58146715e+00, -2.44136930e+00,\n",
      "         8.90383184e-01,  2.09348416e+00,  2.85320301e-02, -6.47175074e-01,\n",
      "        -1.57194424e+00, -4.96377982e-02, -4.18267399e-01,  1.17871988e+00,\n",
      "        -2.61854482e+00,  2.51098537e+00, -2.94046593e+00, -6.84698641e-01,\n",
      "        -3.20401192e+00, -4.44459736e-01,  7.30995297e-01,  8.16648245e-01,\n",
      "         2.40701246e+00,  1.44935310e-01, -1.73654103e+00, -2.83850002e+00,\n",
      "         1.09837210e+00, -1.45332003e+00, -1.37889707e+00, -3.83227617e-01,\n",
      "         2.01055741e+00, -2.77523279e+00,  1.12134421e+00, -7.60145605e-01,\n",
      "         4.57065880e-01, -3.90481681e-01,  1.78710437e+00,  1.24100149e+00,\n",
      "         1.88752544e+00,  9.63965416e-01,  2.18476462e+00,  2.86951137e+00,\n",
      "        -1.81711912e-01,  2.81824207e+00, -9.10733044e-02, -1.77163911e+00,\n",
      "        -1.02581680e-01, -3.25934696e+00,  6.97675049e-01,  6.72114789e-01,\n",
      "        -1.09908617e+00,  9.15572047e-01,  6.30765796e-01, -1.85508244e-02,\n",
      "        -8.03051233e-01, -1.79579288e-01, -2.61420041e-01, -6.98774755e-01,\n",
      "        -5.86979210e-01, -3.06698203e+00,  9.84841883e-01,  1.25296533e+00,\n",
      "        -2.92505771e-01, -3.15925431e+00,  1.58507645e+00, -1.70023775e+00,\n",
      "         2.56799865e+00, -1.41295910e+00, -2.21753383e+00, -3.01366091e-01,\n",
      "         3.69488978e+00, -7.76592791e-01, -2.00253427e-01,  5.35006046e-01,\n",
      "        -9.51654553e-01, -2.75412261e-01,  2.89445782e+00, -1.81521356e+00,\n",
      "        -2.26013088e+00, -6.26026094e-01,  1.51219189e-01,  1.74745369e+00,\n",
      "         1.02416539e+00,  8.23566437e-01, -3.03275061e+00,  2.78828055e-01,\n",
      "         1.12329447e+00,  9.42641437e-01,  1.75637510e-02,  2.07685280e+00,\n",
      "        -9.97349434e-03,  1.70246220e+00, -1.82497048e+00,  6.13681793e-01,\n",
      "         2.25227952e+00, -9.41564798e-01, -2.51976943e+00, -1.15418530e+00,\n",
      "        -6.51248574e-01, -3.05559301e+00,  1.28515348e-01,  1.74535942e+00,\n",
      "        -1.98121333e+00, -1.47267699e+00, -3.35260296e+00,  1.45674932e+00,\n",
      "        -3.53584379e-01,  2.42274284e-01,  1.11497474e+00, -2.48208314e-01,\n",
      "         1.66044068e+00,  1.03796947e+00,  1.68330872e+00,  3.38130295e-01,\n",
      "        -3.51165742e-01, -7.62594640e-01, -2.11363578e+00, -1.20347118e+00,\n",
      "        -1.05646586e+00,  5.84854007e-01,  8.29282165e-01,  2.46937528e-01,\n",
      "        -9.20689106e-01,  1.01607692e+00, -1.33992839e+00, -2.29964450e-01,\n",
      "         9.99825358e-01,  1.79473555e+00,  1.72811244e-02, -1.36405528e+00,\n",
      "        -1.59775898e-01, -5.69732904e-01,  4.86553043e-01,  2.33081684e-01,\n",
      "        -2.37759805e+00, -5.12090266e-01,  6.73857331e-03,  1.00314271e+00,\n",
      "        -9.84173357e-01, -1.28171766e+00, -2.53544807e-01, -2.06536436e+00,\n",
      "         2.24009633e+00,  6.61671758e-01, -2.36688256e+00,  1.03591800e+00,\n",
      "        -5.14129460e-01, -4.19236660e-01,  1.19098341e+00,  7.65109420e-01,\n",
      "        -1.03451598e+00,  1.95005929e+00,  7.62803078e-01,  1.99392307e+00,\n",
      "         6.14238024e-01, -1.79824877e+00, -1.46830916e-01, -1.42597750e-01,\n",
      "        -2.18753147e+00,  6.07680857e-01, -1.21949837e-01,  1.00967562e+00,\n",
      "        -1.93798780e+00, -8.93193722e-01,  1.00772882e+00,  2.43197465e+00,\n",
      "         1.22513199e+00,  4.53536153e-01,  2.20738864e+00, -2.05889130e+00,\n",
      "        -8.55696499e-01,  1.49749565e+00,  1.84762394e+00,  1.22461152e+00,\n",
      "        -1.35086942e+00,  2.51185596e-01, -8.82783294e-01, -3.46429288e-01,\n",
      "         1.07745893e-01,  1.00638819e+00,  2.02513170e+00,  9.69873294e-02,\n",
      "        -1.15601063e+00,  1.65002477e+00, -1.74911308e+00, -7.87997544e-02,\n",
      "         2.62743741e-01,  2.19299555e+00,  3.29823732e-01, -2.20189118e+00,\n",
      "        -4.56517887e+00, -8.36398840e-01,  3.79222572e-01, -1.81909966e+00,\n",
      "         2.04699731e+00, -3.24529558e-01,  4.18196619e-03,  9.49270129e-01,\n",
      "         2.82678872e-01,  3.74753356e+00,  2.64947844e+00,  2.10193753e+00,\n",
      "         1.00541520e+00, -5.18803000e-01,  5.90969443e-01,  2.03251624e+00,\n",
      "        -3.28852415e+00, -1.15708411e-01, -1.30209163e-01, -1.95819676e-01,\n",
      "         1.19150326e-01, -2.03723836e+00,  7.69917786e-01, -6.95337594e-01,\n",
      "         1.30682373e+00, -1.32903337e+00, -1.02565682e+00,  1.26011634e+00,\n",
      "         1.34417093e+00,  7.12326318e-02,  6.86675906e-01,  1.16549194e+00,\n",
      "         3.02883220e+00, -1.08646011e+00,  1.41777724e-01,  1.55007100e+00,\n",
      "        -1.74378383e+00,  1.05838798e-01,  5.44819236e-01, -9.48340595e-01,\n",
      "        -3.68647426e-01,  1.52720734e-01, -1.73574948e+00,  1.24954379e+00,\n",
      "         1.37252724e+00, -8.45659733e-01, -2.16448855e+00,  1.13677609e+00],\n",
      "       dtype=float32)\n",
      " array([-1.94256985e+00,  4.42104833e-03, -1.72584510e+00,  3.23782070e-03,\n",
      "         2.46967316e+00, -2.75891684e-02,  4.56435382e-01,  3.10099506e+00,\n",
      "        -9.40225124e-02, -2.36382693e-01,  3.53250623e+00,  1.43928170e+00,\n",
      "        -2.17169428e+00,  1.09604239e+00,  7.51698673e-01,  7.54990160e-01,\n",
      "         7.66982555e-01, -3.08580548e-01, -1.29177761e+00, -1.67769825e+00,\n",
      "         7.77088761e-01,  7.41139129e-02, -7.36413717e-01,  2.15698734e-01,\n",
      "        -5.31863451e-01, -1.17572427e+00, -2.20699525e+00, -3.61532748e-01,\n",
      "        -3.07905406e-01,  6.53831482e-01,  7.16041088e-01, -4.23211515e-01,\n",
      "        -1.25965416e-01, -4.66064841e-01, -1.57866335e+00, -6.29701793e-01,\n",
      "        -4.09603864e-01,  4.54871029e-01,  9.55387712e-01,  1.42180815e-01,\n",
      "         3.03016275e-01,  8.73680413e-02,  6.20188862e-02,  4.00480628e-01,\n",
      "        -8.97807181e-01,  1.16967559e+00,  3.51967961e-01, -1.88550329e+00,\n",
      "        -7.69446790e-01,  1.16369045e+00, -7.37608492e-01,  4.70321059e-01,\n",
      "         2.27420166e-01, -2.77276659e+00, -5.39229631e-01,  8.52206171e-01,\n",
      "        -1.55421317e-01,  5.68580329e-01,  2.87850440e-01, -3.95088196e-01,\n",
      "        -6.47671744e-02, -1.12338328e+00, -4.22763616e-01, -6.40029371e-01,\n",
      "         2.02007723e+00,  8.10747325e-01, -1.24482620e+00, -1.87181544e+00,\n",
      "         1.49296477e-01,  1.52386391e+00, -3.00882548e-01, -1.46935916e+00,\n",
      "        -9.12871718e-01, -2.01623961e-01, -1.40491486e-01,  8.58075738e-01,\n",
      "        -2.24651957e+00,  1.28255820e+00, -1.88823032e+00, -4.34648514e-01,\n",
      "        -2.21760511e+00, -4.56524044e-01,  5.81380665e-01,  8.13725114e-01,\n",
      "         2.03986192e+00, -4.10748988e-01, -1.41541326e+00, -1.59195137e+00,\n",
      "         7.87934124e-01, -7.60666609e-01, -7.46703684e-01,  2.97385067e-01,\n",
      "         1.72545016e+00, -2.54056811e+00,  4.66460168e-01, -4.02537435e-01,\n",
      "         1.65127933e-01, -8.32147419e-01,  1.15145338e+00,  9.57507968e-01,\n",
      "         1.57188022e+00,  1.17392015e+00,  1.19686353e+00,  1.55507696e+00,\n",
      "        -3.67077351e-01,  2.61282253e+00, -3.95335227e-01, -1.73636460e+00,\n",
      "        -3.27655636e-02, -2.21120954e+00,  6.06953263e-01, -3.53602350e-01,\n",
      "         2.62746602e-01,  8.21281783e-03,  1.32575125e-01,  3.59039456e-01,\n",
      "        -3.80571634e-01,  1.65037233e-02,  2.55764931e-01, -1.37078106e+00,\n",
      "        -4.83505368e-01, -2.09399629e+00,  3.60798806e-01,  5.94170332e-01,\n",
      "        -7.21797764e-01, -1.87697077e+00,  1.02134609e+00, -1.73796117e+00,\n",
      "         2.13878727e+00, -3.13042462e-01, -1.64183354e+00, -8.51880834e-02,\n",
      "         2.30595613e+00, -3.94432604e-01,  2.82646954e-01,  1.29932597e-01,\n",
      "        -1.13369155e+00, -1.06928444e+00,  2.00598717e+00, -1.35206139e+00,\n",
      "        -1.30591583e+00, -4.60173011e-01, -2.02027947e-01,  9.40655529e-01,\n",
      "         2.14291975e-01,  2.47665673e-01, -1.81035221e+00, -2.03990683e-01,\n",
      "         1.09412265e+00,  2.39574015e-01, -6.35280788e-01,  1.58611453e+00,\n",
      "        -8.78304616e-03,  1.06279922e+00, -6.46630645e-01,  3.88221502e-01,\n",
      "         2.23961496e+00, -6.76872730e-01, -1.56211936e+00, -9.94034946e-01,\n",
      "        -6.09502673e-01, -2.25134277e+00,  2.75419801e-01,  1.72041571e+00,\n",
      "        -1.70518100e+00, -8.96380246e-01, -2.75675631e+00,  1.45341849e+00,\n",
      "        -6.08659506e-01,  6.75182939e-01,  5.44757605e-01,  1.35098517e-01,\n",
      "         1.48070526e+00,  7.98201501e-01,  1.57269418e+00,  5.83756566e-01,\n",
      "        -3.95167798e-01, -1.71335816e-01, -1.81562674e+00, -1.23700750e+00,\n",
      "        -8.17150831e-01, -8.33570421e-01,  6.83945179e-01, -6.31687343e-01,\n",
      "        -9.65745807e-01,  1.02547705e+00, -1.00024676e+00, -6.85366035e-01,\n",
      "         3.67377073e-01,  1.09300256e+00, -4.24257576e-01, -1.00949466e+00,\n",
      "        -6.64534152e-01, -9.85670507e-01,  3.88076454e-01,  7.37382650e-01,\n",
      "        -2.30709052e+00, -5.45788407e-01,  3.85175683e-02,  6.44764960e-01,\n",
      "        -1.09632277e+00, -8.08730066e-01,  2.92255759e-01, -1.50614142e+00,\n",
      "         1.47751558e+00,  3.74270976e-01, -1.72879219e+00,  8.24062586e-01,\n",
      "        -1.39104173e-01, -3.36972028e-01,  8.20418358e-01,  1.56947985e-01,\n",
      "        -6.50330901e-01,  1.57342553e+00,  1.30779177e-01,  1.76356637e+00,\n",
      "         1.23448625e-01, -1.72177291e+00,  1.47203496e-02,  4.58325177e-01,\n",
      "        -1.84406781e+00,  7.58792579e-01, -1.72257647e-01,  7.19922543e-01,\n",
      "        -9.90948319e-01, -1.51909018e+00,  1.56796724e-01,  1.69619942e+00,\n",
      "         1.24988973e+00,  3.64285558e-01,  9.27842796e-01, -2.09932184e+00,\n",
      "        -4.57800537e-01,  5.42374313e-01,  1.61327732e+00,  1.29979873e+00,\n",
      "        -1.10108685e+00,  1.19304287e+00, -5.43060482e-01, -2.02781573e-01,\n",
      "        -7.01106787e-01, -2.68423319e-01,  5.68878591e-01, -4.48259301e-02,\n",
      "        -3.22558582e-01,  7.57512510e-01, -1.33144403e+00,  3.76773357e-01,\n",
      "         7.70827770e-01,  1.67822003e+00,  1.64808437e-01, -1.29612386e+00,\n",
      "        -3.20336699e+00, -4.21882272e-02,  4.64709312e-01, -1.38176811e+00,\n",
      "         1.24684644e+00, -6.67568862e-01,  5.69749534e-01,  8.41446280e-01,\n",
      "         7.80325532e-02,  3.33842635e+00,  1.65450966e+00,  1.37237930e+00,\n",
      "         9.33111310e-01, -4.52157408e-01,  8.13544631e-01,  1.20464849e+00,\n",
      "        -2.09388685e+00, -4.11865115e-01,  4.30697918e-01, -5.13605297e-01,\n",
      "        -2.14537501e-01, -4.79418874e-01, -6.01710200e-01, -1.29434690e-02,\n",
      "         1.05586350e+00, -1.44056308e+00, -6.16630197e-01,  1.31729043e+00,\n",
      "         4.59585488e-01,  1.19772442e-01,  1.11490107e+00,  9.05794740e-01,\n",
      "         1.97568429e+00, -7.14700043e-01, -4.66983587e-01,  1.50874257e+00,\n",
      "        -2.06117606e+00,  9.61217061e-02,  5.97452939e-01, -3.56830955e-01,\n",
      "        -1.83998019e-01, -9.19437408e-01, -1.04138112e+00,  1.03274643e+00,\n",
      "         1.14006042e+00, -5.92096925e-01, -1.81781018e+00,  1.11937594e+00],\n",
      "       dtype=float32)\n",
      " ...\n",
      " array([-1.5554852 ,  0.59183073, -1.1383034 ,  0.15321052,  4.727991  ,\n",
      "        -0.28851315, -0.03688707,  3.743432  ,  0.09258975, -0.92743593,\n",
      "         4.730609  ,  0.98775   , -3.1233056 ,  0.79659766, -0.5833688 ,\n",
      "         2.1969159 ,  0.9856892 ,  0.250787  , -1.3274186 , -0.7346638 ,\n",
      "         0.7908813 , -1.3364705 , -1.0469693 ,  0.14210908,  0.3795376 ,\n",
      "        -1.4658258 , -2.2821975 , -0.361084  , -0.18279792, -0.26052552,\n",
      "         0.35134313,  0.38832265, -0.6763883 , -0.53296673, -2.5972874 ,\n",
      "        -0.44851208, -0.8416504 ,  0.47812888,  0.65408075,  0.07044544,\n",
      "        -0.09710789,  0.13850006, -0.65041196,  0.4505694 , -1.6494484 ,\n",
      "         0.46225128,  0.82178557, -2.3312833 , -0.9619474 ,  2.0824947 ,\n",
      "        -1.2805526 ,  1.4807646 ,  0.76562876, -4.2825975 , -0.4506014 ,\n",
      "         0.25427753, -0.02528014,  0.4941741 ,  1.0374517 , -0.5022656 ,\n",
      "         0.5494505 , -0.9567612 ,  0.03287708, -0.7376486 ,  2.9095562 ,\n",
      "         1.5307957 , -2.06131   , -2.501724  ,  1.0079193 ,  1.9124147 ,\n",
      "        -0.04341882, -1.2772833 , -1.947667  , -0.21538974, -0.43104434,\n",
      "         1.0601522 , -2.298065  ,  2.7445843 , -3.6280105 , -0.13229878,\n",
      "        -3.6618278 , -0.42609006,  0.33084813,  1.2072381 ,  2.243973  ,\n",
      "        -0.14440762, -2.555416  , -2.8398762 ,  1.0263749 , -0.9560955 ,\n",
      "        -1.1158403 , -0.37648925,  1.2649213 , -2.6546323 ,  1.8871312 ,\n",
      "        -0.48033196,  0.43584645, -0.6894807 ,  1.3568786 ,  1.750513  ,\n",
      "         2.328069  ,  1.4571143 ,  1.8324881 ,  2.9457762 , -0.44179016,\n",
      "         3.4924614 ,  0.75404716, -2.4548037 ,  0.37371808, -4.235036  ,\n",
      "         1.1721668 ,  1.078718  , -1.1512747 ,  0.703291  ,  1.5780066 ,\n",
      "         0.22362255, -1.0932504 , -0.13214469, -0.47107047, -0.8558353 ,\n",
      "        -0.44419587, -3.4212484 ,  1.5592413 ,  1.1822767 , -0.25038186,\n",
      "        -3.1240277 ,  1.8389232 , -1.4858007 ,  2.1400273 , -1.9196414 ,\n",
      "        -2.6406257 , -0.35946617,  2.763459  , -0.7542135 , -0.32048103,\n",
      "         0.8165924 , -1.0407546 , -0.3426814 ,  3.0116928 , -1.611388  ,\n",
      "        -2.5505502 , -1.0148613 ,  0.23623325,  1.7294961 ,  1.022267  ,\n",
      "         0.86391854, -2.9909687 ,  0.46834192,  1.7434738 ,  1.2773144 ,\n",
      "        -0.01597438,  2.0418892 ,  0.02871984,  1.4918247 , -1.8337687 ,\n",
      "         0.39591977,  2.2302642 , -0.5069913 , -2.9938004 , -1.2735618 ,\n",
      "        -0.1299393 , -3.1273975 , -0.4801233 ,  1.9486748 , -2.382128  ,\n",
      "        -1.7967838 , -3.9922423 ,  1.6436967 , -0.16461875,  0.09894156,\n",
      "         0.9204084 ,  0.01907303,  2.0794103 ,  0.30474734,  2.26834   ,\n",
      "         0.8254463 ,  0.05741997, -0.9197668 , -1.7077391 , -1.285548  ,\n",
      "        -1.3921701 ,  0.60043603,  0.8392703 ,  0.8593701 , -0.6582328 ,\n",
      "         0.31189355, -1.3251615 , -0.16252439,  1.2389026 ,  1.480404  ,\n",
      "         0.3294736 , -1.6632936 , -0.11430215, -1.3060778 , -0.24327427,\n",
      "         0.72684324, -2.501028  , -0.6136991 ,  0.32766986,  1.510504  ,\n",
      "        -1.2630101 , -1.7325267 ,  0.29606828, -2.723372  ,  3.6085541 ,\n",
      "         1.0734016 , -1.9967495 ,  1.3730735 , -0.7629424 , -0.13766298,\n",
      "         0.31942382,  0.8233954 , -0.87723887,  2.0098464 ,  0.8244203 ,\n",
      "         2.21354   ,  0.35686368, -1.6318066 ,  0.33322957, -0.18733169,\n",
      "        -1.9850293 ,  0.97303855, -0.453837  ,  0.4502747 , -2.5630765 ,\n",
      "        -0.83283675,  0.5447243 ,  2.5470507 ,  0.35397273,  0.0875492 ,\n",
      "         2.4270937 , -2.305791  , -1.9068117 ,  1.5162964 ,  1.9669966 ,\n",
      "         1.4365972 , -1.0786672 ,  0.55266726, -1.0272864 , -0.40343028,\n",
      "        -0.73083395,  0.9073842 ,  1.9674958 ,  0.25861615, -1.3815496 ,\n",
      "         2.004372  , -2.0935836 , -0.84082884, -0.31502956,  2.246481  ,\n",
      "         1.1718823 , -3.3296845 , -4.227947  , -1.0221381 ,  0.38722652,\n",
      "        -2.3480353 ,  2.1846914 , -1.3752903 , -0.12743348,  0.94456214,\n",
      "        -0.7316467 ,  4.1324916 ,  1.902784  ,  2.1193643 ,  1.4013946 ,\n",
      "        -0.68645656,  0.32544672,  2.35168   , -3.041959  , -0.06218963,\n",
      "        -0.2653927 ,  0.3766576 ,  0.21088059, -2.034476  ,  0.84367263,\n",
      "         0.32029426,  1.2233926 , -1.420534  , -1.154859  ,  2.1818266 ,\n",
      "         1.0914986 , -0.07107754,  0.20726395,  0.556801  ,  2.0556097 ,\n",
      "        -0.7317819 ,  0.72274566,  1.7391717 , -1.1278318 ,  0.135234  ,\n",
      "         0.5793314 , -0.44483528, -1.1976967 ,  0.83917516, -1.2871453 ,\n",
      "         1.6333266 ,  0.8913041 , -1.3449328 , -1.5967542 ,  1.1116449 ],\n",
      "       dtype=float32)\n",
      " array([-1.90640116e+00,  7.13336051e-01, -1.94804013e+00,  4.21977401e-01,\n",
      "         4.45890713e+00,  4.00111824e-01,  6.06310308e-01,  3.77747655e+00,\n",
      "        -5.31819239e-02, -1.43085313e+00,  5.58030796e+00,  1.22625864e+00,\n",
      "        -3.17293096e+00,  6.62053525e-01,  1.03604928e-01,  2.28459597e+00,\n",
      "         8.05117905e-01, -1.17318235e-01, -1.63435221e+00, -1.43872321e+00,\n",
      "         1.26111114e+00, -6.55870676e-01, -7.79423356e-01,  1.08420312e-01,\n",
      "         5.43867648e-01, -1.35873652e+00, -2.44504786e+00, -7.40953624e-01,\n",
      "        -2.41202176e-01,  6.76538765e-01,  1.11423755e+00, -3.11645269e-01,\n",
      "        -5.97559869e-01, -1.76365840e+00, -2.47153735e+00, -1.20222700e+00,\n",
      "        -4.29643929e-01,  5.81409812e-01,  6.55986786e-01, -5.31451106e-02,\n",
      "         1.56221151e-01,  1.23365588e-01, -2.18879089e-01,  1.72075361e-01,\n",
      "        -1.35629487e+00,  1.06561780e+00,  3.95337343e-01, -1.94611776e+00,\n",
      "        -7.03451633e-01,  1.45137393e+00, -1.58269906e+00,  1.84002006e+00,\n",
      "         1.96402930e-02, -5.03119802e+00, -2.30324611e-01,  7.62737930e-01,\n",
      "         2.13518873e-01,  9.34161127e-01,  7.90762722e-01, -5.22431672e-01,\n",
      "         3.50957155e-01, -8.77089620e-01,  7.10518360e-01, -1.28446412e+00,\n",
      "         2.76168847e+00,  1.45017123e+00, -2.44742393e+00, -2.78393197e+00,\n",
      "         1.15040660e+00,  1.87373137e+00,  7.81552643e-02, -7.87661374e-01,\n",
      "        -1.68151629e+00,  1.66433364e-01, -2.72625029e-01,  1.64794469e+00,\n",
      "        -2.88782644e+00,  3.02645802e+00, -3.35855985e+00,  1.17077984e-01,\n",
      "        -4.09833908e+00, -2.26426199e-01,  9.89276052e-01,  1.39683485e+00,\n",
      "         2.33261180e+00,  3.46109360e-01, -2.31709337e+00, -3.28348088e+00,\n",
      "         6.03924930e-01, -1.06554902e+00, -7.87997246e-01, -3.59222472e-01,\n",
      "         1.48801970e+00, -3.28473830e+00,  6.71603203e-01, -7.19119072e-01,\n",
      "         7.35698283e-01, -1.10186934e+00,  1.17073011e+00,  1.57549536e+00,\n",
      "         2.48901057e+00,  1.02606976e+00,  2.01834607e+00,  3.39912677e+00,\n",
      "        -9.19991553e-01,  3.84235716e+00,  8.74022424e-01, -2.33519053e+00,\n",
      "        -8.36570486e-02, -3.48854208e+00,  1.34558046e+00,  1.39730179e+00,\n",
      "        -1.58357215e+00,  8.72798741e-01,  1.64491606e+00,  4.86481369e-01,\n",
      "        -7.35183060e-01, -6.99140072e-01, -5.69885552e-01, -5.33111215e-01,\n",
      "        -4.27543849e-01, -3.11929345e+00,  1.10646963e+00,  1.19196725e+00,\n",
      "        -7.71078646e-01, -3.14843798e+00,  1.21795928e+00, -2.06479383e+00,\n",
      "         2.71860790e+00, -1.64708126e+00, -2.94518971e+00, -2.28632987e-01,\n",
      "         4.00684500e+00, -5.76004386e-01,  1.79674830e-02,  1.21714449e+00,\n",
      "        -1.56110585e+00, -2.90914327e-01,  2.42881989e+00, -2.10421586e+00,\n",
      "        -1.97488427e+00, -1.43928635e+00,  3.03722292e-01,  1.61052132e+00,\n",
      "         8.55134964e-01,  8.23997617e-01, -4.01497841e+00, -3.65952551e-01,\n",
      "         7.60200560e-01,  9.92321968e-01, -3.04184686e-02,  2.39256716e+00,\n",
      "        -7.00428858e-02,  1.56483853e+00, -1.56597924e+00,  1.04433727e+00,\n",
      "         2.36823678e+00, -1.02020967e+00, -2.15672040e+00, -1.35697675e+00,\n",
      "        -2.38355845e-01, -2.19920039e+00, -3.08111906e-01,  2.38438368e+00,\n",
      "        -1.76504266e+00, -2.02569771e+00, -3.27816892e+00,  1.06440187e+00,\n",
      "         9.85794663e-02,  1.43930033e-01,  1.41076171e+00, -1.47113074e-02,\n",
      "         1.94246030e+00,  2.76433229e-01,  1.21715832e+00,  3.41448426e-01,\n",
      "         3.83887836e-03,  1.13399085e-02, -2.42357826e+00, -1.35415947e+00,\n",
      "        -1.62941527e+00,  3.91044706e-01,  1.12364471e+00,  3.27901185e-01,\n",
      "        -9.35393214e-01,  9.42551851e-01, -1.32286763e+00,  3.62207979e-01,\n",
      "         1.48501360e+00,  2.04234624e+00,  2.60811150e-01, -1.86166191e+00,\n",
      "         1.38491020e-01, -1.28345251e+00,  3.86187047e-01,  7.78639853e-01,\n",
      "        -2.79155374e+00, -5.59367359e-01, -4.79696631e-01,  9.96359527e-01,\n",
      "        -9.37085450e-01, -1.60282326e+00,  2.20478661e-02, -2.38669348e+00,\n",
      "         3.63264036e+00,  8.80839705e-01, -3.44836259e+00,  1.35996878e+00,\n",
      "        -5.60613394e-01, -6.04082718e-02,  1.33597898e+00,  1.26000857e+00,\n",
      "        -1.12313032e+00,  1.55713427e+00,  1.45231533e+00,  2.50603819e+00,\n",
      "         6.25591695e-01, -2.21639252e+00, -8.42550695e-02, -5.62756598e-01,\n",
      "        -2.10640383e+00,  1.59965336e+00, -3.58979076e-01,  2.85953701e-01,\n",
      "        -2.17062306e+00, -8.17989886e-01,  1.39550358e-01,  2.40389013e+00,\n",
      "         9.42689121e-01, -2.18335196e-01,  2.71764255e+00, -2.12493110e+00,\n",
      "        -5.76575160e-01,  1.95335531e+00,  1.62240279e+00,  1.51081097e+00,\n",
      "        -1.39225471e+00,  6.50337219e-01, -4.46586400e-01, -2.28742450e-01,\n",
      "        -3.80555362e-01,  5.58787227e-01,  1.58448887e+00,  9.97310281e-02,\n",
      "        -1.30742335e+00,  1.97096908e+00, -1.76922798e+00,  3.72031853e-02,\n",
      "         9.37150642e-02,  2.32511187e+00,  6.08629227e-01, -2.65735316e+00,\n",
      "        -4.40148163e+00, -6.19608641e-01,  6.88840568e-01, -2.11288452e+00,\n",
      "         1.55188608e+00, -3.87341470e-01,  2.58582145e-01,  1.13559306e+00,\n",
      "        -3.39633226e-01,  4.08723021e+00,  2.98289084e+00,  1.88011217e+00,\n",
      "         2.24480152e+00, -6.37146890e-01, -1.88655689e-01,  2.18508816e+00,\n",
      "        -2.64671683e+00,  3.04520071e-01,  1.88987896e-01,  2.01408327e-01,\n",
      "         7.93248236e-01, -2.25252843e+00,  1.04442132e+00, -2.27036238e-01,\n",
      "         1.47758341e+00, -2.53482270e+00, -1.19831252e+00,  1.54635549e+00,\n",
      "         1.64450145e+00,  1.08990826e-01,  9.88831520e-01,  6.12485766e-01,\n",
      "         1.95224333e+00, -5.09053349e-01,  3.80087286e-01,  1.47320020e+00,\n",
      "        -8.05244267e-01,  2.86360353e-01,  4.99621481e-01, -5.66559672e-01,\n",
      "        -1.18873072e+00, -2.35535745e-02, -1.02429748e+00,  1.43309069e+00,\n",
      "         5.24577737e-01, -1.09269607e+00, -2.42997718e+00,  9.07317162e-01],\n",
      "       dtype=float32)\n",
      " array([-1.4787375 ,  2.0932803 , -2.2330246 ,  0.43662614,  4.03206   ,\n",
      "         0.38838533, -0.07996203,  4.3312707 , -0.67649794, -0.5414718 ,\n",
      "         4.9706903 ,  1.1806589 , -2.7476332 ,  0.6319759 ,  0.3632751 ,\n",
      "         2.0125175 ,  0.6063169 , -0.77118933, -0.67710257, -0.34848225,\n",
      "         1.2537063 , -0.95371616, -0.1995587 , -1.1075461 , -0.12399513,\n",
      "        -1.4442207 , -2.2343056 , -0.92221934, -0.27957037,  0.8021171 ,\n",
      "         0.16430102,  0.15474269, -0.41773444, -1.0723132 , -2.889118  ,\n",
      "        -1.1522942 , -1.0010953 ,  1.1505781 ,  0.757104  ,  0.6340204 ,\n",
      "        -0.23388629,  0.01755649,  0.31551307,  0.15874027, -1.3154708 ,\n",
      "         0.9830962 , -0.05927242, -2.662517  , -1.2095706 ,  3.0827608 ,\n",
      "        -1.9230486 ,  1.3067676 ,  0.7696885 , -4.0361624 , -0.5118805 ,\n",
      "        -0.42896056,  0.51590097,  0.49562567,  1.3697553 ,  0.32870165,\n",
      "         0.77499765, -0.8573311 ,  0.6679636 , -1.6319026 ,  2.5210419 ,\n",
      "         1.1829737 , -2.6843982 , -2.9403424 ,  1.0870237 ,  1.6596916 ,\n",
      "         0.4314337 , -1.0823253 , -1.4286053 ,  0.50452244, -0.19418186,\n",
      "         1.2399251 , -2.8751554 ,  2.6036265 , -3.4540377 , -0.69829935,\n",
      "        -3.8746576 , -0.82843435,  1.2457484 ,  1.1108724 ,  2.0031462 ,\n",
      "        -0.2871133 , -1.9742188 , -2.914375  ,  0.94502926, -1.0233009 ,\n",
      "        -1.3687886 , -0.55814713,  1.5425718 , -3.8031087 ,  1.8682688 ,\n",
      "        -1.3028369 ,  0.9684346 , -0.65741986,  1.5387505 ,  0.8392396 ,\n",
      "         1.4504884 ,  1.3151284 ,  2.6686525 ,  3.1869051 , -0.5544268 ,\n",
      "         2.7224498 ,  0.04256019, -2.0564926 , -0.45652467, -3.7137008 ,\n",
      "        -0.12102256,  0.59735525, -0.99853617,  1.208914  ,  1.4351823 ,\n",
      "         0.20408091, -1.0735939 , -0.66806173, -0.3216626 ,  0.19889638,\n",
      "        -0.3364455 , -3.6732028 ,  0.39477012,  1.0089699 , -0.8940013 ,\n",
      "        -2.4450066 ,  1.6547059 , -1.3349013 ,  1.2333312 , -1.3287877 ,\n",
      "        -1.9314837 , -0.9053483 ,  3.881334  , -1.2451801 , -0.600631  ,\n",
      "         1.1672242 , -1.5656624 , -0.6115346 ,  3.4343114 , -2.3013    ,\n",
      "        -2.5479922 , -1.5827935 ,  0.99331725,  1.526205  ,  0.64921486,\n",
      "         0.64174175, -3.959249  ,  0.25983542,  1.0809236 ,  0.89865726,\n",
      "        -0.27189884,  2.3116872 , -0.0473053 ,  2.1240053 , -1.5005723 ,\n",
      "         1.605749  ,  2.6631672 , -0.31503364, -2.9139519 , -0.6485685 ,\n",
      "        -0.1567005 , -3.22276   , -0.91993266,  1.508688  , -2.6415246 ,\n",
      "        -2.3413765 , -3.500677  ,  1.356331  , -0.3693424 ,  0.49342582,\n",
      "         1.7975721 ,  0.3015892 ,  2.4682844 ,  0.5627934 ,  1.6874372 ,\n",
      "        -0.10167653, -0.56941396, -0.06407317, -1.4577984 , -1.3283141 ,\n",
      "        -0.981777  ,  0.2612434 ,  1.4961991 ,  0.72873545, -0.998868  ,\n",
      "         0.7755074 , -1.3360362 , -0.8107361 ,  1.7682228 ,  2.0059264 ,\n",
      "         0.3824563 , -0.8133095 , -0.4499642 , -0.654822  , -0.8078982 ,\n",
      "        -0.63535476, -2.1118915 ,  0.16300113, -0.20538658,  1.8683368 ,\n",
      "        -1.2465478 , -1.3693537 , -0.05452124, -2.314986  ,  1.7478774 ,\n",
      "         1.0214647 , -2.1250618 ,  0.50486   , -1.2803203 , -0.4520086 ,\n",
      "         0.21051414,  0.9307486 , -0.86992735,  2.2854102 ,  0.81758636,\n",
      "         2.2219713 ,  0.34096563, -0.68833464, -0.16432585, -0.09777022,\n",
      "        -2.114989  ,  0.5390346 ,  0.03114689,  1.1743404 , -2.186897  ,\n",
      "        -0.8909256 ,  0.6303723 ,  2.8870232 ,  1.0788932 ,  0.49119326,\n",
      "         1.8758101 , -2.3660855 , -1.3679174 ,  1.243533  ,  1.6114386 ,\n",
      "         0.8857599 , -1.7589961 ,  0.8426557 , -0.5927128 , -0.67227465,\n",
      "        -0.2869555 ,  1.3942113 ,  2.1450534 , -0.17662935, -1.167952  ,\n",
      "         1.1151708 , -1.4888659 , -0.17633876, -0.37024644,  2.5053744 ,\n",
      "         0.8149715 , -2.3573313 , -4.731913  , -0.3774136 ,  0.35427982,\n",
      "        -1.9794662 ,  1.7620391 , -0.5500161 , -0.32424608,  1.6941257 ,\n",
      "        -0.31534535,  3.3670058 ,  2.8118408 ,  2.0860631 ,  1.2516032 ,\n",
      "        -0.03980142, -0.12194361,  2.3164377 , -3.4315214 , -0.1475215 ,\n",
      "         0.6732672 , -0.09114609, -0.00560034, -1.9955429 ,  0.5875013 ,\n",
      "         0.01292024,  1.2331935 , -1.195204  , -1.6991551 ,  2.5059807 ,\n",
      "        -0.15535402, -0.3277076 , -0.03345557,  1.0983288 ,  2.977823  ,\n",
      "        -0.73037976,  0.6896464 ,  1.2469544 , -1.08036   ,  0.30425376,\n",
      "         0.9154453 , -1.2834717 , -0.23893686,  0.44787297, -1.484447  ,\n",
      "         1.6006904 ,  1.4296584 ,  0.4828211 , -2.869074  ,  1.2537112 ],\n",
      "       dtype=float32)                                                    ]\n"
     ]
    }
   ],
   "source": [
    "print(\"original_text_vector:\")\n",
    "print(df['original_text_vector'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbfe987d68c20cc7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-16T02:04:28.146632900Z",
     "start_time": "2023-08-16T02:04:28.117115700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned_text_vector\n",
      "[array([-0.17023614,  1.1278214 , -2.2035916 , -1.1955578 ,  1.6034375 ,\n",
      "         0.48677844,  1.4271044 ,  3.1684027 , -1.089497  , -1.0168833 ,\n",
      "         2.1629252 ,  0.11855585, -3.5777528 ,  1.4258801 ,  0.787707  ,\n",
      "         0.8597439 ,  1.2948399 , -0.9969584 , -0.5926872 ,  0.29276463,\n",
      "         0.16431993,  1.7454278 , -0.6545242 , -0.5969262 , -0.32764015,\n",
      "        -0.1126143 , -0.97828287, -0.13737954,  0.13403286,  1.185818  ,\n",
      "         1.2662437 ,  0.7460496 , -0.98786354, -1.5575564 ,  1.0260066 ,\n",
      "        -0.94350207, -0.74118924,  0.12869163,  0.92891026,  0.46200153,\n",
      "        -1.0066432 , -0.1519366 ,  0.7731333 ,  0.56800467, -2.1170402 ,\n",
      "         0.84049326,  0.373796  , -2.4839318 ,  0.29365337,  0.728167  ,\n",
      "        -0.33094934, -0.7141598 ,  0.9037554 , -2.3139718 , -0.8906349 ,\n",
      "         1.4650499 , -0.13164079,  1.0613987 ,  1.6455289 ,  0.7254925 ,\n",
      "         2.497695  ,  0.51226836, -0.8356584 , -1.4531273 ,  1.4884053 ,\n",
      "         0.6306557 , -1.6074775 , -1.4820359 , -0.24412796,  1.5445492 ,\n",
      "        -0.44185257, -0.07185868, -0.94984573, -1.1016266 ,  0.43071055,\n",
      "         0.0325661 , -1.6072936 ,  0.8219398 , -0.39946136,  0.20026207,\n",
      "        -1.9621377 , -0.613245  ,  1.2012224 ,  0.16491978,  0.71676695,\n",
      "         1.0798299 ,  0.7977684 , -1.5145851 , -0.09289462, -0.52522475,\n",
      "        -0.25835514,  0.75131047,  2.1620133 , -2.0140567 , -0.01015029,\n",
      "         0.16316883,  0.04225257, -1.0462601 ,  1.4109553 , -0.4776755 ,\n",
      "         1.4941237 ,  0.5609013 ,  1.6650903 ,  1.4857893 , -1.2253814 ,\n",
      "         2.4517028 , -1.8786737 ,  0.8589967 , -1.7488359 , -0.9689716 ,\n",
      "        -0.40846616, -0.581848  , -0.49516398,  0.10641662, -0.9650244 ,\n",
      "         1.3966188 , -1.1770451 , -1.2164192 , -0.51950896, -0.689833  ,\n",
      "        -1.7633159 , -1.4309069 , -0.1931555 ,  0.6429599 ,  0.5331087 ,\n",
      "        -1.013408  ,  0.98584676, -0.9043289 ,  1.6527352 , -0.47943637,\n",
      "        -1.3563675 , -1.0105823 ,  0.62943286,  0.49357262,  0.14746894,\n",
      "        -0.924144  , -1.9968802 ,  0.35673475,  2.401467  , -0.57105535,\n",
      "        -1.2340348 ,  1.0708985 ,  0.5736649 ,  0.93389314, -1.4635025 ,\n",
      "        -0.15875581, -3.0486717 , -0.11141597,  0.7561653 ,  1.8645388 ,\n",
      "        -0.70798427,  0.72871494,  1.5180309 ,  1.5626779 , -2.4504778 ,\n",
      "        -0.3961767 ,  2.662471  , -0.04886082, -0.94576037, -0.93165535,\n",
      "        -0.92732805, -2.075     ,  0.30100885,  0.41245022, -1.8748094 ,\n",
      "         0.2021677 , -1.7551692 ,  0.6606712 , -0.17142388,  0.1382999 ,\n",
      "         0.51979834,  0.38257205, -0.16326405,  0.44704068,  0.87010735,\n",
      "        -0.25149003, -0.48742133,  0.95705956, -0.42370504, -0.06046475,\n",
      "         0.03607226,  0.44327152,  1.8519398 , -0.5537216 , -0.843139  ,\n",
      "         0.24645126,  0.24265212, -1.4900242 ,  1.1109887 ,  1.7258235 ,\n",
      "        -0.166245  ,  0.41323107, -1.4907241 ,  0.6540058 ,  0.4216535 ,\n",
      "        -1.0468613 , -1.8014473 , -0.44293016, -1.8984069 ,  2.301489  ,\n",
      "        -1.2942278 , -1.5784806 , -1.6267899 , -0.90536225, -0.2235748 ,\n",
      "         1.5371549 , -0.48855275,  0.34369645,  1.2213185 , -1.2027494 ,\n",
      "         3.5225205 ,  0.08491413, -1.3451873 , -1.077015  ,  0.57658696,\n",
      "         1.2083315 ,  1.0454701 , -1.6789527 , -0.7553827 , -0.69525796,\n",
      "        -0.52132505, -0.426317  ,  0.23605515,  1.078453  , -0.8731082 ,\n",
      "        -1.343914  , -0.6376495 ,  1.9007478 ,  1.66745   ,  1.3907775 ,\n",
      "         0.2873992 , -1.1883348 , -1.3352612 ,  0.23345728,  0.19074431,\n",
      "         1.4797108 , -0.62545425, -0.2276353 , -0.10799464,  0.848185  ,\n",
      "        -0.6787511 , -0.29862258,  0.79499924, -0.63146317, -0.28137484,\n",
      "         0.6618937 , -1.496384  , -0.31543052,  2.0465906 ,  0.5877667 ,\n",
      "        -1.4527334 , -1.1700686 , -3.2331583 , -1.224799  ,  1.0821862 ,\n",
      "        -1.5191877 ,  1.4863677 ,  0.70238596,  0.21332683,  1.1146005 ,\n",
      "         0.48020518,  1.7668879 ,  0.73859054,  1.0988587 ,  0.94689095,\n",
      "         1.0402914 ,  0.52648145,  1.0407959 , -2.6166074 ,  0.10113623,\n",
      "         0.27249035,  0.4270672 , -0.82780695, -1.6140225 , -0.95944166,\n",
      "        -2.3145025 ,  0.6955367 , -1.409539  , -0.62584925,  1.5685991 ,\n",
      "         0.09407584,  0.14149542,  0.11693109,  0.27638447,  2.1652007 ,\n",
      "         0.41648394,  1.6286286 ,  1.7228866 , -2.4983513 , -0.4588657 ,\n",
      "         2.1923246 , -0.48814467,  0.10442802, -1.4863169 , -1.0220225 ,\n",
      "        -0.26683432,  1.0800499 , -0.40137738, -2.5129507 ,  1.0493624 ],\n",
      "       dtype=float32)\n",
      " array([-0.3486856 ,  0.6266792 , -1.7451725 ,  0.01966631,  1.7828586 ,\n",
      "        -0.04888417,  0.30024585,  2.1667778 , -0.83479154, -0.30407315,\n",
      "         3.0301814 ,  0.2947648 , -2.912934  ,  0.77104306, -0.04787833,\n",
      "         1.2046227 ,  2.056963  , -0.35931602, -0.26698038, -0.19856817,\n",
      "        -0.1557135 ,  0.16294475, -1.1217906 , -0.22869475, -0.73341674,\n",
      "        -0.48700818, -1.048424  , -0.5416743 , -0.17434876,  0.19239604,\n",
      "         1.2175474 ,  0.7248882 , -0.6391345 , -0.74720466,  0.44927543,\n",
      "        -0.42860326, -0.6025217 ,  0.37699863,  0.26168424,  0.6764976 ,\n",
      "        -0.75084615, -0.43320325,  0.57936215,  0.25235295, -1.189584  ,\n",
      "         0.28035602,  0.749057  , -2.0293844 , -0.14607973,  0.73985285,\n",
      "        -0.69735694,  0.5940568 ,  0.60069287, -2.225065  , -0.7661525 ,\n",
      "         1.0417889 , -0.0217757 ,  0.66336507,  0.91912544,  0.19520548,\n",
      "         1.5793582 , -0.3177979 , -1.5112692 , -1.3998584 ,  1.5112478 ,\n",
      "         0.40264067, -2.5479684 , -1.7023695 , -0.05542788,  1.9615778 ,\n",
      "        -0.37918684, -0.11463079, -0.73637563, -0.64930266,  0.40954623,\n",
      "         0.3954852 , -0.8916999 ,  0.94935966, -0.93607396,  0.08558518,\n",
      "        -1.5354706 , -0.14349231,  1.0802466 ,  0.8741515 ,  0.6622012 ,\n",
      "         1.0669231 ,  0.0804817 , -1.5480925 ,  0.907243  , -0.24346712,\n",
      "        -0.25089893,  0.7623054 ,  2.0615575 , -1.0496978 ,  0.8579852 ,\n",
      "         0.11282373, -0.6677309 , -0.565513  ,  1.5023524 , -0.41712543,\n",
      "         1.5529653 ,  0.3028957 ,  1.2799946 ,  1.6105726 , -0.08904712,\n",
      "         1.7621033 , -1.3361666 , -0.09244626, -0.7404877 , -1.3107599 ,\n",
      "         0.18260309, -0.33290222, -0.41387302, -0.03332731, -0.6742195 ,\n",
      "         1.1126237 , -0.6864958 , -0.77135056, -0.6000159 , -0.6639211 ,\n",
      "        -1.1473565 , -2.061052  ,  0.34369338,  0.45102513,  0.41331604,\n",
      "        -1.6486906 ,  1.6724471 , -0.70537335,  2.09526   , -0.46833792,\n",
      "        -1.2406495 , -0.3790942 ,  1.0736544 , -0.01453805,  0.19317524,\n",
      "        -0.590474  , -1.0277048 ,  0.22023408,  1.905388  , -0.5114168 ,\n",
      "        -1.741883  ,  0.48543966,  0.24664453,  0.7772238 , -0.32874185,\n",
      "        -0.13887747, -2.2799857 ,  0.03393864,  0.51761997,  1.1767379 ,\n",
      "        -0.67390394,  1.0982404 ,  0.27366593,  1.3732244 , -1.3059292 ,\n",
      "        -0.08823936,  1.6778362 , -0.1912157 , -1.3418549 , -1.3779778 ,\n",
      "        -0.8729936 , -2.1421509 ,  0.15964548,  0.82168114, -0.7637522 ,\n",
      "        -0.17029087, -2.1924732 ,  0.9837537 , -0.09465785, -0.29459715,\n",
      "         0.08314168,  0.4487394 ,  0.22254239,  0.5036898 ,  0.77141273,\n",
      "         0.04544984, -0.03657038,  0.09398165, -0.980363  , -0.75485265,\n",
      "        -0.13066019,  1.5347761 ,  0.7108299 , -0.19762772, -0.12615496,\n",
      "         0.32666463,  0.22025263, -0.8141157 ,  1.0688838 ,  1.3483343 ,\n",
      "        -0.24118526, -0.5625416 , -1.0420374 ,  0.25166333,  0.11696365,\n",
      "        -0.54202706, -1.6689205 , -0.20952623, -0.70327336,  1.8061005 ,\n",
      "        -1.2092441 , -0.5451927 , -0.99071145, -0.5962809 ,  0.137043  ,\n",
      "         0.25433636, -0.7354271 ,  0.62631947,  0.35863146, -0.39830452,\n",
      "         1.8045437 , -0.31856155, -0.6820327 ,  0.7217376 ,  0.40015152,\n",
      "         1.0327977 ,  1.13821   , -0.760905  ,  0.14842908, -0.37754574,\n",
      "        -1.1862826 , -0.3880623 ,  0.05453983,  0.7693368 , -1.2109944 ,\n",
      "         0.02626526,  0.1178306 ,  1.5940365 ,  1.0110147 ,  1.0602854 ,\n",
      "         0.8313295 , -1.7597898 , -0.69461864,  0.6172811 ,  0.91090095,\n",
      "         0.8213469 , -0.06362423, -0.0668954 , -0.19252564,  0.58460766,\n",
      "        -0.28171182,  0.10573465,  1.0654358 ,  0.17033336, -0.8804989 ,\n",
      "         0.84538895, -1.0545449 , -0.26640466,  1.1340044 ,  1.0860224 ,\n",
      "        -0.6600915 , -1.2832417 , -3.1219182 , -1.6545371 ,  0.6853879 ,\n",
      "        -1.7407187 ,  1.72406   , -0.31057578, -0.08256374,  0.48200592,\n",
      "         0.5407561 ,  1.9166687 ,  0.5398125 ,  0.97635084,  0.8145322 ,\n",
      "         0.38596317,  0.459361  ,  1.1528751 , -2.9460862 , -0.57892257,\n",
      "        -0.47967243,  0.12673213, -0.40467   , -1.5158254 , -0.13370094,\n",
      "        -2.2333934 ,  0.9093667 , -1.1797775 , -0.6185381 ,  1.3429682 ,\n",
      "         0.25815696,  0.17581871,  0.13838966,  0.06309914,  1.7973963 ,\n",
      "         0.15250842,  1.2752546 ,  1.248996  , -1.6482    , -0.7242922 ,\n",
      "         1.3953866 , -0.5396893 ,  0.1367626 , -0.44207346, -1.8121469 ,\n",
      "        -0.25833252,  0.72741234, -0.13827363, -1.2201518 ,  0.69372624],\n",
      "       dtype=float32)\n",
      " array([-3.4773135e-01,  7.2573858e-01, -1.7822778e+00,  2.7102894e-01,\n",
      "         1.3772594e+00, -8.3604520e-03,  3.8513657e-01,  2.4242442e+00,\n",
      "        -1.4386193e+00,  1.5707219e-01,  2.1962242e+00,  4.7210807e-01,\n",
      "        -2.3698905e+00,  1.4138904e+00,  1.0206436e+00,  4.5470834e-01,\n",
      "         1.9467204e+00, -1.0890553e+00, -2.9821846e-01,  2.9944759e-03,\n",
      "        -1.8297502e-01,  9.0290242e-01, -6.3349336e-01, -7.2887844e-01,\n",
      "        -1.3994281e+00, -7.1354818e-01, -1.0187137e+00, -6.7007548e-01,\n",
      "        -1.6854857e-01,  2.2949190e-01,  1.1301532e+00,  5.5137408e-01,\n",
      "        -5.9266227e-01, -4.4073111e-01,  7.6348442e-01, -3.6859316e-01,\n",
      "        -6.1062515e-01,  4.2986029e-01,  1.2350965e+00,  1.0073279e+00,\n",
      "        -8.0548513e-01, -7.6054043e-01,  7.9475993e-01,  4.7456422e-01,\n",
      "        -9.1498715e-01,  3.9711371e-01,  8.1147212e-01, -2.8838394e+00,\n",
      "        -4.5259544e-01,  9.1028261e-01,  4.2534649e-01,  2.4435660e-02,\n",
      "         8.7281185e-01, -2.0541377e+00, -8.4624434e-01,  7.4276149e-01,\n",
      "         4.5209125e-01,  6.4667678e-01,  1.0800320e+00,  3.3311829e-01,\n",
      "         1.4640747e+00, -4.2629027e-01, -1.6003059e+00, -1.2011741e+00,\n",
      "         6.5158951e-01,  4.7498804e-01, -1.8978475e+00, -1.3255960e+00,\n",
      "        -2.1765816e-01,  1.9363962e+00, -2.2783281e-01, -7.3351216e-01,\n",
      "        -5.3564352e-01, -7.8600311e-01,  3.8838945e-02, -2.0080727e-01,\n",
      "        -7.0820236e-01,  5.6827599e-01, -6.6587061e-02, -2.4354436e-01,\n",
      "        -1.5444593e+00, -5.9406179e-01,  1.0519326e+00,  4.9457523e-01,\n",
      "         1.5008839e+00,  9.0590012e-01,  8.2112211e-01, -1.2722789e+00,\n",
      "         8.5312217e-01, -3.9870080e-01, -4.2304668e-01,  7.3545623e-01,\n",
      "         1.9908075e+00, -1.2620796e+00,  9.5381725e-01, -8.5237145e-01,\n",
      "        -3.3932269e-02, -4.8760140e-01,  1.3841751e+00, -7.9030788e-01,\n",
      "         7.4417460e-01,  4.9096933e-01,  9.4135892e-01,  4.9147308e-01,\n",
      "        -6.7933893e-01,  1.9036685e+00, -1.7870160e+00, -7.1753733e-02,\n",
      "        -1.0282969e+00, -1.1463039e+00, -4.9150282e-01, -1.3227494e+00,\n",
      "         1.9575426e-01, -2.0732279e-01, -2.9581669e-01,  1.4605116e+00,\n",
      "        -1.0730797e+00, -7.7397394e-01, -1.0727869e-01, -9.3600428e-01,\n",
      "        -1.3554409e+00, -1.3456192e+00, -6.9875467e-01,  5.4118586e-01,\n",
      "         2.1895842e-01, -1.2730606e+00,  2.0642087e+00, -1.6789170e-01,\n",
      "         1.8317388e+00,  4.3194603e-02, -1.2705917e+00, -2.0253706e-01,\n",
      "         3.9614287e-01,  1.6584286e-01,  5.3792351e-01, -3.5924822e-01,\n",
      "        -6.7360824e-01, -3.8238445e-01,  1.9884081e+00, -4.7099540e-01,\n",
      "        -1.7107186e+00, -1.0560266e-01,  1.7066610e-01,  8.4159130e-01,\n",
      "        -9.1629457e-01, -3.3654204e-01, -1.5694472e+00,  6.8006915e-01,\n",
      "         5.2470189e-01,  1.0048803e+00, -4.6927637e-01,  1.1067545e+00,\n",
      "         2.4662520e-01,  1.3527533e+00, -6.1268193e-01,  1.8272959e-01,\n",
      "         2.5312362e+00,  9.3452483e-03, -8.9916289e-01, -8.6625838e-01,\n",
      "        -8.3879656e-01, -1.9686116e+00,  2.3024979e-01,  2.9989448e-01,\n",
      "        -9.1956931e-01, -1.2220116e-01, -1.4533544e+00,  1.0335560e+00,\n",
      "        -5.9651440e-01,  3.6432564e-01, -1.8872628e-01,  7.7517653e-01,\n",
      "         1.0043035e+00,  4.2443606e-01,  8.9143038e-01, -5.1369673e-01,\n",
      "        -3.7772262e-01,  4.0467164e-01, -2.7717929e-03, -8.5320735e-01,\n",
      "         1.8664980e-02,  6.8442136e-02,  9.1974282e-01, -4.6475056e-01,\n",
      "        -4.5248032e-01, -6.3280798e-02,  3.8122389e-01, -1.3639764e+00,\n",
      "         1.3580933e+00,  7.1899271e-01, -2.7609435e-01,  4.9492028e-01,\n",
      "        -9.8014784e-01,  1.1084865e-01, -5.9879285e-01, -4.3814218e-01,\n",
      "        -1.2741656e+00, -7.8627266e-02, -8.5693020e-01,  2.4228394e+00,\n",
      "        -1.8797013e+00, -4.5734090e-01, -1.0973750e+00, -9.0484869e-01,\n",
      "        -5.1903194e-01,  8.2303399e-01, -5.4660356e-01,  1.3286150e-01,\n",
      "         4.8439860e-01, -8.4703463e-01,  1.5166388e+00, -3.9536828e-01,\n",
      "        -7.6864409e-01,  1.8882269e-01,  2.7894542e-01,  6.9230098e-01,\n",
      "         1.0539036e+00, -9.9775428e-01, -3.6288738e-01, -3.0939484e-01,\n",
      "        -1.0826250e+00, -7.9585624e-01,  2.1465538e-01,  7.5853646e-01,\n",
      "        -2.8424004e-01, -7.1154529e-01,  1.1642509e-02,  1.0476735e+00,\n",
      "         1.4194071e+00,  1.0090020e+00,  1.0360444e-02, -1.9198784e+00,\n",
      "        -4.4697103e-01, -8.8748969e-02,  1.8378115e-01,  1.0244164e+00,\n",
      "        -6.1835521e-01,  5.7373148e-01,  1.0464426e-01,  7.4008089e-01,\n",
      "        -1.0596504e+00, -8.8236094e-01,  8.8023257e-01, -9.0637669e-02,\n",
      "        -7.5156933e-01,  3.2798740e-01, -1.1930974e+00, -3.3893332e-01,\n",
      "         1.0176831e+00,  8.9285386e-01, -1.2449746e+00, -9.5336670e-01,\n",
      "        -2.5283566e+00, -8.7538683e-01,  6.3450795e-01, -1.4644240e+00,\n",
      "         1.0370386e+00,  9.3222380e-02, -5.3232151e-01,  5.5628854e-01,\n",
      "         7.0394981e-01,  1.8532001e+00,  7.8690588e-02,  3.9508033e-01,\n",
      "         5.7639837e-01,  1.1628592e-01,  4.8030853e-01,  8.5989499e-01,\n",
      "        -2.5446405e+00, -2.9988906e-01, -3.2927211e-02, -3.7484699e-01,\n",
      "        -5.2954531e-01, -6.8918842e-01, -1.3514256e+00, -1.8229560e+00,\n",
      "         6.6199857e-01, -7.6054752e-01, -3.7022215e-01,  1.6267706e+00,\n",
      "        -9.9047685e-01, -1.3050924e-01,  3.2628506e-01,  1.3370945e-01,\n",
      "         2.0915284e+00,  5.0627249e-01,  4.0080291e-01,  5.8537942e-01,\n",
      "        -1.8714565e+00, -5.2490395e-01,  1.7284935e+00, -6.5030348e-01,\n",
      "         1.4105079e+00, -1.2224225e+00, -1.5994524e+00,  2.2695115e-01,\n",
      "         1.2023296e+00, -4.7222111e-01, -1.3938184e+00,  6.9330257e-01],\n",
      "       dtype=float32)\n",
      " ...\n",
      " array([-0.05498353, -0.53463846, -0.421873  ,  0.34666365,  2.9098773 ,\n",
      "        -0.04605905,  0.22122146,  2.9264374 , -1.6559823 , -0.58190006,\n",
      "         3.616629  ,  1.302465  , -3.718868  ,  1.1687497 , -0.85612357,\n",
      "         1.4567419 ,  1.5504261 ,  0.4886201 , -1.7914472 ,  0.58472645,\n",
      "        -0.53790975, -0.744448  , -1.7345545 ,  0.875279  , -0.19319494,\n",
      "        -0.31027213, -0.81790555, -0.07020129, -0.5411359 , -1.0350933 ,\n",
      "         0.70418155,  1.0277541 , -1.3938389 ,  0.01679229,  0.6610596 ,\n",
      "        -0.08743301, -0.62593037, -0.44448063,  1.3534898 ,  1.1524385 ,\n",
      "        -0.8179751 ,  0.05282547, -0.05462209,  1.2479229 , -0.8706783 ,\n",
      "        -0.16239333,  2.42993   , -1.9661258 , -0.58695424,  0.20586923,\n",
      "        -0.37894636,  1.5167662 ,  1.2087669 , -3.1834798 , -1.0670213 ,\n",
      "         1.0926427 , -0.8008314 ,  1.2327812 ,  0.34029603, -0.42920935,\n",
      "         1.4722039 ,  0.23023683, -1.766197  , -1.2486333 ,  1.9841126 ,\n",
      "         1.885445  , -2.55635   , -1.6925433 ,  0.05630621,  3.4933538 ,\n",
      "         0.06622779, -1.3639091 , -0.73403764, -0.42053568, -0.42841405,\n",
      "         1.051195  , -0.75841683,  1.1628742 , -1.6882831 ,  1.3063066 ,\n",
      "        -2.2094474 ,  0.09195448,  0.9948679 ,  1.26491   ,  0.10230093,\n",
      "         0.69723517, -0.2685486 , -2.0897884 ,  1.239306  , -0.16827331,\n",
      "         0.04284609,  0.94956934,  0.7608985 , -0.34961262,  1.7163194 ,\n",
      "        -0.03646279, -0.8465334 , -0.55706453,  0.80112755,  0.844881  ,\n",
      "         2.615879  ,  1.5574625 ,  0.62498313,  1.0197498 , -0.5646874 ,\n",
      "         2.7602696 , -0.8327587 , -1.7292726 , -0.30536616, -3.2783434 ,\n",
      "         1.4408127 , -0.858788  , -0.22304712, -0.17053413,  0.20244622,\n",
      "         2.7253287 , -2.5629811 , -1.6686137 , -0.7618388 , -1.2082965 ,\n",
      "        -1.1003449 , -2.780247  ,  1.424018  ,  0.70728946,  0.06647339,\n",
      "        -2.374464  ,  2.7556536 , -0.14904207,  2.1112657 , -1.8644747 ,\n",
      "        -2.1123035 ,  0.47856528,  0.47452265,  0.5419667 ,  0.28587413,\n",
      "        -0.5875321 , -0.96912587, -0.30959454,  2.8855424 , -0.31888714,\n",
      "        -1.5684266 ,  0.19335766,  0.69892704,  1.5288    , -0.4028132 ,\n",
      "        -0.1522928 , -2.1015055 ,  0.02524361,  0.70819885,  2.0968223 ,\n",
      "        -0.5295215 ,  0.98720634, -0.01736182,  2.090031  , -0.8118873 ,\n",
      "        -0.55853087,  1.9390365 ,  0.83826363, -2.4344165 , -1.7363075 ,\n",
      "        -0.8703308 , -2.2855663 , -1.0963086 ,  1.4464203 , -1.3129424 ,\n",
      "        -0.6453574 , -2.9544897 ,  0.8869745 ,  0.0102806 , -0.4098916 ,\n",
      "         0.05718461,  0.83987445,  0.64560556, -0.53253627,  2.398333  ,\n",
      "         1.1713272 ,  0.13076249, -0.7141491 , -1.0506884 , -1.006601  ,\n",
      "        -0.23202634,  0.8636151 ,  0.6803135 ,  0.85555595,  0.36204597,\n",
      "        -0.395932  ,  0.7027402 , -1.214842  ,  1.9024059 ,  1.5554043 ,\n",
      "        -0.16196062, -1.225585  , -2.125317  , -0.60093457, -0.7112055 ,\n",
      "         0.5048425 , -2.1755562 ,  0.01180688, -0.22801217,  2.446298  ,\n",
      "        -1.9771311 , -0.9730312 , -0.00950779, -1.5664105 ,  2.556259  ,\n",
      "         0.72321457, -0.33169115,  1.3251176 , -0.28248855,  0.23580474,\n",
      "         0.19388393, -0.38870484, -0.23676722,  1.0471808 ,  0.41068608,\n",
      "         1.1151962 ,  1.4288399 , -1.6406451 ,  0.54420274,  0.6185147 ,\n",
      "        -1.0539955 ,  0.08143254, -0.57287544, -0.23195471, -1.1420032 ,\n",
      "        -0.4561507 ,  0.15469773,  1.731249  , -0.5674666 , -0.36167485,\n",
      "         1.2068204 , -2.575663  , -1.6485718 ,  0.65398574,  0.96048206,\n",
      "         1.1689557 , -0.2285852 ,  0.2441286 ,  0.17543443, -0.15294729,\n",
      "        -1.6318625 , -0.64097756,  0.8445985 ,  0.85955775, -0.81210446,\n",
      "         1.7663074 , -1.597695  , -1.1683042 ,  1.0416216 ,  1.200047  ,\n",
      "         0.38037705, -2.817871  , -3.3816078 , -1.6619906 ,  0.23234905,\n",
      "        -2.4363256 ,  1.239122  , -1.7369634 , -0.17203721,  0.6666406 ,\n",
      "        -0.7367366 ,  2.6173797 , -0.7849577 ,  0.99384326,  1.3625085 ,\n",
      "        -0.48030868, -0.06077814,  1.6764059 , -3.0800552 , -1.2415694 ,\n",
      "        -1.4627932 ,  0.36343387, -0.4191208 , -2.0138543 ,  0.59205747,\n",
      "        -1.271638  ,  0.86218333, -2.0958793 , -0.8994804 ,  1.9654498 ,\n",
      "         0.9045305 ,  0.16971405,  0.25126046, -1.2826824 ,  1.6986806 ,\n",
      "         0.14534022,  2.5023797 ,  1.8303126 , -1.67137   , -1.0290772 ,\n",
      "         1.3803564 , -0.15044811, -1.4301234 ,  0.24708517, -1.6853697 ,\n",
      "         0.46822447,  0.973086  , -1.2689718 , -0.79964286,  1.1388913 ],\n",
      "       dtype=float32)\n",
      " array([-5.93950331e-01,  4.13192749e-01, -1.82282019e+00,  6.57029450e-01,\n",
      "         2.57579923e+00,  5.63003540e-01,  1.02946496e+00,  2.53901458e+00,\n",
      "        -1.08634150e+00, -3.16151679e-01,  3.68534184e+00,  1.09153414e+00,\n",
      "        -3.56845665e+00,  1.31085956e+00, -4.39686269e-01,  1.98545027e+00,\n",
      "         1.28089416e+00, -3.42657901e-02, -1.43395877e+00, -1.65250272e-01,\n",
      "         2.16123134e-01,  2.07897380e-01, -1.18884766e+00,  5.61605573e-01,\n",
      "         3.54629427e-01, -7.09294617e-01, -1.71086073e+00, -7.86108255e-01,\n",
      "        -1.21860832e-01,  1.54511169e-01,  1.30484033e+00,  2.06829816e-01,\n",
      "        -3.29385459e-01, -1.39261055e+00, -1.16732433e-01, -1.21168590e+00,\n",
      "        -3.71028483e-01,  6.47486866e-01,  1.49808064e-01,  4.14777130e-01,\n",
      "        -1.01954865e+00, -3.58025730e-02,  9.86766815e-01,  6.82991982e-01,\n",
      "        -7.10153937e-01,  1.83126166e-01,  5.88404834e-01, -1.41643095e+00,\n",
      "        -5.36854267e-01,  2.52727985e-01, -7.90114880e-01,  2.18378735e+00,\n",
      "         2.64827460e-01, -3.37865067e+00, -4.82558429e-01,  7.32620180e-01,\n",
      "        -3.14705044e-01,  5.16990304e-01,  6.71712041e-01, -5.05616292e-02,\n",
      "         7.39675105e-01, -2.44624123e-01, -7.88214803e-01, -1.26184452e+00,\n",
      "         2.29165363e+00,  8.06478322e-01, -2.40443158e+00, -1.38651097e+00,\n",
      "         1.23288000e+00,  2.36427927e+00,  4.63251203e-01, -5.15715957e-01,\n",
      "        -8.45657766e-01,  2.27278516e-01, -2.00297460e-01,  1.34120739e+00,\n",
      "        -1.76249003e+00,  1.40034270e+00, -9.78803277e-01,  3.00463527e-01,\n",
      "        -2.87472153e+00, -3.51553679e-01,  9.03003573e-01,  1.48668981e+00,\n",
      "         5.07445157e-01,  9.43940103e-01, -8.10949743e-01, -2.57523012e+00,\n",
      "         8.86978388e-01, -3.06983560e-01,  2.11613059e-01,  7.11898446e-01,\n",
      "         8.50033641e-01, -1.43301630e+00,  1.11926210e+00, -1.84949905e-01,\n",
      "         1.37598485e-01, -9.21860456e-01,  9.83704150e-01,  5.12991011e-01,\n",
      "         2.66716099e+00,  4.22671407e-01,  1.82437038e+00,  1.90354848e+00,\n",
      "        -1.05043089e+00,  2.58151364e+00, -5.21166384e-01, -1.17091727e+00,\n",
      "        -4.95488644e-01, -2.35366774e+00,  7.96863794e-01, -4.71539736e-01,\n",
      "        -1.30547869e+00, -4.59394753e-01,  7.11753964e-01,  1.90266883e+00,\n",
      "        -1.66077924e+00, -1.80540085e+00, -4.59129840e-01, -4.95963395e-01,\n",
      "        -5.91503680e-01, -2.06777549e+00,  2.04089135e-01,  3.16740364e-01,\n",
      "        -8.45870793e-01, -2.10524201e+00,  1.62158120e+00, -5.34720063e-01,\n",
      "         2.06692362e+00, -8.48464489e-01, -2.01246858e+00, -1.48668557e-01,\n",
      "         2.44586134e+00,  5.16297333e-02,  2.91018426e-01, -9.52023268e-02,\n",
      "        -1.49024284e+00,  6.90973997e-01,  1.89419472e+00, -7.89319992e-01,\n",
      "        -1.50825810e+00, -3.71325910e-01,  2.69648761e-01,  1.14137805e+00,\n",
      "         5.27610302e-01, -2.05719363e-04, -3.40413809e+00, -8.34563255e-01,\n",
      "         4.02945966e-01,  1.36727583e+00, -3.15954655e-01,  8.47767055e-01,\n",
      "         2.33919889e-01,  1.79586804e+00, -8.06264997e-01,  9.03620601e-01,\n",
      "         2.11118770e+00,  1.03766054e-01, -9.92411137e-01, -1.66071284e+00,\n",
      "        -7.81139672e-01, -1.30415821e+00, -3.44015002e-01,  1.53816259e+00,\n",
      "        -8.71773303e-01, -1.21408451e+00, -2.21358705e+00,  9.66614634e-02,\n",
      "         3.04879278e-01, -4.91913587e-01,  5.52366436e-01,  1.26352119e+00,\n",
      "         5.17811835e-01, -3.14201355e-01,  4.57634002e-01,  2.51219571e-01,\n",
      "        -2.27175295e-01,  6.75410479e-02, -1.26224434e+00, -8.36212575e-01,\n",
      "        -8.08361113e-01,  7.65840411e-01,  7.87421942e-01,  7.71277726e-01,\n",
      "        -1.27708381e-02,  4.85142499e-01,  4.29361641e-01, -1.18298635e-01,\n",
      "         1.57648349e+00,  2.17193508e+00,  2.72042751e-01, -1.21117568e+00,\n",
      "        -9.51037705e-01, -5.14858425e-01, -1.54595077e-01, -1.39620394e-01,\n",
      "        -1.88275266e+00,  3.35324323e-04, -1.52967644e+00,  2.04139972e+00,\n",
      "        -1.32383084e+00, -8.08734298e-01, -6.56270146e-01, -1.20037079e+00,\n",
      "         1.90587676e+00,  2.89922714e-01, -1.66296077e+00,  8.22573483e-01,\n",
      "        -5.11398077e-01,  3.09747875e-01,  8.34545016e-01,  2.33522817e-01,\n",
      "        -9.82697546e-01,  5.73945820e-01,  1.01999271e+00,  1.39649832e+00,\n",
      "         1.85598671e+00, -1.50273550e+00,  2.29308922e-02, -6.65841341e-01,\n",
      "        -9.18494523e-01,  9.48557258e-01, -3.51714939e-01,  3.12421620e-02,\n",
      "        -1.44260192e+00, -5.98531604e-01, -3.67999785e-02,  1.67236257e+00,\n",
      "         7.09545195e-01,  5.30055583e-01,  1.69162643e+00, -1.85810733e+00,\n",
      "        -7.70380259e-01,  9.81567800e-01,  1.10516548e+00,  7.78940916e-01,\n",
      "        -6.02334678e-01,  2.28482291e-01, -3.30643535e-01, -9.51831862e-02,\n",
      "        -7.48477101e-01, -3.45751941e-01,  4.15812016e-01,  9.80217680e-02,\n",
      "        -1.86881816e+00,  1.25146532e+00, -1.53049707e+00,  5.41256189e-01,\n",
      "         1.08360600e+00,  1.28915739e+00, -4.19633299e-01, -2.32405353e+00,\n",
      "        -3.26414347e+00, -1.09522069e+00,  1.26292539e+00, -2.35233831e+00,\n",
      "         9.21646476e-01, -9.99945998e-01,  3.77791017e-01,  6.63479388e-01,\n",
      "        -4.29444462e-01,  2.37391472e+00,  1.33184850e+00,  7.70688951e-01,\n",
      "         1.94654930e+00, -3.42560381e-01, -1.55453924e-02,  9.65447545e-01,\n",
      "        -2.38568807e+00, -1.61737025e-01, -8.46633539e-02,  2.75965244e-01,\n",
      "         1.77798152e-01, -1.93551803e+00,  6.96282268e-01, -1.38630116e+00,\n",
      "         1.32612109e+00, -2.99290037e+00, -7.30893552e-01,  1.63874876e+00,\n",
      "         8.01576734e-01,  1.06248200e-01,  9.42482293e-01, -4.37596530e-01,\n",
      "         1.41181993e+00,  1.68071464e-01,  1.36786222e+00,  1.12811720e+00,\n",
      "        -1.63541639e+00, -2.66591161e-01,  1.33899820e+00, -7.94147789e-01,\n",
      "        -1.34200823e+00,  2.03362748e-01, -1.45301914e+00,  4.00453717e-01,\n",
      "         5.80986559e-01, -9.41613436e-01, -1.58934796e+00,  1.52053356e+00],\n",
      "       dtype=float32)\n",
      " array([-1.62370920e-01,  1.26296270e+00, -1.87143183e+00,  8.13613296e-01,\n",
      "         1.34241581e+00,  6.26007497e-01,  3.63479525e-01,  3.14333820e+00,\n",
      "        -2.22281003e+00,  2.58690298e-01,  2.73290229e+00,  9.37467933e-01,\n",
      "        -2.94515872e+00,  1.29140937e+00,  2.92481214e-01,  1.21613228e+00,\n",
      "         1.34222209e+00, -7.09480286e-01, -7.06718341e-02,  9.64246094e-02,\n",
      "         4.30091113e-01,  2.61243135e-01, -8.92136618e-02, -1.18093240e+00,\n",
      "        -2.63263494e-01, -3.90539557e-01, -1.38956845e+00, -1.01420689e+00,\n",
      "         2.35517859e-01,  1.16011047e+00,  7.72504866e-01,  2.13089392e-01,\n",
      "        -2.18099892e-01, -1.48510170e+00, -1.49526641e-01, -1.38789773e+00,\n",
      "        -7.50071466e-01,  1.35699141e+00,  1.31118119e+00,  9.25710917e-01,\n",
      "        -8.73736203e-01, -1.55772597e-01,  1.42130172e+00,  1.32297486e-01,\n",
      "        -7.12361217e-01,  7.84258842e-01,  7.44500160e-01, -1.87187600e+00,\n",
      "        -4.18824553e-01,  8.77940536e-01, -6.42224014e-01,  1.28088748e+00,\n",
      "        -3.39850201e-04, -2.38470197e+00, -1.08237004e+00,  5.66939354e-01,\n",
      "         4.16161805e-01,  8.34830701e-01,  1.42489076e+00,  9.05584931e-01,\n",
      "         1.03231096e+00, -2.14343280e-01, -4.52496678e-01, -1.50647688e+00,\n",
      "         1.28826833e+00,  9.72214162e-01, -2.44723558e+00, -1.41836357e+00,\n",
      "        -2.16491833e-01,  2.72157836e+00,  5.96845567e-01, -5.65716863e-01,\n",
      "        -1.76383585e-01,  1.39776781e-01,  7.93656334e-02,  6.59720004e-01,\n",
      "        -1.11385500e+00,  7.64411509e-01, -1.16171217e+00, -2.63303697e-01,\n",
      "        -2.90893483e+00, -5.11959910e-01,  1.57454538e+00,  7.75282919e-01,\n",
      "         1.42376274e-02,  6.17951930e-01,  4.36006546e-01, -1.72943866e+00,\n",
      "         5.67255318e-01,  2.45659575e-01,  3.57897848e-01,  8.57097149e-01,\n",
      "         8.88097882e-01, -1.38345301e+00,  1.39421487e+00, -9.13985312e-01,\n",
      "         7.20639348e-01, -8.37247431e-01,  8.89515102e-01, -3.66292149e-01,\n",
      "         9.93496656e-01,  5.02524018e-01,  1.82700646e+00,  1.42908370e+00,\n",
      "        -7.43210435e-01,  1.97618484e+00, -1.87353325e+00, -9.87045884e-01,\n",
      "        -1.02026820e+00, -1.47161591e+00, -4.47779030e-01, -1.32779217e+00,\n",
      "        -2.08487421e-01, -8.71109217e-02, -3.56274426e-01,  1.83841288e+00,\n",
      "        -1.68593860e+00, -1.61751270e+00,  1.84473142e-01,  1.80927962e-01,\n",
      "        -7.79831052e-01, -2.00719309e+00, -6.61994517e-01,  1.64222524e-01,\n",
      "        -1.06779182e+00, -1.01739085e+00,  1.62495542e+00, -2.97054529e-01,\n",
      "         1.41170168e+00, -3.88241023e-01, -8.33992541e-01, -3.86207342e-01,\n",
      "         1.67267621e+00,  1.16059668e-01,  2.34586775e-01, -3.68964344e-01,\n",
      "        -1.90193093e+00, -1.25682846e-01,  2.20643282e+00, -1.04443109e+00,\n",
      "        -1.29470229e+00, -3.10032845e-01,  2.87960500e-01,  7.82577932e-01,\n",
      "        -5.42373002e-01, -2.21568465e-01, -2.61120200e+00, -6.43708825e-01,\n",
      "        -9.76214372e-03,  1.45831239e+00, -1.10781693e+00,  1.00722718e+00,\n",
      "         4.42196548e-01,  1.83471322e+00, -4.23131734e-01,  1.00070739e+00,\n",
      "         2.41155410e+00,  4.68023241e-01, -7.89079845e-01, -4.43817019e-01,\n",
      "        -5.82010269e-01, -1.63571513e+00, -4.36246127e-01, -1.98045149e-04,\n",
      "        -8.63044381e-01, -3.85125339e-01, -1.74126530e+00,  3.78805965e-01,\n",
      "        -3.71009260e-01, -1.52400151e-01,  5.57564855e-01,  1.30125964e+00,\n",
      "         9.57141280e-01, -3.14157099e-01,  6.23499811e-01, -4.09768462e-01,\n",
      "        -7.88957953e-01,  5.58331072e-01, -5.49747050e-01, -6.49484634e-01,\n",
      "         4.49071005e-02,  6.03127480e-01,  1.01933241e+00,  7.97333539e-01,\n",
      "        -4.95888084e-01, -7.90177956e-02,  6.43965602e-01, -9.87453759e-01,\n",
      "         1.65741003e+00,  1.61210108e+00, -2.41691824e-02, -1.19892828e-01,\n",
      "        -1.59829116e+00, -1.14708848e-01, -1.01306832e+00, -1.53596067e+00,\n",
      "        -1.33887327e+00,  3.99422437e-01, -1.65425074e+00,  2.65822339e+00,\n",
      "        -1.81683493e+00, -4.12906319e-01, -7.82859683e-01, -1.17427468e+00,\n",
      "         6.55498430e-02,  3.66743475e-01, -5.43226779e-01, -1.61869034e-01,\n",
      "        -5.00124872e-01,  3.48923326e-01,  7.33650088e-01,  3.02220359e-02,\n",
      "        -1.08361113e+00,  3.03468347e-01,  5.37755430e-01,  1.12386382e+00,\n",
      "         1.54676521e+00, -1.69642940e-01, -7.95667648e-01, -3.09535693e-02,\n",
      "        -1.27186060e+00, -5.45965552e-01, -1.10832050e-01,  6.20459259e-01,\n",
      "        -9.73168135e-01, -4.28442389e-01, -5.00467643e-02,  1.97390676e+00,\n",
      "         1.15455115e+00,  1.17919791e+00,  7.06174195e-01, -2.39933228e+00,\n",
      "        -6.29577935e-01,  2.94802636e-01,  1.18987806e-01,  2.15225652e-01,\n",
      "        -9.54254210e-01,  8.30363393e-01,  2.70482600e-01, -4.79228497e-02,\n",
      "        -5.00508010e-01, -2.23812371e-01,  6.47479594e-01,  4.22144949e-01,\n",
      "        -9.94103968e-01,  7.73731709e-01, -1.26723969e+00,  4.18632448e-01,\n",
      "         7.82963097e-01,  1.08183539e+00, -4.49922293e-01, -1.20084083e+00,\n",
      "        -2.94529486e+00, -3.05831760e-01,  8.18476617e-01, -2.17814469e+00,\n",
      "         7.43372798e-01, -2.04726215e-02,  3.20029967e-02,  1.23319256e+00,\n",
      "        -2.56921113e-01,  1.39993834e+00,  4.07663256e-01,  7.48076975e-01,\n",
      "         1.06464946e+00,  9.40713957e-02, -8.27094793e-01,  1.05007148e+00,\n",
      "        -3.06377339e+00, -5.55722058e-01,  5.62102675e-01, -6.99702740e-01,\n",
      "        -2.28596792e-01, -1.27578211e+00,  1.29659967e-02, -1.79532695e+00,\n",
      "         7.44799733e-01, -1.79010010e+00, -1.34420204e+00,  2.38548994e+00,\n",
      "        -7.69214869e-01, -1.83758542e-01, -2.31778190e-01, -3.70674074e-01,\n",
      "         2.15529108e+00,  2.09958926e-01,  8.57715786e-01,  3.01848888e-01,\n",
      "        -1.46170068e+00, -1.36968955e-01,  1.25503373e+00, -4.33614016e-01,\n",
      "        -2.23888949e-01,  4.01858032e-01, -1.19262958e+00,  3.34141433e-01,\n",
      "         5.36140680e-01,  4.23657566e-01, -1.68016005e+00,  9.33150649e-01],\n",
      "       dtype=float32)                                                       ]\n"
     ]
    }
   ],
   "source": [
    "print(\"cleaned_text_vector\")\n",
    "print(df['cleaned_text_vector'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa9f5c8fab09840",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-16T02:04:31.612012400Z",
     "start_time": "2023-08-16T02:04:31.534455100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 51543\n",
      "Validation set: 6443\n",
      "Test set: 6443\n"
     ]
    }
   ],
   "source": [
    "# Data Split Variation Nr.1 (only Vectors of original texts)\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = split_data(df,['original_text_vector'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabcfbb31c5ab95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Split Variation Nr.2 (Number of Nationalities or Religious or Political Groups + Vader Compound + Vectors of original texts)\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = split_data(df,['norp_count', 'vader_compound', 'original_text_vector'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371d1c69df9eeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Split Variation Nr.3 (Number of Geopolitical Entities + Vader Compound + Vectors of original texts)\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = split_data(df,['gpe_count', 'vader_compound', 'original_text_vector'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11f9fbfa5430bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Split Variation Nr.4 (only Vectors of pre-processed texts)\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = split_data(df,['cleaned_text_vector'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cbb6a7b4200ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Split Variation Nr.5 (Number of Nationalities or Religious or Political Groups + Vader Compound + Vectors of pre-preocessed texts)\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = split_data(df,['norp_count', 'vader_compound', 'cleaned_text_vector'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96debebf13b7171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Split Variation Nr.6 (Number of Geopolitical Entities + Vader Compound + Vectors of pre-processed texts)\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = split_data(df,['gpe_count', 'vader_compound', 'cleaned_text_vector'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf83f9bedd3542d",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a65320917d0079e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-16T02:07:39.563490700Z",
     "start_time": "2023-08-16T02:07:39.547265900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b434f39c527c23a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-16T02:07:41.329380200Z",
     "start_time": "2023-08-16T02:07:41.296813300Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_val, X_test = map(torch.tensor, (X_train, X_val, X_test))\n",
    "y_train, y_val, y_test = map(torch.tensor, (y_train, y_val, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aeb33c2e133880b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-16T02:07:42.954231600Z",
     "start_time": "2023-08-16T02:07:42.934567500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([51543, 300])\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be19ae5d8024bd7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-16T02:07:43.521142300Z",
     "start_time": "2023-08-16T02:07:43.493999500Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = X_train.unsqueeze(1)  # Adds a sequence dimension\n",
    "X_val = X_val.unsqueeze(1)\n",
    "X_test = X_test.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6953a8df64e4621",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-16T02:07:44.134620500Z",
     "start_time": "2023-08-16T02:07:44.119478100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([51543, 1, 300])\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1faec96ce2d042e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-16T02:07:44.656371600Z",
     "start_time": "2023-08-16T02:07:44.561466500Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_val, X_test = X_train.to(device), X_val.to(device), X_test.to(device)\n",
    "y_train, y_val, y_test = y_train.to(device), y_val.to(device), y_test.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9cd4dca249df8f39",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-16T02:07:45.531941300Z",
     "start_time": "2023-08-16T02:07:45.516316400Z"
    }
   },
   "outputs": [],
   "source": [
    "class NewsClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, bidirectional=False):\n",
    "        super(NewsClassifier, self).__init__()\n",
    "\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, bidirectional=bidirectional, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim * (2 if bidirectional else 1), output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        final_out = lstm_out[:, -1, :]\n",
    "        out = self.fc(final_out)\n",
    "        return self.sigmoid(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af4c718f32c29e9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-16T02:07:47.023395700Z",
     "start_time": "2023-08-16T02:07:46.595877300Z"
    }
   },
   "outputs": [],
   "source": [
    "input_dim = X_train.shape[2]  # Number of input features\n",
    "hidden_dim = 50\n",
    "output_dim = 1  # Binary classification\n",
    "\n",
    "lstm = NewsClassifier(input_dim, hidden_dim, output_dim, bidirectional=False).to(device)\n",
    "bi_lstm = NewsClassifier(input_dim, hidden_dim, output_dim, bidirectional=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cd6af8ea47131fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-16T02:07:47.560288600Z",
     "start_time": "2023-08-16T02:07:47.543647500Z"
    }
   },
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "455126e5d8b7c03d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-16T02:13:24.263547400Z",
     "start_time": "2023-08-16T02:13:24.253902300Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, criterion, X_train, y_train, X_val, y_val, num_epochs=2000):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    prev_val_loss = float('inf')\n",
    "    increasing_epochs = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train.float())\n",
    "        loss = criterion(outputs.squeeze(), y_train.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Validation Loss\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_val.float())\n",
    "            val_loss = criterion(val_outputs.squeeze(), y_val.float())\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "        val_losses.append(val_loss.item())\n",
    "\n",
    "        print(f\"Epoch: {epoch+1}/{num_epochs}, Train Loss: {loss.item()}, Validation Loss: {val_loss.item()}\")\n",
    "\n",
    "        # Check for increasing validation loss\n",
    "        if val_loss.item() > prev_val_loss:\n",
    "            increasing_epochs += 1\n",
    "        else:\n",
    "            increasing_epochs = 0  # Reset the counter\n",
    "\n",
    "        prev_val_loss = val_loss.item()  # Update the previous validation loss\n",
    "\n",
    "        # Early stopping condition\n",
    "        if increasing_epochs >= 3:\n",
    "            print(f\"\\nEarly stopping at epoch {epoch+1} due to validation loss increasing for 3 consecutive epochs.\")\n",
    "            break\n",
    "    return model, train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d58d87281f3e60bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-16T02:13:25.886963900Z",
     "start_time": "2023-08-16T02:13:25.867162500Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(X_test.float())\n",
    "        test_predictions = (test_outputs.squeeze() > 0.5).float()\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test.cpu(), test_predictions.cpu())\n",
    "    report = classification_report(y_test.cpu(), test_predictions.cpu(), output_dict=True)\n",
    "    conf_matrix = confusion_matrix(y_test.cpu(), test_predictions.cpu())\n",
    "\n",
    "    return accuracy, report, conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9c93676d798f3ba8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-16T02:13:27.306607Z",
     "start_time": "2023-08-16T02:13:27.286727Z"
    }
   },
   "outputs": [],
   "source": [
    "def print_evaluation_metrics(accuracy, report, conf_matrix, train_losses, val_losses, model_name):\n",
    "    print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
    "    precision_0, precision_1 = report['0']['precision'], report['1']['precision']\n",
    "    recall_0, recall_1 = report['0']['recall'], report['1']['recall']\n",
    "    f1_0, f1_1 = report['0']['f1-score'], report['1']['f1-score']\n",
    "    print(f\"\\nClass 0 - Precision: {precision_0:.4f}, Recall: {recall_0:.4f}, F1 Score: {f1_0:.4f}\")\n",
    "    print(f\"Class 1 - Precision: {precision_1:.4f}, Recall: {recall_1:.4f}, F1 Score: {f1_1:.4f}\")\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(conf_matrix)\n",
    "    cm_display = ConfusionMatrixDisplay(conf_matrix).plot()\n",
    "    plt.title(\"Confusion Matrix - \" + model_name)\n",
    "    plt.show()\n",
    "    # Plotting the loss curves\n",
    "    plt.figure()\n",
    "    plt.plot(train_losses, label=\"Training Loss\")\n",
    "    plt.plot(val_losses, label=\"Validation Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(f\"{model_name} - Loss Curves\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "57de5ce7996004be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-16T02:13:47.372870Z",
     "start_time": "2023-08-16T02:13:28.563103800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2000, Train Loss: 0.6844332814216614, Validation Loss: 0.6736944317817688\n",
      "Epoch: 2/2000, Train Loss: 0.6731305122375488, Validation Loss: 0.6627963185310364\n",
      "Epoch: 3/2000, Train Loss: 0.6621474623680115, Validation Loss: 0.6526367664337158\n",
      "Epoch: 4/2000, Train Loss: 0.6519083976745605, Validation Loss: 0.6429540514945984\n",
      "Epoch: 5/2000, Train Loss: 0.6421332955360413, Validation Loss: 0.6331369280815125\n",
      "Epoch: 6/2000, Train Loss: 0.6321753263473511, Validation Loss: 0.6226779818534851\n",
      "Epoch: 7/2000, Train Loss: 0.6215511560440063, Validation Loss: 0.6113573908805847\n",
      "Epoch: 8/2000, Train Loss: 0.610100269317627, Validation Loss: 0.5995379090309143\n",
      "Epoch: 9/2000, Train Loss: 0.5982277393341064, Validation Loss: 0.5880106091499329\n",
      "Epoch: 10/2000, Train Loss: 0.5867345929145813, Validation Loss: 0.5772975087165833\n",
      "Epoch: 11/2000, Train Loss: 0.5761063694953918, Validation Loss: 0.5670292973518372\n",
      "Epoch: 12/2000, Train Loss: 0.5659083724021912, Validation Loss: 0.5567367076873779\n",
      "Epoch: 13/2000, Train Loss: 0.5556557774543762, Validation Loss: 0.5465772747993469\n",
      "Epoch: 14/2000, Train Loss: 0.5455145835876465, Validation Loss: 0.5366541743278503\n",
      "Epoch: 15/2000, Train Loss: 0.5355989336967468, Validation Loss: 0.5270475745201111\n",
      "Epoch: 16/2000, Train Loss: 0.5259954333305359, Validation Loss: 0.5179340839385986\n",
      "Epoch: 17/2000, Train Loss: 0.5168806314468384, Validation Loss: 0.5092793703079224\n",
      "Epoch: 18/2000, Train Loss: 0.5082280039787292, Validation Loss: 0.500898003578186\n",
      "Epoch: 19/2000, Train Loss: 0.499870240688324, Validation Loss: 0.49262571334838867\n",
      "Epoch: 20/2000, Train Loss: 0.49166983366012573, Validation Loss: 0.48443901538848877\n",
      "Epoch: 21/2000, Train Loss: 0.48360827565193176, Validation Loss: 0.47651657462120056\n",
      "Epoch: 22/2000, Train Loss: 0.4758298397064209, Validation Loss: 0.46902474761009216\n",
      "Epoch: 23/2000, Train Loss: 0.46846020221710205, Validation Loss: 0.4619733393192291\n",
      "Epoch: 24/2000, Train Loss: 0.4614975154399872, Validation Loss: 0.4552311897277832\n",
      "Epoch: 25/2000, Train Loss: 0.4548214077949524, Validation Loss: 0.4486132562160492\n",
      "Epoch: 26/2000, Train Loss: 0.4482560455799103, Validation Loss: 0.44205403327941895\n",
      "Epoch: 27/2000, Train Loss: 0.4417327642440796, Validation Loss: 0.4355909824371338\n",
      "Epoch: 28/2000, Train Loss: 0.43528592586517334, Validation Loss: 0.42928940057754517\n",
      "Epoch: 29/2000, Train Loss: 0.4289875626564026, Validation Loss: 0.42318010330200195\n",
      "Epoch: 30/2000, Train Loss: 0.4228762984275818, Validation Loss: 0.4172690510749817\n",
      "Epoch: 31/2000, Train Loss: 0.41696229577064514, Validation Loss: 0.411541610956192\n",
      "Epoch: 32/2000, Train Loss: 0.4112352430820465, Validation Loss: 0.40596309304237366\n",
      "Epoch: 33/2000, Train Loss: 0.40566733479499817, Validation Loss: 0.40049150586128235\n",
      "Epoch: 34/2000, Train Loss: 0.4002228081226349, Validation Loss: 0.39510390162467957\n",
      "Epoch: 35/2000, Train Loss: 0.39487791061401367, Validation Loss: 0.3898327946662903\n",
      "Epoch: 36/2000, Train Loss: 0.38965532183647156, Validation Loss: 0.38472092151641846\n",
      "Epoch: 37/2000, Train Loss: 0.3845879137516022, Validation Loss: 0.37978559732437134\n",
      "Epoch: 38/2000, Train Loss: 0.3796897530555725, Validation Loss: 0.37500739097595215\n",
      "Epoch: 39/2000, Train Loss: 0.3749392330646515, Validation Loss: 0.37036243081092834\n",
      "Epoch: 40/2000, Train Loss: 0.37030690908432007, Validation Loss: 0.3658292293548584\n",
      "Epoch: 41/2000, Train Loss: 0.3657669126987457, Validation Loss: 0.3613956570625305\n",
      "Epoch: 42/2000, Train Loss: 0.3613083064556122, Validation Loss: 0.35706546902656555\n",
      "Epoch: 43/2000, Train Loss: 0.35693955421447754, Validation Loss: 0.3528641164302826\n",
      "Epoch: 44/2000, Train Loss: 0.3526899814605713, Validation Loss: 0.34881365299224854\n",
      "Epoch: 45/2000, Train Loss: 0.34858494997024536, Validation Loss: 0.3449051082134247\n",
      "Epoch: 46/2000, Train Loss: 0.3446192443370819, Validation Loss: 0.3410993218421936\n",
      "Epoch: 47/2000, Train Loss: 0.34075841307640076, Validation Loss: 0.3373591899871826\n",
      "Epoch: 48/2000, Train Loss: 0.33697107434272766, Validation Loss: 0.33368852734565735\n",
      "Epoch: 49/2000, Train Loss: 0.3332626223564148, Validation Loss: 0.3301170766353607\n",
      "Epoch: 50/2000, Train Loss: 0.3296583294868469, Validation Loss: 0.3266619145870209\n",
      "Epoch: 51/2000, Train Loss: 0.3261701166629791, Validation Loss: 0.32331645488739014\n",
      "Epoch: 52/2000, Train Loss: 0.32278692722320557, Validation Loss: 0.3200699985027313\n",
      "Epoch: 53/2000, Train Loss: 0.3194938600063324, Validation Loss: 0.31691688299179077\n",
      "Epoch: 54/2000, Train Loss: 0.3162866234779358, Validation Loss: 0.3138542175292969\n",
      "Epoch: 55/2000, Train Loss: 0.31316646933555603, Validation Loss: 0.3108808100223541\n",
      "Epoch: 56/2000, Train Loss: 0.3101331889629364, Validation Loss: 0.30799195170402527\n",
      "Epoch: 57/2000, Train Loss: 0.3071818947792053, Validation Loss: 0.305182546377182\n",
      "Epoch: 58/2000, Train Loss: 0.3043048679828644, Validation Loss: 0.3024512529373169\n",
      "Epoch: 59/2000, Train Loss: 0.3014988601207733, Validation Loss: 0.2997923493385315\n",
      "Epoch: 60/2000, Train Loss: 0.2987665832042694, Validation Loss: 0.29719799757003784\n",
      "Epoch: 61/2000, Train Loss: 0.2961091101169586, Validation Loss: 0.2946634590625763\n",
      "Epoch: 62/2000, Train Loss: 0.29352161288261414, Validation Loss: 0.2921809256076813\n",
      "Epoch: 63/2000, Train Loss: 0.2909957766532898, Validation Loss: 0.28974947333335876\n",
      "Epoch: 64/2000, Train Loss: 0.28852730989456177, Validation Loss: 0.2873758375644684\n",
      "Epoch: 65/2000, Train Loss: 0.286119282245636, Validation Loss: 0.2850632667541504\n",
      "Epoch: 66/2000, Train Loss: 0.2837772071361542, Validation Loss: 0.2828133702278137\n",
      "Epoch: 67/2000, Train Loss: 0.28150075674057007, Validation Loss: 0.280622661113739\n",
      "Epoch: 68/2000, Train Loss: 0.2792828381061554, Validation Loss: 0.27848565578460693\n",
      "Epoch: 69/2000, Train Loss: 0.27711760997772217, Validation Loss: 0.2764061987400055\n",
      "Epoch: 70/2000, Train Loss: 0.2750045657157898, Validation Loss: 0.2743850350379944\n",
      "Epoch: 71/2000, Train Loss: 0.2729451358318329, Validation Loss: 0.2724182903766632\n",
      "Epoch: 72/2000, Train Loss: 0.2709389328956604, Validation Loss: 0.27050134539604187\n",
      "Epoch: 73/2000, Train Loss: 0.26898330450057983, Validation Loss: 0.26862651109695435\n",
      "Epoch: 74/2000, Train Loss: 0.2670750617980957, Validation Loss: 0.2667941153049469\n",
      "Epoch: 75/2000, Train Loss: 0.2652123272418976, Validation Loss: 0.2650046646595001\n",
      "Epoch: 76/2000, Train Loss: 0.26339417695999146, Validation Loss: 0.2632579207420349\n",
      "Epoch: 77/2000, Train Loss: 0.26161956787109375, Validation Loss: 0.2615545094013214\n",
      "Epoch: 78/2000, Train Loss: 0.2598867416381836, Validation Loss: 0.259891539812088\n",
      "Epoch: 79/2000, Train Loss: 0.2581935524940491, Validation Loss: 0.258271187543869\n",
      "Epoch: 80/2000, Train Loss: 0.2565388083457947, Validation Loss: 0.25669074058532715\n",
      "Epoch: 81/2000, Train Loss: 0.2549218237400055, Validation Loss: 0.2551494240760803\n",
      "Epoch: 82/2000, Train Loss: 0.25334104895591736, Validation Loss: 0.25364330410957336\n",
      "Epoch: 83/2000, Train Loss: 0.2517947554588318, Validation Loss: 0.2521713674068451\n",
      "Epoch: 84/2000, Train Loss: 0.25028151273727417, Validation Loss: 0.2507314383983612\n",
      "Epoch: 85/2000, Train Loss: 0.24880044162273407, Validation Loss: 0.24932238459587097\n",
      "Epoch: 86/2000, Train Loss: 0.24735060334205627, Validation Loss: 0.24794332683086395\n",
      "Epoch: 87/2000, Train Loss: 0.24593104422092438, Validation Loss: 0.24659359455108643\n",
      "Epoch: 88/2000, Train Loss: 0.24454078078269958, Validation Loss: 0.24527284502983093\n",
      "Epoch: 89/2000, Train Loss: 0.2431788295507431, Validation Loss: 0.24397845566272736\n",
      "Epoch: 90/2000, Train Loss: 0.24184419214725494, Validation Loss: 0.24271118640899658\n",
      "Epoch: 91/2000, Train Loss: 0.24053597450256348, Validation Loss: 0.24146762490272522\n",
      "Epoch: 92/2000, Train Loss: 0.23925358057022095, Validation Loss: 0.2402534782886505\n",
      "Epoch: 93/2000, Train Loss: 0.23799625039100647, Validation Loss: 0.23905786871910095\n",
      "Epoch: 94/2000, Train Loss: 0.23676373064517975, Validation Loss: 0.23790249228477478\n",
      "Epoch: 95/2000, Train Loss: 0.23555748164653778, Validation Loss: 0.2367558777332306\n",
      "Epoch: 96/2000, Train Loss: 0.23438675701618195, Validation Loss: 0.2357289344072342\n",
      "Epoch: 97/2000, Train Loss: 0.23328207433223724, Validation Loss: 0.23475226759910583\n",
      "Epoch: 98/2000, Train Loss: 0.23233051598072052, Validation Loss: 0.23403669893741608\n",
      "Epoch: 99/2000, Train Loss: 0.23147530853748322, Validation Loss: 0.23290182650089264\n",
      "Epoch: 100/2000, Train Loss: 0.23041601479053497, Validation Loss: 0.2315753996372223\n",
      "Epoch: 101/2000, Train Loss: 0.22897472977638245, Validation Loss: 0.23063646256923676\n",
      "Epoch: 102/2000, Train Loss: 0.22799226641654968, Validation Loss: 0.22990109026432037\n",
      "Epoch: 103/2000, Train Loss: 0.22729289531707764, Validation Loss: 0.22885961830615997\n",
      "Epoch: 104/2000, Train Loss: 0.22612662613391876, Validation Loss: 0.22778739035129547\n",
      "Epoch: 105/2000, Train Loss: 0.22504396736621857, Validation Loss: 0.22706563770771027\n",
      "Epoch: 106/2000, Train Loss: 0.22434042394161224, Validation Loss: 0.22620336711406708\n",
      "Epoch: 107/2000, Train Loss: 0.2233627438545227, Validation Loss: 0.22516965866088867\n",
      "Epoch: 108/2000, Train Loss: 0.2223273068666458, Validation Loss: 0.22443966567516327\n",
      "Epoch: 109/2000, Train Loss: 0.2215985506772995, Validation Loss: 0.2236795574426651\n",
      "Epoch: 110/2000, Train Loss: 0.22072628140449524, Validation Loss: 0.22271287441253662\n",
      "Epoch: 111/2000, Train Loss: 0.21976064145565033, Validation Loss: 0.22197996079921722\n",
      "Epoch: 112/2000, Train Loss: 0.21902017295360565, Validation Loss: 0.22128023207187653\n",
      "Epoch: 113/2000, Train Loss: 0.2182181030511856, Validation Loss: 0.22037415206432343\n",
      "Epoch: 114/2000, Train Loss: 0.21731795370578766, Validation Loss: 0.21964885294437408\n",
      "Epoch: 115/2000, Train Loss: 0.21657462418079376, Validation Loss: 0.21899807453155518\n",
      "Epoch: 116/2000, Train Loss: 0.2158258557319641, Validation Loss: 0.2181515395641327\n",
      "Epoch: 117/2000, Train Loss: 0.21498526632785797, Validation Loss: 0.21744106709957123\n",
      "Epoch: 118/2000, Train Loss: 0.2142431139945984, Validation Loss: 0.2168232947587967\n",
      "Epoch: 119/2000, Train Loss: 0.21353580057621002, Validation Loss: 0.2160273939371109\n",
      "Epoch: 120/2000, Train Loss: 0.21275220811367035, Validation Loss: 0.21533161401748657\n",
      "Epoch: 121/2000, Train Loss: 0.21201445162296295, Validation Loss: 0.2147289216518402\n",
      "Epoch: 122/2000, Train Loss: 0.21133562922477722, Validation Loss: 0.21398715674877167\n",
      "Epoch: 123/2000, Train Loss: 0.21060802042484283, Validation Loss: 0.21331968903541565\n",
      "Epoch: 124/2000, Train Loss: 0.20988190174102783, Validation Loss: 0.212717667222023\n",
      "Epoch: 125/2000, Train Loss: 0.20921719074249268, Validation Loss: 0.21203121542930603\n",
      "Epoch: 126/2000, Train Loss: 0.20854097604751587, Validation Loss: 0.21140208840370178\n",
      "Epoch: 127/2000, Train Loss: 0.2078397125005722, Validation Loss: 0.2107822746038437\n",
      "Epoch: 128/2000, Train Loss: 0.20717862248420715, Validation Loss: 0.21014264225959778\n",
      "Epoch: 129/2000, Train Loss: 0.20653995871543884, Validation Loss: 0.20956048369407654\n",
      "Epoch: 130/2000, Train Loss: 0.20587821304798126, Validation Loss: 0.20892517268657684\n",
      "Epoch: 131/2000, Train Loss: 0.20522339642047882, Validation Loss: 0.2083227038383484\n",
      "Epoch: 132/2000, Train Loss: 0.2046014368534088, Validation Loss: 0.20777884125709534\n",
      "Epoch: 133/2000, Train Loss: 0.2039816975593567, Validation Loss: 0.20714892446994781\n",
      "Epoch: 134/2000, Train Loss: 0.20335040986537933, Validation Loss: 0.2065756767988205\n",
      "Epoch: 135/2000, Train Loss: 0.20273324847221375, Validation Loss: 0.206035315990448\n",
      "Epoch: 136/2000, Train Loss: 0.2021375447511673, Validation Loss: 0.20543789863586426\n",
      "Epoch: 137/2000, Train Loss: 0.20154213905334473, Validation Loss: 0.2049017995595932\n",
      "Epoch: 138/2000, Train Loss: 0.20094159245491028, Validation Loss: 0.20433832705020905\n",
      "Epoch: 139/2000, Train Loss: 0.2003525346517563, Validation Loss: 0.20378151535987854\n",
      "Epoch: 140/2000, Train Loss: 0.19977931678295135, Validation Loss: 0.20327630639076233\n",
      "Epoch: 141/2000, Train Loss: 0.19920943677425385, Validation Loss: 0.20270735025405884\n",
      "Epoch: 142/2000, Train Loss: 0.1986377090215683, Validation Loss: 0.20218916237354279\n",
      "Epoch: 143/2000, Train Loss: 0.19807229936122894, Validation Loss: 0.2016761749982834\n",
      "Epoch: 144/2000, Train Loss: 0.19751916825771332, Validation Loss: 0.201140359044075\n",
      "Epoch: 145/2000, Train Loss: 0.19697368144989014, Validation Loss: 0.20065900683403015\n",
      "Epoch: 146/2000, Train Loss: 0.19642950594425201, Validation Loss: 0.20012681186199188\n",
      "Epoch: 147/2000, Train Loss: 0.19588759541511536, Validation Loss: 0.19963352382183075\n",
      "Epoch: 148/2000, Train Loss: 0.19535282254219055, Validation Loss: 0.19914843142032623\n",
      "Epoch: 149/2000, Train Loss: 0.1948269009590149, Validation Loss: 0.198640838265419\n",
      "Epoch: 150/2000, Train Loss: 0.19430701434612274, Validation Loss: 0.1981838047504425\n",
      "Epoch: 151/2000, Train Loss: 0.19379007816314697, Validation Loss: 0.19768059253692627\n",
      "Epoch: 152/2000, Train Loss: 0.19327610731124878, Validation Loss: 0.197220116853714\n",
      "Epoch: 153/2000, Train Loss: 0.19276711344718933, Validation Loss: 0.1967499852180481\n",
      "Epoch: 154/2000, Train Loss: 0.19226467609405518, Validation Loss: 0.19627796113491058\n",
      "Epoch: 155/2000, Train Loss: 0.19176843762397766, Validation Loss: 0.19583922624588013\n",
      "Epoch: 156/2000, Train Loss: 0.19127710163593292, Validation Loss: 0.19536232948303223\n",
      "Epoch: 157/2000, Train Loss: 0.19078955054283142, Validation Loss: 0.19493494927883148\n",
      "Epoch: 158/2000, Train Loss: 0.19030553102493286, Validation Loss: 0.19446928799152374\n",
      "Epoch: 159/2000, Train Loss: 0.18982554972171783, Validation Loss: 0.19403964281082153\n",
      "Epoch: 160/2000, Train Loss: 0.1893501579761505, Validation Loss: 0.19359658658504486\n",
      "Epoch: 161/2000, Train Loss: 0.18887962400913239, Validation Loss: 0.19316090643405914\n",
      "Epoch: 162/2000, Train Loss: 0.1884138137102127, Validation Loss: 0.19274085760116577\n",
      "Epoch: 163/2000, Train Loss: 0.18795233964920044, Validation Loss: 0.19230128824710846\n",
      "Epoch: 164/2000, Train Loss: 0.18749472498893738, Validation Loss: 0.19189947843551636\n",
      "Epoch: 165/2000, Train Loss: 0.18704059720039368, Validation Loss: 0.1914595663547516\n",
      "Epoch: 166/2000, Train Loss: 0.18659022450447083, Validation Loss: 0.1910722404718399\n",
      "Epoch: 167/2000, Train Loss: 0.18614403903484344, Validation Loss: 0.19063378870487213\n",
      "Epoch: 168/2000, Train Loss: 0.18570175766944885, Validation Loss: 0.19026166200637817\n",
      "Epoch: 169/2000, Train Loss: 0.18526290357112885, Validation Loss: 0.18982383608818054\n",
      "Epoch: 170/2000, Train Loss: 0.18482699990272522, Validation Loss: 0.18947508931159973\n",
      "Epoch: 171/2000, Train Loss: 0.18439197540283203, Validation Loss: 0.18903669714927673\n",
      "Epoch: 172/2000, Train Loss: 0.18395787477493286, Validation Loss: 0.1887369453907013\n",
      "Epoch: 173/2000, Train Loss: 0.1835329532623291, Validation Loss: 0.188301220536232\n",
      "Epoch: 174/2000, Train Loss: 0.18314401805400848, Validation Loss: 0.18808194994926453\n",
      "Epoch: 175/2000, Train Loss: 0.18276524543762207, Validation Loss: 0.18759259581565857\n",
      "Epoch: 176/2000, Train Loss: 0.18243536353111267, Validation Loss: 0.187602236866951\n",
      "Epoch: 177/2000, Train Loss: 0.18216121196746826, Validation Loss: 0.18708129227161407\n",
      "Epoch: 178/2000, Train Loss: 0.1819353699684143, Validation Loss: 0.18711547553539276\n",
      "Epoch: 179/2000, Train Loss: 0.18156443536281586, Validation Loss: 0.18624164164066315\n",
      "Epoch: 180/2000, Train Loss: 0.18099352717399597, Validation Loss: 0.18582770228385925\n",
      "Epoch: 181/2000, Train Loss: 0.18031921982765198, Validation Loss: 0.18537969887256622\n",
      "Epoch: 182/2000, Train Loss: 0.17987340688705444, Validation Loss: 0.18505340814590454\n",
      "Epoch: 183/2000, Train Loss: 0.17966201901435852, Validation Loss: 0.18508005142211914\n",
      "Epoch: 184/2000, Train Loss: 0.17940153181552887, Validation Loss: 0.1843775361776352\n",
      "Epoch: 185/2000, Train Loss: 0.17893479764461517, Validation Loss: 0.18401913344860077\n",
      "Epoch: 186/2000, Train Loss: 0.17837809026241302, Validation Loss: 0.18365418910980225\n",
      "Epoch: 187/2000, Train Loss: 0.1779915988445282, Validation Loss: 0.18330425024032593\n",
      "Epoch: 188/2000, Train Loss: 0.17774760723114014, Validation Loss: 0.18323318660259247\n",
      "Epoch: 189/2000, Train Loss: 0.17742134630680084, Validation Loss: 0.18261298537254333\n",
      "Epoch: 190/2000, Train Loss: 0.17696048319339752, Validation Loss: 0.182292640209198\n",
      "Epoch: 191/2000, Train Loss: 0.17651218175888062, Validation Loss: 0.1820579320192337\n",
      "Epoch: 192/2000, Train Loss: 0.176191046833992, Validation Loss: 0.18167120218276978\n",
      "Epoch: 193/2000, Train Loss: 0.17591024935245514, Validation Loss: 0.18152424693107605\n",
      "Epoch: 194/2000, Train Loss: 0.17553997039794922, Validation Loss: 0.1809949427843094\n",
      "Epoch: 195/2000, Train Loss: 0.17510701715946198, Validation Loss: 0.18068699538707733\n",
      "Epoch: 196/2000, Train Loss: 0.17471858859062195, Validation Loss: 0.18047794699668884\n",
      "Epoch: 197/2000, Train Loss: 0.17440630495548248, Validation Loss: 0.1800735741853714\n",
      "Epoch: 198/2000, Train Loss: 0.17409519851207733, Validation Loss: 0.17988678812980652\n",
      "Epoch: 199/2000, Train Loss: 0.1737229973077774, Validation Loss: 0.17942911386489868\n",
      "Epoch: 200/2000, Train Loss: 0.17332717776298523, Validation Loss: 0.17912283539772034\n",
      "Epoch: 201/2000, Train Loss: 0.1729685217142105, Validation Loss: 0.17889411747455597\n",
      "Epoch: 202/2000, Train Loss: 0.17265130579471588, Validation Loss: 0.1784871369600296\n",
      "Epoch: 203/2000, Train Loss: 0.17233064770698547, Validation Loss: 0.17828424274921417\n",
      "Epoch: 204/2000, Train Loss: 0.1719772070646286, Validation Loss: 0.1778489351272583\n",
      "Epoch: 205/2000, Train Loss: 0.17160747945308685, Validation Loss: 0.17756164073944092\n",
      "Epoch: 206/2000, Train Loss: 0.17125201225280762, Validation Loss: 0.17728543281555176\n",
      "Epoch: 207/2000, Train Loss: 0.17092262208461761, Validation Loss: 0.17691662907600403\n",
      "Epoch: 208/2000, Train Loss: 0.17060668766498566, Validation Loss: 0.17671914398670197\n",
      "Epoch: 209/2000, Train Loss: 0.17028212547302246, Validation Loss: 0.17629273235797882\n",
      "Epoch: 210/2000, Train Loss: 0.16994211077690125, Validation Loss: 0.17605777084827423\n",
      "Epoch: 211/2000, Train Loss: 0.16959550976753235, Validation Loss: 0.175698921084404\n",
      "Epoch: 212/2000, Train Loss: 0.1692548543214798, Validation Loss: 0.17540551722049713\n",
      "Epoch: 213/2000, Train Loss: 0.1689261496067047, Validation Loss: 0.17515647411346436\n",
      "Epoch: 214/2000, Train Loss: 0.1686081886291504, Validation Loss: 0.1747981309890747\n",
      "Epoch: 215/2000, Train Loss: 0.16829420626163483, Validation Loss: 0.1746039241552353\n",
      "Epoch: 216/2000, Train Loss: 0.16797827184200287, Validation Loss: 0.17420926690101624\n",
      "Epoch: 217/2000, Train Loss: 0.16765867173671722, Validation Loss: 0.17401725053787231\n",
      "Epoch: 218/2000, Train Loss: 0.1673341989517212, Validation Loss: 0.17363128066062927\n",
      "Epoch: 219/2000, Train Loss: 0.16700780391693115, Validation Loss: 0.17341011762619019\n",
      "Epoch: 220/2000, Train Loss: 0.16668304800987244, Validation Loss: 0.1730634868144989\n",
      "Epoch: 221/2000, Train Loss: 0.16636113822460175, Validation Loss: 0.17280353605747223\n",
      "Epoch: 222/2000, Train Loss: 0.16604265570640564, Validation Loss: 0.17250457406044006\n",
      "Epoch: 223/2000, Train Loss: 0.16572780907154083, Validation Loss: 0.17221105098724365\n",
      "Epoch: 224/2000, Train Loss: 0.16541528701782227, Validation Loss: 0.17195309698581696\n",
      "Epoch: 225/2000, Train Loss: 0.16510504484176636, Validation Loss: 0.17162764072418213\n",
      "Epoch: 226/2000, Train Loss: 0.1647973656654358, Validation Loss: 0.1714112013578415\n",
      "Epoch: 227/2000, Train Loss: 0.16449278593063354, Validation Loss: 0.17104673385620117\n",
      "Epoch: 228/2000, Train Loss: 0.16419373452663422, Validation Loss: 0.17089499533176422\n",
      "Epoch: 229/2000, Train Loss: 0.16390378773212433, Validation Loss: 0.17047686874866486\n",
      "Epoch: 230/2000, Train Loss: 0.1636301428079605, Validation Loss: 0.17046836018562317\n",
      "Epoch: 231/2000, Train Loss: 0.16338391602039337, Validation Loss: 0.16999031603336334\n",
      "Epoch: 232/2000, Train Loss: 0.16317610442638397, Validation Loss: 0.17019382119178772\n",
      "Epoch: 233/2000, Train Loss: 0.16299432516098022, Validation Loss: 0.16959527134895325\n",
      "Epoch: 234/2000, Train Loss: 0.16280321776866913, Validation Loss: 0.16974416375160217\n",
      "Epoch: 235/2000, Train Loss: 0.1624893695116043, Validation Loss: 0.1689319759607315\n",
      "Epoch: 236/2000, Train Loss: 0.16205638647079468, Validation Loss: 0.1687421351671219\n",
      "Epoch: 237/2000, Train Loss: 0.16158339381217957, Validation Loss: 0.16832351684570312\n",
      "Epoch: 238/2000, Train Loss: 0.16123348474502563, Validation Loss: 0.16803237795829773\n",
      "Epoch: 239/2000, Train Loss: 0.16103041172027588, Validation Loss: 0.16814163327217102\n",
      "Epoch: 240/2000, Train Loss: 0.16086868941783905, Validation Loss: 0.1676000952720642\n",
      "Epoch: 241/2000, Train Loss: 0.16063393652439117, Validation Loss: 0.16756823658943176\n",
      "Epoch: 242/2000, Train Loss: 0.16027648746967316, Validation Loss: 0.1669957935810089\n",
      "Epoch: 243/2000, Train Loss: 0.15989236533641815, Validation Loss: 0.16675911843776703\n",
      "Epoch: 244/2000, Train Loss: 0.15958437323570251, Validation Loss: 0.16665469110012054\n",
      "Epoch: 245/2000, Train Loss: 0.15936774015426636, Validation Loss: 0.16625620424747467\n",
      "Epoch: 246/2000, Train Loss: 0.1591687649488449, Validation Loss: 0.1662755310535431\n",
      "Epoch: 247/2000, Train Loss: 0.15890859067440033, Validation Loss: 0.16573740541934967\n",
      "Epoch: 248/2000, Train Loss: 0.15858793258666992, Validation Loss: 0.16557690501213074\n",
      "Epoch: 249/2000, Train Loss: 0.15825991332530975, Validation Loss: 0.16528651118278503\n",
      "Epoch: 250/2000, Train Loss: 0.1579817682504654, Validation Loss: 0.16498544812202454\n",
      "Epoch: 251/2000, Train Loss: 0.15775270760059357, Validation Loss: 0.1649502068758011\n",
      "Epoch: 252/2000, Train Loss: 0.15752942860126495, Validation Loss: 0.16450247168540955\n",
      "Epoch: 253/2000, Train Loss: 0.1572759449481964, Validation Loss: 0.16442400217056274\n",
      "Epoch: 254/2000, Train Loss: 0.15698620676994324, Validation Loss: 0.16401340067386627\n",
      "Epoch: 255/2000, Train Loss: 0.15669067203998566, Validation Loss: 0.16380688548088074\n",
      "Epoch: 256/2000, Train Loss: 0.15641707181930542, Validation Loss: 0.16361753642559052\n",
      "Epoch: 257/2000, Train Loss: 0.1561715006828308, Validation Loss: 0.16328907012939453\n",
      "Epoch: 258/2000, Train Loss: 0.15593862533569336, Validation Loss: 0.16322031617164612\n",
      "Epoch: 259/2000, Train Loss: 0.1556980013847351, Validation Loss: 0.16281206905841827\n",
      "Epoch: 260/2000, Train Loss: 0.15544100105762482, Validation Loss: 0.16270773112773895\n",
      "Epoch: 261/2000, Train Loss: 0.15517045557498932, Validation Loss: 0.16234992444515228\n",
      "Epoch: 262/2000, Train Loss: 0.15489961206912994, Validation Loss: 0.16215571761131287\n",
      "Epoch: 263/2000, Train Loss: 0.15463842451572418, Validation Loss: 0.16193333268165588\n",
      "Epoch: 264/2000, Train Loss: 0.1543898731470108, Validation Loss: 0.16164690256118774\n",
      "Epoch: 265/2000, Train Loss: 0.1541501134634018, Validation Loss: 0.1615263670682907\n",
      "Epoch: 266/2000, Train Loss: 0.1539125144481659, Validation Loss: 0.16117390990257263\n",
      "Epoch: 267/2000, Train Loss: 0.15367215871810913, Validation Loss: 0.16108128428459167\n",
      "Epoch: 268/2000, Train Loss: 0.153426393866539, Validation Loss: 0.16071417927742004\n",
      "Epoch: 269/2000, Train Loss: 0.15317651629447937, Validation Loss: 0.16059619188308716\n",
      "Epoch: 270/2000, Train Loss: 0.15292415022850037, Validation Loss: 0.16026292741298676\n",
      "Epoch: 271/2000, Train Loss: 0.1526724249124527, Validation Loss: 0.16009745001792908\n",
      "Epoch: 272/2000, Train Loss: 0.15242312848567963, Validation Loss: 0.15982352197170258\n",
      "Epoch: 273/2000, Train Loss: 0.15217718482017517, Validation Loss: 0.15960954129695892\n",
      "Epoch: 274/2000, Train Loss: 0.1519344002008438, Validation Loss: 0.15939435362815857\n",
      "Epoch: 275/2000, Train Loss: 0.15169432759284973, Validation Loss: 0.15913763642311096\n",
      "Epoch: 276/2000, Train Loss: 0.15145641565322876, Validation Loss: 0.1589699238538742\n",
      "Epoch: 277/2000, Train Loss: 0.15122030675411224, Validation Loss: 0.15867476165294647\n",
      "Epoch: 278/2000, Train Loss: 0.15098610520362854, Validation Loss: 0.15855209529399872\n",
      "Epoch: 279/2000, Train Loss: 0.15075425803661346, Validation Loss: 0.15821799635887146\n",
      "Epoch: 280/2000, Train Loss: 0.1505260318517685, Validation Loss: 0.1581563949584961\n",
      "Epoch: 281/2000, Train Loss: 0.1503029614686966, Validation Loss: 0.15777283906936646\n",
      "Epoch: 282/2000, Train Loss: 0.15008895099163055, Validation Loss: 0.15781269967556\n",
      "Epoch: 283/2000, Train Loss: 0.14988724887371063, Validation Loss: 0.15736129879951477\n",
      "Epoch: 284/2000, Train Loss: 0.14970679581165314, Validation Loss: 0.1575678288936615\n",
      "Epoch: 285/2000, Train Loss: 0.14954781532287598, Validation Loss: 0.15702567994594574\n",
      "Epoch: 286/2000, Train Loss: 0.14941787719726562, Validation Loss: 0.15739065408706665\n",
      "Epoch: 287/2000, Train Loss: 0.14927399158477783, Validation Loss: 0.15668991208076477\n",
      "Epoch: 288/2000, Train Loss: 0.14909273386001587, Validation Loss: 0.15691602230072021\n",
      "Epoch: 289/2000, Train Loss: 0.1487841010093689, Validation Loss: 0.15612083673477173\n",
      "Epoch: 290/2000, Train Loss: 0.14840301871299744, Validation Loss: 0.1560315489768982\n",
      "Epoch: 291/2000, Train Loss: 0.14802846312522888, Validation Loss: 0.15571263432502747\n",
      "Epoch: 292/2000, Train Loss: 0.14776629209518433, Validation Loss: 0.15544858574867249\n",
      "Epoch: 293/2000, Train Loss: 0.14761608839035034, Validation Loss: 0.15562425553798676\n",
      "Epoch: 294/2000, Train Loss: 0.14749789237976074, Validation Loss: 0.15510691702365875\n",
      "Epoch: 295/2000, Train Loss: 0.1473315954208374, Validation Loss: 0.15522463619709015\n",
      "Epoch: 296/2000, Train Loss: 0.1470678746700287, Validation Loss: 0.15464700758457184\n",
      "Epoch: 297/2000, Train Loss: 0.14675982296466827, Validation Loss: 0.15453097224235535\n",
      "Epoch: 298/2000, Train Loss: 0.14647912979125977, Validation Loss: 0.15434865653514862\n",
      "Epoch: 299/2000, Train Loss: 0.14627163112163544, Validation Loss: 0.1540507674217224\n",
      "Epoch: 300/2000, Train Loss: 0.14611442387104034, Validation Loss: 0.15415048599243164\n",
      "Epoch: 301/2000, Train Loss: 0.1459498256444931, Validation Loss: 0.15367108583450317\n",
      "Epoch: 302/2000, Train Loss: 0.14574231207370758, Validation Loss: 0.15369060635566711\n",
      "Epoch: 303/2000, Train Loss: 0.145488902926445, Validation Loss: 0.15327459573745728\n",
      "Epoch: 304/2000, Train Loss: 0.1452307552099228, Validation Loss: 0.15312698483467102\n",
      "Epoch: 305/2000, Train Loss: 0.1450016349554062, Validation Loss: 0.15300191938877106\n",
      "Epoch: 306/2000, Train Loss: 0.1448090374469757, Validation Loss: 0.152696430683136\n",
      "Epoch: 307/2000, Train Loss: 0.14463303983211517, Validation Loss: 0.15272893011569977\n",
      "Epoch: 308/2000, Train Loss: 0.1444462388753891, Validation Loss: 0.1523158997297287\n",
      "Epoch: 309/2000, Train Loss: 0.1442369520664215, Validation Loss: 0.15229855477809906\n",
      "Epoch: 310/2000, Train Loss: 0.1440076380968094, Validation Loss: 0.1519480049610138\n",
      "Epoch: 311/2000, Train Loss: 0.14377658069133759, Validation Loss: 0.1518106311559677\n",
      "Epoch: 312/2000, Train Loss: 0.14355775713920593, Validation Loss: 0.15164242684841156\n",
      "Epoch: 313/2000, Train Loss: 0.1433555781841278, Validation Loss: 0.15138398110866547\n",
      "Epoch: 314/2000, Train Loss: 0.14316391944885254, Validation Loss: 0.1513456553220749\n",
      "Epoch: 315/2000, Train Loss: 0.14297236502170563, Validation Loss: 0.1510031372308731\n",
      "Epoch: 316/2000, Train Loss: 0.1427738219499588, Validation Loss: 0.15098293125629425\n",
      "Epoch: 317/2000, Train Loss: 0.14256493747234344, Validation Loss: 0.15063828229904175\n",
      "Epoch: 318/2000, Train Loss: 0.14234957098960876, Validation Loss: 0.15056343376636505\n",
      "Epoch: 319/2000, Train Loss: 0.14213231205940247, Validation Loss: 0.1502929925918579\n",
      "Epoch: 320/2000, Train Loss: 0.14191898703575134, Validation Loss: 0.15013715624809265\n",
      "Epoch: 321/2000, Train Loss: 0.14171229302883148, Validation Loss: 0.14996463060379028\n",
      "Epoch: 322/2000, Train Loss: 0.1415124088525772, Validation Loss: 0.14973260462284088\n",
      "Epoch: 323/2000, Train Loss: 0.14131782948970795, Validation Loss: 0.14963722229003906\n",
      "Epoch: 324/2000, Train Loss: 0.14112642407417297, Validation Loss: 0.14935259521007538\n",
      "Epoch: 325/2000, Train Loss: 0.14093664288520813, Validation Loss: 0.14930008351802826\n",
      "Epoch: 326/2000, Train Loss: 0.14074687659740448, Validation Loss: 0.14898847043514252\n",
      "Epoch: 327/2000, Train Loss: 0.1405569314956665, Validation Loss: 0.1489495038986206\n",
      "Epoch: 328/2000, Train Loss: 0.14036604762077332, Validation Loss: 0.14863111078739166\n",
      "Epoch: 329/2000, Train Loss: 0.14017494022846222, Validation Loss: 0.14859044551849365\n",
      "Epoch: 330/2000, Train Loss: 0.1399831920862198, Validation Loss: 0.14827938377857208\n",
      "Epoch: 331/2000, Train Loss: 0.13979169726371765, Validation Loss: 0.1482338160276413\n",
      "Epoch: 332/2000, Train Loss: 0.1396002173423767, Validation Loss: 0.14793354272842407\n",
      "Epoch: 333/2000, Train Loss: 0.13940955698490143, Validation Loss: 0.147884801030159\n",
      "Epoch: 334/2000, Train Loss: 0.13921959698200226, Validation Loss: 0.147588849067688\n",
      "Epoch: 335/2000, Train Loss: 0.1390310525894165, Validation Loss: 0.1475449502468109\n",
      "Epoch: 336/2000, Train Loss: 0.13884387910366058, Validation Loss: 0.14724385738372803\n",
      "Epoch: 337/2000, Train Loss: 0.13865897059440613, Validation Loss: 0.14722180366516113\n",
      "Epoch: 338/2000, Train Loss: 0.1384763866662979, Validation Loss: 0.14690303802490234\n",
      "Epoch: 339/2000, Train Loss: 0.13829763233661652, Validation Loss: 0.14692702889442444\n",
      "Epoch: 340/2000, Train Loss: 0.13812315464019775, Validation Loss: 0.14657112956047058\n",
      "Epoch: 341/2000, Train Loss: 0.1379561424255371, Validation Loss: 0.14667555689811707\n",
      "Epoch: 342/2000, Train Loss: 0.1377972960472107, Validation Loss: 0.1462596356868744\n",
      "Epoch: 343/2000, Train Loss: 0.13765332102775574, Validation Loss: 0.14649170637130737\n",
      "Epoch: 344/2000, Train Loss: 0.13752181828022003, Validation Loss: 0.14599397778511047\n",
      "Epoch: 345/2000, Train Loss: 0.13741232454776764, Validation Loss: 0.14637142419815063\n",
      "Epoch: 346/2000, Train Loss: 0.13730141520500183, Validation Loss: 0.14575904607772827\n",
      "Epoch: 347/2000, Train Loss: 0.1371903121471405, Validation Loss: 0.14614452421665192\n",
      "Epoch: 348/2000, Train Loss: 0.13701246678829193, Validation Loss: 0.14541010558605194\n",
      "Epoch: 349/2000, Train Loss: 0.1367792934179306, Validation Loss: 0.1455613225698471\n",
      "Epoch: 350/2000, Train Loss: 0.13647308945655823, Validation Loss: 0.14498071372509003\n",
      "Epoch: 351/2000, Train Loss: 0.13617683947086334, Validation Loss: 0.14489543437957764\n",
      "Epoch: 352/2000, Train Loss: 0.13594353199005127, Validation Loss: 0.1448034793138504\n",
      "Epoch: 353/2000, Train Loss: 0.13579176366329193, Validation Loss: 0.14451991021633148\n",
      "Epoch: 354/2000, Train Loss: 0.13568954169750214, Validation Loss: 0.14473263919353485\n",
      "Epoch: 355/2000, Train Loss: 0.13558217883110046, Validation Loss: 0.1442461460828781\n",
      "Epoch: 356/2000, Train Loss: 0.13543573021888733, Validation Loss: 0.14440344274044037\n",
      "Epoch: 357/2000, Train Loss: 0.1352289617061615, Validation Loss: 0.1439080834388733\n",
      "Epoch: 358/2000, Train Loss: 0.13499660789966583, Validation Loss: 0.1438770294189453\n",
      "Epoch: 359/2000, Train Loss: 0.13477425277233124, Validation Loss: 0.14366555213928223\n",
      "Epoch: 360/2000, Train Loss: 0.13459230959415436, Validation Loss: 0.14346188306808472\n",
      "Epoch: 361/2000, Train Loss: 0.13444890081882477, Validation Loss: 0.14352402091026306\n",
      "Epoch: 362/2000, Train Loss: 0.13432034850120544, Validation Loss: 0.14316298067569733\n",
      "Epoch: 363/2000, Train Loss: 0.13418202102184296, Validation Loss: 0.14327163994312286\n",
      "Epoch: 364/2000, Train Loss: 0.13401612639427185, Validation Loss: 0.14286337792873383\n",
      "Epoch: 365/2000, Train Loss: 0.1338292956352234, Validation Loss: 0.14286914467811584\n",
      "Epoch: 366/2000, Train Loss: 0.13363374769687653, Validation Loss: 0.14259330928325653\n",
      "Epoch: 367/2000, Train Loss: 0.13344839215278625, Validation Loss: 0.1424659788608551\n",
      "Epoch: 368/2000, Train Loss: 0.1332814246416092, Validation Loss: 0.14238758385181427\n",
      "Epoch: 369/2000, Train Loss: 0.1331305056810379, Validation Loss: 0.1421373039484024\n",
      "Epoch: 370/2000, Train Loss: 0.13298650085926056, Validation Loss: 0.14217115938663483\n",
      "Epoch: 371/2000, Train Loss: 0.13283903896808624, Validation Loss: 0.14184542000293732\n",
      "Epoch: 372/2000, Train Loss: 0.13268323242664337, Validation Loss: 0.14187990128993988\n",
      "Epoch: 373/2000, Train Loss: 0.13251695036888123, Validation Loss: 0.14156363904476166\n",
      "Epoch: 374/2000, Train Loss: 0.13234537839889526, Validation Loss: 0.1415334939956665\n",
      "Epoch: 375/2000, Train Loss: 0.13217297196388245, Validation Loss: 0.14130249619483948\n",
      "Epoch: 376/2000, Train Loss: 0.13200485706329346, Validation Loss: 0.14118798077106476\n",
      "Epoch: 377/2000, Train Loss: 0.1318429410457611, Validation Loss: 0.14106397330760956\n",
      "Epoch: 378/2000, Train Loss: 0.13168686628341675, Validation Loss: 0.14087176322937012\n",
      "Epoch: 379/2000, Train Loss: 0.1315346658229828, Validation Loss: 0.1408279985189438\n",
      "Epoch: 380/2000, Train Loss: 0.13138394057750702, Validation Loss: 0.14057931303977966\n",
      "Epoch: 381/2000, Train Loss: 0.13123293220996857, Validation Loss: 0.14057449996471405\n",
      "Epoch: 382/2000, Train Loss: 0.13108018040657043, Validation Loss: 0.1402972787618637\n",
      "Epoch: 383/2000, Train Loss: 0.13092577457427979, Validation Loss: 0.14029893279075623\n",
      "Epoch: 384/2000, Train Loss: 0.130769282579422, Validation Loss: 0.1400201916694641\n",
      "Epoch: 385/2000, Train Loss: 0.1306118667125702, Validation Loss: 0.1400100737810135\n",
      "Epoch: 386/2000, Train Loss: 0.13045351207256317, Validation Loss: 0.13974837958812714\n",
      "Epoch: 387/2000, Train Loss: 0.1302952915430069, Validation Loss: 0.1397184133529663\n",
      "Epoch: 388/2000, Train Loss: 0.13013724982738495, Validation Loss: 0.13948029279708862\n",
      "Epoch: 389/2000, Train Loss: 0.12997999787330627, Validation Loss: 0.13942959904670715\n",
      "Epoch: 390/2000, Train Loss: 0.12982340157032013, Validation Loss: 0.1392134130001068\n",
      "Epoch: 391/2000, Train Loss: 0.1296677142381668, Validation Loss: 0.1391475647687912\n",
      "Epoch: 392/2000, Train Loss: 0.12951281666755676, Validation Loss: 0.13894784450531006\n",
      "Epoch: 393/2000, Train Loss: 0.12935873866081238, Validation Loss: 0.13887567818164825\n",
      "Epoch: 394/2000, Train Loss: 0.1292053610086441, Validation Loss: 0.13868382573127747\n",
      "Epoch: 395/2000, Train Loss: 0.12905272841453552, Validation Loss: 0.13861440122127533\n",
      "Epoch: 396/2000, Train Loss: 0.12890076637268066, Validation Loss: 0.13841888308525085\n",
      "Epoch: 397/2000, Train Loss: 0.12874962389469147, Validation Loss: 0.13836364448070526\n",
      "Epoch: 398/2000, Train Loss: 0.12859944999217987, Validation Loss: 0.1381511390209198\n",
      "Epoch: 399/2000, Train Loss: 0.12845073640346527, Validation Loss: 0.1381288319826126\n",
      "Epoch: 400/2000, Train Loss: 0.1283041387796402, Validation Loss: 0.13788212835788727\n",
      "Epoch: 401/2000, Train Loss: 0.1281612366437912, Validation Loss: 0.13792580366134644\n",
      "Epoch: 402/2000, Train Loss: 0.12802402675151825, Validation Loss: 0.13761992752552032\n",
      "Epoch: 403/2000, Train Loss: 0.12789767980575562, Validation Loss: 0.13779322803020477\n",
      "Epoch: 404/2000, Train Loss: 0.12778757512569427, Validation Loss: 0.13739918172359467\n",
      "Epoch: 405/2000, Train Loss: 0.12770946323871613, Validation Loss: 0.13782058656215668\n",
      "Epoch: 406/2000, Train Loss: 0.1276704967021942, Validation Loss: 0.13731715083122253\n",
      "Epoch: 407/2000, Train Loss: 0.12770311534404755, Validation Loss: 0.13808290660381317\n",
      "Epoch: 408/2000, Train Loss: 0.127756267786026, Validation Loss: 0.13736958801746368\n",
      "Epoch: 409/2000, Train Loss: 0.127821147441864, Validation Loss: 0.13805411756038666\n",
      "Epoch: 410/2000, Train Loss: 0.12763528525829315, Validation Loss: 0.13692067563533783\n",
      "Epoch: 411/2000, Train Loss: 0.12724046409130096, Validation Loss: 0.13694636523723602\n",
      "Epoch: 412/2000, Train Loss: 0.12670160830020905, Validation Loss: 0.1364455670118332\n",
      "Epoch: 413/2000, Train Loss: 0.12638123333454132, Validation Loss: 0.13629402220249176\n",
      "Epoch: 414/2000, Train Loss: 0.12636640667915344, Validation Loss: 0.13682813942432404\n",
      "Epoch: 415/2000, Train Loss: 0.1264512687921524, Validation Loss: 0.1362135112285614\n",
      "Epoch: 416/2000, Train Loss: 0.12639306485652924, Validation Loss: 0.13644473254680634\n",
      "Epoch: 417/2000, Train Loss: 0.1260783076286316, Validation Loss: 0.1357889473438263\n",
      "Epoch: 418/2000, Train Loss: 0.1257343590259552, Validation Loss: 0.13568557798862457\n",
      "Epoch: 419/2000, Train Loss: 0.12556229531764984, Validation Loss: 0.1359175443649292\n",
      "Epoch: 420/2000, Train Loss: 0.12555210292339325, Validation Loss: 0.1355111002922058\n",
      "Epoch: 421/2000, Train Loss: 0.125522643327713, Validation Loss: 0.13576629757881165\n",
      "Epoch: 422/2000, Train Loss: 0.12533234059810638, Validation Loss: 0.13520485162734985\n",
      "Epoch: 423/2000, Train Loss: 0.12506699562072754, Validation Loss: 0.13513417541980743\n",
      "Epoch: 424/2000, Train Loss: 0.12487160414457321, Validation Loss: 0.13518603146076202\n",
      "Epoch: 425/2000, Train Loss: 0.1247936487197876, Validation Loss: 0.13486938178539276\n",
      "Epoch: 426/2000, Train Loss: 0.12473837286233902, Validation Loss: 0.13508202135562897\n",
      "Epoch: 427/2000, Train Loss: 0.12459919601678848, Validation Loss: 0.13461725413799286\n",
      "Epoch: 428/2000, Train Loss: 0.1243939995765686, Validation Loss: 0.13457955420017242\n",
      "Epoch: 429/2000, Train Loss: 0.12420603632926941, Validation Loss: 0.13451580703258514\n",
      "Epoch: 430/2000, Train Loss: 0.12408909946680069, Validation Loss: 0.13426527380943298\n",
      "Epoch: 431/2000, Train Loss: 0.1240069568157196, Validation Loss: 0.13441859185695648\n",
      "Epoch: 432/2000, Train Loss: 0.12389127910137177, Validation Loss: 0.13403064012527466\n",
      "Epoch: 433/2000, Train Loss: 0.12372791022062302, Validation Loss: 0.1340259611606598\n",
      "Epoch: 434/2000, Train Loss: 0.12355406582355499, Validation Loss: 0.13387665152549744\n",
      "Epoch: 435/2000, Train Loss: 0.12341456860303879, Validation Loss: 0.1336909383535385\n",
      "Epoch: 436/2000, Train Loss: 0.12330831587314606, Validation Loss: 0.1337764412164688\n",
      "Epoch: 437/2000, Train Loss: 0.12320000678300858, Validation Loss: 0.13345728814601898\n",
      "Epoch: 438/2000, Train Loss: 0.12306541204452515, Validation Loss: 0.13348160684108734\n",
      "Epoch: 439/2000, Train Loss: 0.12291017174720764, Validation Loss: 0.13326621055603027\n",
      "Epoch: 440/2000, Train Loss: 0.12276152521371841, Validation Loss: 0.13314500451087952\n",
      "Epoch: 441/2000, Train Loss: 0.12263406813144684, Validation Loss: 0.133143350481987\n",
      "Epoch: 442/2000, Train Loss: 0.1225195974111557, Validation Loss: 0.13289795815944672\n",
      "Epoch: 443/2000, Train Loss: 0.12240009009838104, Validation Loss: 0.1329352706670761\n",
      "Epoch: 444/2000, Train Loss: 0.12226541340351105, Validation Loss: 0.13268908858299255\n",
      "Epoch: 445/2000, Train Loss: 0.12212228029966354, Validation Loss: 0.13263295590877533\n",
      "Epoch: 446/2000, Train Loss: 0.12198285013437271, Validation Loss: 0.13252748548984528\n",
      "Epoch: 447/2000, Train Loss: 0.12185417115688324, Validation Loss: 0.13236209750175476\n",
      "Epoch: 448/2000, Train Loss: 0.12173289805650711, Validation Loss: 0.1323646754026413\n",
      "Epoch: 449/2000, Train Loss: 0.12161052227020264, Validation Loss: 0.1321384757757187\n",
      "Epoch: 450/2000, Train Loss: 0.12148168683052063, Validation Loss: 0.13212977349758148\n",
      "Epoch: 451/2000, Train Loss: 0.12134692072868347, Validation Loss: 0.13194231688976288\n",
      "Epoch: 452/2000, Train Loss: 0.12121157348155975, Validation Loss: 0.1318589299917221\n",
      "Epoch: 453/2000, Train Loss: 0.12108013778924942, Validation Loss: 0.13176926970481873\n",
      "Epoch: 454/2000, Train Loss: 0.12095359712839127, Validation Loss: 0.13160955905914307\n",
      "Epoch: 455/2000, Train Loss: 0.1208295077085495, Validation Loss: 0.1315837800502777\n",
      "Epoch: 456/2000, Train Loss: 0.12070445716381073, Validation Loss: 0.13138653337955475\n",
      "Epoch: 457/2000, Train Loss: 0.12057659029960632, Validation Loss: 0.1313576102256775\n",
      "Epoch: 458/2000, Train Loss: 0.12044601142406464, Validation Loss: 0.13118013739585876\n",
      "Epoch: 459/2000, Train Loss: 0.12031447887420654, Validation Loss: 0.1311062127351761\n",
      "Epoch: 460/2000, Train Loss: 0.12018377333879471, Validation Loss: 0.13098442554473877\n",
      "Epoch: 461/2000, Train Loss: 0.12005474418401718, Validation Loss: 0.13085706532001495\n",
      "Epoch: 462/2000, Train Loss: 0.11992723494768143, Validation Loss: 0.13078634440898895\n",
      "Epoch: 463/2000, Train Loss: 0.11980035156011581, Validation Loss: 0.1306203305721283\n",
      "Epoch: 464/2000, Train Loss: 0.11967313289642334, Validation Loss: 0.13057313859462738\n",
      "Epoch: 465/2000, Train Loss: 0.11954496800899506, Validation Loss: 0.13039466738700867\n",
      "Epoch: 466/2000, Train Loss: 0.1194157749414444, Validation Loss: 0.13034431636333466\n",
      "Epoch: 467/2000, Train Loss: 0.1192857027053833, Validation Loss: 0.13017785549163818\n",
      "Epoch: 468/2000, Train Loss: 0.11915519833564758, Validation Loss: 0.13010865449905396\n",
      "Epoch: 469/2000, Train Loss: 0.1190246120095253, Validation Loss: 0.12996777892112732\n",
      "Epoch: 470/2000, Train Loss: 0.11889420449733734, Validation Loss: 0.12987451255321503\n",
      "Epoch: 471/2000, Train Loss: 0.11876413971185684, Validation Loss: 0.12976108491420746\n",
      "Epoch: 472/2000, Train Loss: 0.11863449215888977, Validation Loss: 0.1296447217464447\n",
      "Epoch: 473/2000, Train Loss: 0.11850525438785553, Validation Loss: 0.12955345213413239\n",
      "Epoch: 474/2000, Train Loss: 0.11837638914585114, Validation Loss: 0.1294167935848236\n",
      "Epoch: 475/2000, Train Loss: 0.11824780702590942, Validation Loss: 0.12934176623821259\n",
      "Epoch: 476/2000, Train Loss: 0.11811952292919159, Validation Loss: 0.12918853759765625\n",
      "Epoch: 477/2000, Train Loss: 0.11799163371324539, Validation Loss: 0.12912915647029877\n",
      "Epoch: 478/2000, Train Loss: 0.11786427348852158, Validation Loss: 0.128962442278862\n",
      "Epoch: 479/2000, Train Loss: 0.11773763597011566, Validation Loss: 0.12892349064350128\n",
      "Epoch: 480/2000, Train Loss: 0.1176118329167366, Validation Loss: 0.12874090671539307\n",
      "Epoch: 481/2000, Train Loss: 0.11748712509870529, Validation Loss: 0.12873129546642303\n",
      "Epoch: 482/2000, Train Loss: 0.11736380308866501, Validation Loss: 0.12852251529693604\n",
      "Epoch: 483/2000, Train Loss: 0.11724258214235306, Validation Loss: 0.12855951488018036\n",
      "Epoch: 484/2000, Train Loss: 0.11712437123060226, Validation Loss: 0.1283058375120163\n",
      "Epoch: 485/2000, Train Loss: 0.11701126396656036, Validation Loss: 0.12842491269111633\n",
      "Epoch: 486/2000, Train Loss: 0.11690574884414673, Validation Loss: 0.12809985876083374\n",
      "Epoch: 487/2000, Train Loss: 0.11681400239467621, Validation Loss: 0.1283724159002304\n",
      "Epoch: 488/2000, Train Loss: 0.11674191802740097, Validation Loss: 0.12794770300388336\n",
      "Epoch: 489/2000, Train Loss: 0.11670510470867157, Validation Loss: 0.1284869760274887\n",
      "Epoch: 490/2000, Train Loss: 0.11670590937137604, Validation Loss: 0.12793317437171936\n",
      "Epoch: 491/2000, Train Loss: 0.11676336824893951, Validation Loss: 0.12875081598758698\n",
      "Epoch: 492/2000, Train Loss: 0.11680633574724197, Validation Loss: 0.1279478818178177\n",
      "Epoch: 493/2000, Train Loss: 0.11680677533149719, Validation Loss: 0.12856025993824005\n",
      "Epoch: 494/2000, Train Loss: 0.11656932532787323, Validation Loss: 0.12749260663986206\n",
      "Epoch: 495/2000, Train Loss: 0.11618301272392273, Validation Loss: 0.127553790807724\n",
      "Epoch: 496/2000, Train Loss: 0.11576007306575775, Validation Loss: 0.1271946132183075\n",
      "Epoch: 497/2000, Train Loss: 0.11553407460451126, Validation Loss: 0.12703384459018707\n",
      "Epoch: 498/2000, Train Loss: 0.11553090810775757, Validation Loss: 0.1275431215763092\n",
      "Epoch: 499/2000, Train Loss: 0.11560055613517761, Validation Loss: 0.12698066234588623\n",
      "Epoch: 500/2000, Train Loss: 0.11558037996292114, Validation Loss: 0.12735162675380707\n",
      "Epoch: 501/2000, Train Loss: 0.1153663694858551, Validation Loss: 0.1266653835773468\n",
      "Epoch: 502/2000, Train Loss: 0.1150745376944542, Validation Loss: 0.12665870785713196\n",
      "Epoch: 503/2000, Train Loss: 0.11485198140144348, Validation Loss: 0.12667496502399445\n",
      "Epoch: 504/2000, Train Loss: 0.11477674543857574, Validation Loss: 0.12640772759914398\n",
      "Epoch: 505/2000, Train Loss: 0.11477507650852203, Validation Loss: 0.12677571177482605\n",
      "Epoch: 506/2000, Train Loss: 0.11471516638994217, Validation Loss: 0.12622785568237305\n",
      "Epoch: 507/2000, Train Loss: 0.11455526202917099, Validation Loss: 0.12634211778640747\n",
      "Epoch: 508/2000, Train Loss: 0.1143411248922348, Validation Loss: 0.1260720193386078\n",
      "Epoch: 509/2000, Train Loss: 0.11417512595653534, Validation Loss: 0.12594160437583923\n",
      "Epoch: 510/2000, Train Loss: 0.11409072577953339, Validation Loss: 0.12613576650619507\n",
      "Epoch: 511/2000, Train Loss: 0.11404033750295639, Validation Loss: 0.12576201558113098\n",
      "Epoch: 512/2000, Train Loss: 0.11395809799432755, Validation Loss: 0.1259474903345108\n",
      "Epoch: 513/2000, Train Loss: 0.11381518095731735, Validation Loss: 0.12557975947856903\n",
      "Epoch: 514/2000, Train Loss: 0.11365000903606415, Validation Loss: 0.12556129693984985\n",
      "Epoch: 515/2000, Train Loss: 0.11351018399000168, Validation Loss: 0.12554055452346802\n",
      "Epoch: 516/2000, Train Loss: 0.11341403424739838, Validation Loss: 0.125327467918396\n",
      "Epoch: 517/2000, Train Loss: 0.11333779990673065, Validation Loss: 0.12549053132534027\n",
      "Epoch: 518/2000, Train Loss: 0.11324477940797806, Validation Loss: 0.12515756487846375\n",
      "Epoch: 519/2000, Train Loss: 0.11312250047922134, Validation Loss: 0.12522229552268982\n",
      "Epoch: 520/2000, Train Loss: 0.11298364400863647, Validation Loss: 0.12502114474773407\n",
      "Epoch: 521/2000, Train Loss: 0.11285480856895447, Validation Loss: 0.12492651492357254\n",
      "Epoch: 522/2000, Train Loss: 0.1127481758594513, Validation Loss: 0.12495817244052887\n",
      "Epoch: 523/2000, Train Loss: 0.1126561090350151, Validation Loss: 0.12473075836896896\n",
      "Epoch: 524/2000, Train Loss: 0.11256171762943268, Validation Loss: 0.12483028322458267\n",
      "Epoch: 525/2000, Train Loss: 0.11245304346084595, Validation Loss: 0.12457803636789322\n",
      "Epoch: 526/2000, Train Loss: 0.11233268678188324, Validation Loss: 0.12459283322095871\n",
      "Epoch: 527/2000, Train Loss: 0.11221059411764145, Validation Loss: 0.12446185946464539\n",
      "Epoch: 528/2000, Train Loss: 0.11209684610366821, Validation Loss: 0.12435509264469147\n",
      "Epoch: 529/2000, Train Loss: 0.11199342459440231, Validation Loss: 0.12436486780643463\n",
      "Epoch: 530/2000, Train Loss: 0.11189503222703934, Validation Loss: 0.12416735291481018\n",
      "Epoch: 531/2000, Train Loss: 0.11179450154304504, Validation Loss: 0.12421849370002747\n",
      "Epoch: 532/2000, Train Loss: 0.11168751865625381, Validation Loss: 0.12401179224252701\n",
      "Epoch: 533/2000, Train Loss: 0.11157536506652832, Validation Loss: 0.12401336431503296\n",
      "Epoch: 534/2000, Train Loss: 0.11146198958158493, Validation Loss: 0.12388195097446442\n",
      "Epoch: 535/2000, Train Loss: 0.11135160177946091, Validation Loss: 0.12380179017782211\n",
      "Epoch: 536/2000, Train Loss: 0.11124584078788757, Validation Loss: 0.1237655058503151\n",
      "Epoch: 537/2000, Train Loss: 0.11114351451396942, Validation Loss: 0.12361674010753632\n",
      "Epoch: 538/2000, Train Loss: 0.11104204505681992, Validation Loss: 0.12363094836473465\n",
      "Epoch: 539/2000, Train Loss: 0.11093909293413162, Validation Loss: 0.123454749584198\n",
      "Epoch: 540/2000, Train Loss: 0.1108338013291359, Validation Loss: 0.12346130609512329\n",
      "Epoch: 541/2000, Train Loss: 0.11072655767202377, Validation Loss: 0.1233062818646431\n",
      "Epoch: 542/2000, Train Loss: 0.11061877757310867, Validation Loss: 0.12327040731906891\n",
      "Epoch: 543/2000, Train Loss: 0.11051177978515625, Validation Loss: 0.12316803634166718\n",
      "Epoch: 544/2000, Train Loss: 0.11040637642145157, Validation Loss: 0.12308307737112045\n",
      "Epoch: 545/2000, Train Loss: 0.11030258238315582, Validation Loss: 0.12303420156240463\n",
      "Epoch: 546/2000, Train Loss: 0.11019992083311081, Validation Loss: 0.12291014194488525\n",
      "Epoch: 547/2000, Train Loss: 0.11009769886732101, Validation Loss: 0.12289299070835114\n",
      "Epoch: 548/2000, Train Loss: 0.10999535024166107, Validation Loss: 0.12274772673845291\n",
      "Epoch: 549/2000, Train Loss: 0.10989253222942352, Validation Loss: 0.12273722887039185\n",
      "Epoch: 550/2000, Train Loss: 0.1097891554236412, Validation Loss: 0.12259145081043243\n",
      "Epoch: 551/2000, Train Loss: 0.10968543589115143, Validation Loss: 0.12257128953933716\n",
      "Epoch: 552/2000, Train Loss: 0.10958156734704971, Validation Loss: 0.12244097888469696\n",
      "Epoch: 553/2000, Train Loss: 0.10947781056165695, Validation Loss: 0.12240270525217056\n",
      "Epoch: 554/2000, Train Loss: 0.10937434434890747, Validation Loss: 0.12229380756616592\n",
      "Epoch: 555/2000, Train Loss: 0.10927126556634903, Validation Loss: 0.12223421782255173\n",
      "Epoch: 556/2000, Train Loss: 0.10916859656572342, Validation Loss: 0.12214591354131699\n",
      "Epoch: 557/2000, Train Loss: 0.10906632989645004, Validation Loss: 0.12206710129976273\n",
      "Epoch: 558/2000, Train Loss: 0.10896441340446472, Validation Loss: 0.1219971776008606\n",
      "Epoch: 559/2000, Train Loss: 0.10886280238628387, Validation Loss: 0.12190353125333786\n",
      "Epoch: 560/2000, Train Loss: 0.10876143723726273, Validation Loss: 0.12184897065162659\n",
      "Epoch: 561/2000, Train Loss: 0.10866031795740128, Validation Loss: 0.12174267321825027\n",
      "Epoch: 562/2000, Train Loss: 0.10855945199728012, Validation Loss: 0.1217004805803299\n",
      "Epoch: 563/2000, Train Loss: 0.10845880210399628, Validation Loss: 0.12158138304948807\n",
      "Epoch: 564/2000, Train Loss: 0.10835843533277512, Validation Loss: 0.12155213952064514\n",
      "Epoch: 565/2000, Train Loss: 0.10825840383768082, Validation Loss: 0.12141887098550797\n",
      "Epoch: 566/2000, Train Loss: 0.10815879702568054, Validation Loss: 0.12140783667564392\n",
      "Epoch: 567/2000, Train Loss: 0.10805976390838623, Validation Loss: 0.12125551700592041\n",
      "Epoch: 568/2000, Train Loss: 0.10796153545379639, Validation Loss: 0.12127236276865005\n",
      "Epoch: 569/2000, Train Loss: 0.10786445438861847, Validation Loss: 0.12109039723873138\n",
      "Epoch: 570/2000, Train Loss: 0.1077692061662674, Validation Loss: 0.12115315347909927\n",
      "Epoch: 571/2000, Train Loss: 0.10767663270235062, Validation Loss: 0.12092471867799759\n",
      "Epoch: 572/2000, Train Loss: 0.10758865624666214, Validation Loss: 0.1210678368806839\n",
      "Epoch: 573/2000, Train Loss: 0.10750742256641388, Validation Loss: 0.12076911330223083\n",
      "Epoch: 574/2000, Train Loss: 0.10743827372789383, Validation Loss: 0.12105514854192734\n",
      "Epoch: 575/2000, Train Loss: 0.10738565027713776, Validation Loss: 0.12065839022397995\n",
      "Epoch: 576/2000, Train Loss: 0.10736257582902908, Validation Loss: 0.1211824044585228\n",
      "Epoch: 577/2000, Train Loss: 0.10737031698226929, Validation Loss: 0.12065993249416351\n",
      "Epoch: 578/2000, Train Loss: 0.1074279248714447, Validation Loss: 0.12145674228668213\n",
      "Epoch: 579/2000, Train Loss: 0.10748578608036041, Validation Loss: 0.12072718143463135\n",
      "Epoch: 580/2000, Train Loss: 0.1075349822640419, Validation Loss: 0.12145823985338211\n",
      "Epoch: 581/2000, Train Loss: 0.1074070930480957, Validation Loss: 0.12044306844472885\n",
      "Epoch: 582/2000, Train Loss: 0.1071380004286766, Validation Loss: 0.12065939605236053\n",
      "Epoch: 583/2000, Train Loss: 0.10673592984676361, Validation Loss: 0.12004061043262482\n",
      "Epoch: 584/2000, Train Loss: 0.10642721503973007, Validation Loss: 0.11997091770172119\n",
      "Epoch: 585/2000, Train Loss: 0.10631778836250305, Validation Loss: 0.1202937439084053\n",
      "Epoch: 586/2000, Train Loss: 0.10636482387781143, Validation Loss: 0.1199129968881607\n",
      "Epoch: 587/2000, Train Loss: 0.10642930865287781, Validation Loss: 0.12042286992073059\n",
      "Epoch: 588/2000, Train Loss: 0.10636308789253235, Validation Loss: 0.11973261833190918\n",
      "Epoch: 589/2000, Train Loss: 0.10616761445999146, Validation Loss: 0.11987189203500748\n",
      "Epoch: 590/2000, Train Loss: 0.10591419041156769, Validation Loss: 0.11957268416881561\n",
      "Epoch: 591/2000, Train Loss: 0.10574300587177277, Validation Loss: 0.11945797502994537\n",
      "Epoch: 592/2000, Train Loss: 0.10569247603416443, Validation Loss: 0.11975755542516708\n",
      "Epoch: 593/2000, Train Loss: 0.10569406300783157, Validation Loss: 0.11935581266880035\n",
      "Epoch: 594/2000, Train Loss: 0.10565482825040817, Validation Loss: 0.11964139342308044\n",
      "Epoch: 595/2000, Train Loss: 0.10552270710468292, Validation Loss: 0.11918288469314575\n",
      "Epoch: 596/2000, Train Loss: 0.10534734278917313, Validation Loss: 0.11920688301324844\n",
      "Epoch: 597/2000, Train Loss: 0.10519774258136749, Validation Loss: 0.11916019767522812\n",
      "Epoch: 598/2000, Train Loss: 0.1051144078373909, Validation Loss: 0.11896099895238876\n",
      "Epoch: 599/2000, Train Loss: 0.10507310926914215, Validation Loss: 0.11919375509023666\n",
      "Epoch: 600/2000, Train Loss: 0.10501846671104431, Validation Loss: 0.1188221275806427\n",
      "Epoch: 601/2000, Train Loss: 0.10492099821567535, Validation Loss: 0.11896242946386337\n",
      "Epoch: 602/2000, Train Loss: 0.10478747636079788, Validation Loss: 0.11870812624692917\n",
      "Epoch: 603/2000, Train Loss: 0.1046585813164711, Validation Loss: 0.11866126954555511\n",
      "Epoch: 604/2000, Train Loss: 0.10456108301877975, Validation Loss: 0.11870268732309341\n",
      "Epoch: 605/2000, Train Loss: 0.10449214279651642, Validation Loss: 0.11847994476556778\n",
      "Epoch: 606/2000, Train Loss: 0.10442744195461273, Validation Loss: 0.11862855404615402\n",
      "Epoch: 607/2000, Train Loss: 0.10434361547231674, Validation Loss: 0.11833847314119339\n",
      "Epoch: 608/2000, Train Loss: 0.10423887521028519, Validation Loss: 0.11839745193719864\n",
      "Epoch: 609/2000, Train Loss: 0.10412595421075821, Validation Loss: 0.11824598163366318\n",
      "Epoch: 610/2000, Train Loss: 0.1040230542421341, Validation Loss: 0.11816758662462234\n",
      "Epoch: 611/2000, Train Loss: 0.10393678396940231, Validation Loss: 0.1182035505771637\n",
      "Epoch: 612/2000, Train Loss: 0.10386054962873459, Validation Loss: 0.11800651997327805\n",
      "Epoch: 613/2000, Train Loss: 0.10378239303827286, Validation Loss: 0.11809585988521576\n",
      "Epoch: 614/2000, Train Loss: 0.10369391739368439, Validation Loss: 0.11787398159503937\n",
      "Epoch: 615/2000, Train Loss: 0.10359648615121841, Validation Loss: 0.1178995743393898\n",
      "Epoch: 616/2000, Train Loss: 0.10349652916193008, Validation Loss: 0.11777415126562119\n",
      "Epoch: 617/2000, Train Loss: 0.1034015417098999, Validation Loss: 0.11770094186067581\n",
      "Epoch: 618/2000, Train Loss: 0.10331409424543381, Validation Loss: 0.11769687384366989\n",
      "Epoch: 619/2000, Train Loss: 0.10323163866996765, Validation Loss: 0.11754098534584045\n",
      "Epoch: 620/2000, Train Loss: 0.10314929485321045, Validation Loss: 0.1175866425037384\n",
      "Epoch: 621/2000, Train Loss: 0.10306313633918762, Validation Loss: 0.11740684509277344\n",
      "Epoch: 622/2000, Train Loss: 0.10297270864248276, Validation Loss: 0.11742646992206573\n",
      "Epoch: 623/2000, Train Loss: 0.10287979990243912, Validation Loss: 0.11729403585195541\n",
      "Epoch: 624/2000, Train Loss: 0.10278753191232681, Validation Loss: 0.11725141108036041\n",
      "Epoch: 625/2000, Train Loss: 0.1026979312300682, Validation Loss: 0.11719620227813721\n",
      "Epoch: 626/2000, Train Loss: 0.10261136293411255, Validation Loss: 0.11709047853946686\n",
      "Epoch: 627/2000, Train Loss: 0.10252663493156433, Validation Loss: 0.11708971112966537\n",
      "Epoch: 628/2000, Train Loss: 0.10244203358888626, Validation Loss: 0.1169472485780716\n",
      "Epoch: 629/2000, Train Loss: 0.10235626995563507, Validation Loss: 0.11695832759141922\n",
      "Epoch: 630/2000, Train Loss: 0.10226888209581375, Validation Loss: 0.11681869626045227\n",
      "Epoch: 631/2000, Train Loss: 0.1021803691983223, Validation Loss: 0.11680787801742554\n",
      "Epoch: 632/2000, Train Loss: 0.10209155082702637, Validation Loss: 0.11670199036598206\n",
      "Epoch: 633/2000, Train Loss: 0.1020032986998558, Validation Loss: 0.11665333807468414\n",
      "Epoch: 634/2000, Train Loss: 0.10191605240106583, Validation Loss: 0.11659061908721924\n",
      "Epoch: 635/2000, Train Loss: 0.1018298789858818, Validation Loss: 0.11650518327951431\n",
      "Epoch: 636/2000, Train Loss: 0.10174446552991867, Validation Loss: 0.11647646874189377\n",
      "Epoch: 637/2000, Train Loss: 0.101659394800663, Validation Loss: 0.11636660248041153\n",
      "Epoch: 638/2000, Train Loss: 0.10157427191734314, Validation Loss: 0.11635389178991318\n",
      "Epoch: 639/2000, Train Loss: 0.10148883610963821, Validation Loss: 0.1162356361746788\n",
      "Epoch: 640/2000, Train Loss: 0.10140306502580643, Validation Loss: 0.11622169613838196\n",
      "Epoch: 641/2000, Train Loss: 0.10131703317165375, Validation Loss: 0.11610952019691467\n",
      "Epoch: 642/2000, Train Loss: 0.10123087465763092, Validation Loss: 0.11608366668224335\n",
      "Epoch: 643/2000, Train Loss: 0.10114476829767227, Validation Loss: 0.11598727852106094\n",
      "Epoch: 644/2000, Train Loss: 0.10105884820222855, Validation Loss: 0.11594529449939728\n",
      "Epoch: 645/2000, Train Loss: 0.10097319632768631, Validation Loss: 0.1158677339553833\n",
      "Epoch: 646/2000, Train Loss: 0.10088783502578735, Validation Loss: 0.11580905318260193\n",
      "Epoch: 647/2000, Train Loss: 0.10080274939537048, Validation Loss: 0.11574814468622208\n",
      "Epoch: 648/2000, Train Loss: 0.10071791708469391, Validation Loss: 0.11567475646734238\n",
      "Epoch: 649/2000, Train Loss: 0.10063327103853226, Validation Loss: 0.11562707275152206\n",
      "Epoch: 650/2000, Train Loss: 0.10054877400398254, Validation Loss: 0.11554273962974548\n",
      "Epoch: 651/2000, Train Loss: 0.10046441107988358, Validation Loss: 0.11550548672676086\n",
      "Epoch: 652/2000, Train Loss: 0.10038014501333237, Validation Loss: 0.11541339755058289\n",
      "Epoch: 653/2000, Train Loss: 0.10029596090316772, Validation Loss: 0.11538428068161011\n",
      "Epoch: 654/2000, Train Loss: 0.10021188110113144, Validation Loss: 0.11528564989566803\n",
      "Epoch: 655/2000, Train Loss: 0.10012790560722351, Validation Loss: 0.11526359617710114\n",
      "Epoch: 656/2000, Train Loss: 0.10004405677318573, Validation Loss: 0.11515822261571884\n",
      "Epoch: 657/2000, Train Loss: 0.09996037930250168, Validation Loss: 0.11514453589916229\n",
      "Epoch: 658/2000, Train Loss: 0.09987691044807434, Validation Loss: 0.11503084003925323\n",
      "Epoch: 659/2000, Train Loss: 0.09979373961687088, Validation Loss: 0.11502885818481445\n",
      "Epoch: 660/2000, Train Loss: 0.09971090406179428, Validation Loss: 0.11490271240472794\n",
      "Epoch: 661/2000, Train Loss: 0.09962858259677887, Validation Loss: 0.1149183064699173\n",
      "Epoch: 662/2000, Train Loss: 0.0995469018816948, Validation Loss: 0.11477278172969818\n",
      "Epoch: 663/2000, Train Loss: 0.0994662195444107, Validation Loss: 0.1148165762424469\n",
      "Epoch: 664/2000, Train Loss: 0.09938684105873108, Validation Loss: 0.11464156955480576\n",
      "Epoch: 665/2000, Train Loss: 0.09930960834026337, Validation Loss: 0.11473219096660614\n",
      "Epoch: 666/2000, Train Loss: 0.09923526644706726, Validation Loss: 0.11451274901628494\n",
      "Epoch: 667/2000, Train Loss: 0.09916593134403229, Validation Loss: 0.11468169838190079\n",
      "Epoch: 668/2000, Train Loss: 0.09910321980714798, Validation Loss: 0.11439741402864456\n",
      "Epoch: 669/2000, Train Loss: 0.09905247390270233, Validation Loss: 0.11469736695289612\n",
      "Epoch: 670/2000, Train Loss: 0.09901617467403412, Validation Loss: 0.11432592570781708\n",
      "Epoch: 671/2000, Train Loss: 0.09900637716054916, Validation Loss: 0.11482998728752136\n",
      "Epoch: 672/2000, Train Loss: 0.09901975095272064, Validation Loss: 0.11434845626354218\n",
      "Epoch: 673/2000, Train Loss: 0.09907504916191101, Validation Loss: 0.1150774359703064\n",
      "Epoch: 674/2000, Train Loss: 0.09912609308958054, Validation Loss: 0.11442414671182632\n",
      "Epoch: 675/2000, Train Loss: 0.09917887300252914, Validation Loss: 0.11512962728738785\n",
      "Epoch: 676/2000, Train Loss: 0.09909454733133316, Validation Loss: 0.11423203349113464\n",
      "Epoch: 677/2000, Train Loss: 0.0989040657877922, Validation Loss: 0.11452372372150421\n",
      "Epoch: 678/2000, Train Loss: 0.09856593608856201, Validation Loss: 0.11382660269737244\n",
      "Epoch: 679/2000, Train Loss: 0.09825541824102402, Validation Loss: 0.11381803452968597\n",
      "Epoch: 680/2000, Train Loss: 0.0980774387717247, Validation Loss: 0.1139163076877594\n",
      "Epoch: 681/2000, Train Loss: 0.09805867820978165, Validation Loss: 0.11368067562580109\n",
      "Epoch: 682/2000, Train Loss: 0.09812048077583313, Validation Loss: 0.11417265981435776\n",
      "Epoch: 683/2000, Train Loss: 0.09814111143350601, Validation Loss: 0.11362259089946747\n",
      "Epoch: 684/2000, Train Loss: 0.09806489199399948, Validation Loss: 0.11389818042516708\n",
      "Epoch: 685/2000, Train Loss: 0.0978749692440033, Validation Loss: 0.11340782046318054\n",
      "Epoch: 686/2000, Train Loss: 0.0976710394024849, Validation Loss: 0.11341552436351776\n",
      "Epoch: 687/2000, Train Loss: 0.09753207862377167, Validation Loss: 0.11345430463552475\n",
      "Epoch: 688/2000, Train Loss: 0.09748442471027374, Validation Loss: 0.11324877291917801\n",
      "Epoch: 689/2000, Train Loss: 0.09748230874538422, Validation Loss: 0.11356502771377563\n",
      "Epoch: 690/2000, Train Loss: 0.09745336323976517, Validation Loss: 0.11315249651670456\n",
      "Epoch: 691/2000, Train Loss: 0.09736629575490952, Validation Loss: 0.11333121359348297\n",
      "Epoch: 692/2000, Train Loss: 0.09722688794136047, Validation Loss: 0.1130187138915062\n",
      "Epoch: 693/2000, Train Loss: 0.0970895066857338, Validation Loss: 0.11299595981836319\n",
      "Epoch: 694/2000, Train Loss: 0.09699215739965439, Validation Loss: 0.11303392797708511\n",
      "Epoch: 695/2000, Train Loss: 0.09693865478038788, Validation Loss: 0.11283546686172485\n",
      "Epoch: 696/2000, Train Loss: 0.09690065681934357, Validation Loss: 0.11303898692131042\n",
      "Epoch: 697/2000, Train Loss: 0.09684323519468307, Validation Loss: 0.11273003369569778\n",
      "Epoch: 698/2000, Train Loss: 0.09675563126802444, Validation Loss: 0.11284782737493515\n",
      "Epoch: 699/2000, Train Loss: 0.09664636105298996, Validation Loss: 0.1126374825835228\n",
      "Epoch: 700/2000, Train Loss: 0.09654145687818527, Validation Loss: 0.11260518431663513\n",
      "Epoch: 701/2000, Train Loss: 0.09645717591047287, Validation Loss: 0.112615667283535\n",
      "Epoch: 702/2000, Train Loss: 0.0963933914899826, Validation Loss: 0.1124488115310669\n",
      "Epoch: 703/2000, Train Loss: 0.09633656591176987, Validation Loss: 0.11257148534059525\n",
      "Epoch: 704/2000, Train Loss: 0.09627094119787216, Validation Loss: 0.11233792454004288\n",
      "Epoch: 705/2000, Train Loss: 0.09619114547967911, Validation Loss: 0.11242014914751053\n",
      "Epoch: 706/2000, Train Loss: 0.09610050171613693, Validation Loss: 0.11224786937236786\n",
      "Epoch: 707/2000, Train Loss: 0.0960099846124649, Validation Loss: 0.11222707480192184\n",
      "Epoch: 708/2000, Train Loss: 0.09592755138874054, Validation Loss: 0.11219164729118347\n",
      "Epoch: 709/2000, Train Loss: 0.09585507959127426, Validation Loss: 0.11207099258899689\n",
      "Epoch: 710/2000, Train Loss: 0.09578825533390045, Validation Loss: 0.11212921142578125\n",
      "Epoch: 711/2000, Train Loss: 0.09572039544582367, Validation Loss: 0.11195281147956848\n",
      "Epoch: 712/2000, Train Loss: 0.09564723819494247, Validation Loss: 0.11201384663581848\n",
      "Epoch: 713/2000, Train Loss: 0.0955679714679718, Validation Loss: 0.11185496300458908\n",
      "Epoch: 714/2000, Train Loss: 0.09548589587211609, Validation Loss: 0.11185956001281738\n",
      "Epoch: 715/2000, Train Loss: 0.0954047217965126, Validation Loss: 0.11177584528923035\n",
      "Epoch: 716/2000, Train Loss: 0.09532716870307922, Validation Loss: 0.11171073466539383\n",
      "Epoch: 717/2000, Train Loss: 0.09525366127490997, Validation Loss: 0.11170580238103867\n",
      "Epoch: 718/2000, Train Loss: 0.09518255293369293, Validation Loss: 0.11158803850412369\n",
      "Epoch: 719/2000, Train Loss: 0.09511160105466843, Validation Loss: 0.11162072420120239\n",
      "Epoch: 720/2000, Train Loss: 0.09503893554210663, Validation Loss: 0.11148358136415482\n",
      "Epoch: 721/2000, Train Loss: 0.09496407210826874, Validation Loss: 0.11150627583265305\n",
      "Epoch: 722/2000, Train Loss: 0.09488743543624878, Validation Loss: 0.1113869920372963\n",
      "Epoch: 723/2000, Train Loss: 0.09481026977300644, Validation Loss: 0.11137310415506363\n",
      "Epoch: 724/2000, Train Loss: 0.09473371505737305, Validation Loss: 0.1112976223230362\n",
      "Epoch: 725/2000, Train Loss: 0.09465853869915009, Validation Loss: 0.11124312877655029\n",
      "Epoch: 726/2000, Train Loss: 0.09458478540182114, Validation Loss: 0.11121360212564468\n",
      "Epoch: 727/2000, Train Loss: 0.09451204538345337, Validation Loss: 0.11112648248672485\n",
      "Epoch: 728/2000, Train Loss: 0.09443973749876022, Validation Loss: 0.1111249178647995\n",
      "Epoch: 729/2000, Train Loss: 0.09436724334955215, Validation Loss: 0.11101935058832169\n",
      "Epoch: 730/2000, Train Loss: 0.09429430216550827, Validation Loss: 0.11102385073900223\n",
      "Epoch: 731/2000, Train Loss: 0.09422075003385544, Validation Loss: 0.1109176054596901\n",
      "Epoch: 732/2000, Train Loss: 0.09414676576852798, Validation Loss: 0.11091320961713791\n",
      "Epoch: 733/2000, Train Loss: 0.09407252818346024, Validation Loss: 0.11082115024328232\n",
      "Epoch: 734/2000, Train Loss: 0.09399833530187607, Validation Loss: 0.1107998713850975\n",
      "Epoch: 735/2000, Train Loss: 0.09392434358596802, Validation Loss: 0.1107284426689148\n",
      "Epoch: 736/2000, Train Loss: 0.09385070204734802, Validation Loss: 0.11068737506866455\n",
      "Epoch: 737/2000, Train Loss: 0.09377741813659668, Validation Loss: 0.11063623428344727\n",
      "Epoch: 738/2000, Train Loss: 0.09370449185371399, Validation Loss: 0.11057746410369873\n",
      "Epoch: 739/2000, Train Loss: 0.0936318039894104, Validation Loss: 0.11054325103759766\n",
      "Epoch: 740/2000, Train Loss: 0.09355930984020233, Validation Loss: 0.1104714572429657\n",
      "Epoch: 741/2000, Train Loss: 0.09348691254854202, Validation Loss: 0.11044909805059433\n",
      "Epoch: 742/2000, Train Loss: 0.09341457486152649, Validation Loss: 0.11036836355924606\n",
      "Epoch: 743/2000, Train Loss: 0.09334225207567215, Validation Loss: 0.11035283654928207\n",
      "Epoch: 744/2000, Train Loss: 0.09326993674039841, Validation Loss: 0.11026658862829208\n",
      "Epoch: 745/2000, Train Loss: 0.09319761395454407, Validation Loss: 0.11025520414113998\n",
      "Epoch: 746/2000, Train Loss: 0.09312529861927032, Validation Loss: 0.11016648262739182\n",
      "Epoch: 747/2000, Train Loss: 0.09305299818515778, Validation Loss: 0.11015857756137848\n",
      "Epoch: 748/2000, Train Loss: 0.09298071265220642, Validation Loss: 0.11006826162338257\n",
      "Epoch: 749/2000, Train Loss: 0.09290849417448044, Validation Loss: 0.11006364226341248\n",
      "Epoch: 750/2000, Train Loss: 0.09283628314733505, Validation Loss: 0.10997019708156586\n",
      "Epoch: 751/2000, Train Loss: 0.09276419132947922, Validation Loss: 0.10996976494789124\n",
      "Epoch: 752/2000, Train Loss: 0.09269219636917114, Validation Loss: 0.10987074673175812\n",
      "Epoch: 753/2000, Train Loss: 0.0926203802227974, Validation Loss: 0.10987772047519684\n",
      "Epoch: 754/2000, Train Loss: 0.09254879504442215, Validation Loss: 0.109769806265831\n",
      "Epoch: 755/2000, Train Loss: 0.09247756004333496, Validation Loss: 0.10978946834802628\n",
      "Epoch: 756/2000, Train Loss: 0.09240678697824478, Validation Loss: 0.1096673458814621\n",
      "Epoch: 757/2000, Train Loss: 0.0923367589712143, Validation Loss: 0.10970775783061981\n",
      "Epoch: 758/2000, Train Loss: 0.09226766228675842, Validation Loss: 0.10956378281116486\n",
      "Epoch: 759/2000, Train Loss: 0.09220009297132492, Validation Loss: 0.1096382886171341\n",
      "Epoch: 760/2000, Train Loss: 0.092134490609169, Validation Loss: 0.10946181416511536\n",
      "Epoch: 761/2000, Train Loss: 0.0920722633600235, Validation Loss: 0.10959215462207794\n",
      "Epoch: 762/2000, Train Loss: 0.0920143648982048, Validation Loss: 0.10936836898326874\n",
      "Epoch: 763/2000, Train Loss: 0.09196417778730392, Validation Loss: 0.109589584171772\n",
      "Epoch: 764/2000, Train Loss: 0.09192321449518204, Validation Loss: 0.10930118709802628\n",
      "Epoch: 765/2000, Train Loss: 0.0918993204832077, Validation Loss: 0.10966610908508301\n",
      "Epoch: 766/2000, Train Loss: 0.0918920487165451, Validation Loss: 0.10929703712463379\n",
      "Epoch: 767/2000, Train Loss: 0.09191656112670898, Validation Loss: 0.10985462367534637\n",
      "Epoch: 768/2000, Train Loss: 0.09195464849472046, Validation Loss: 0.10937785357236862\n",
      "Epoch: 769/2000, Train Loss: 0.09202385693788528, Validation Loss: 0.11005975306034088\n",
      "Epoch: 770/2000, Train Loss: 0.09204355627298355, Validation Loss: 0.10939650982618332\n",
      "Epoch: 771/2000, Train Loss: 0.09202510118484497, Validation Loss: 0.10989369451999664\n",
      "Epoch: 772/2000, Train Loss: 0.09184536337852478, Validation Loss: 0.10909011214971542\n",
      "Epoch: 773/2000, Train Loss: 0.0915878564119339, Validation Loss: 0.10922635346651077\n",
      "Epoch: 774/2000, Train Loss: 0.0912870392203331, Validation Loss: 0.10881836712360382\n",
      "Epoch: 775/2000, Train Loss: 0.09107893705368042, Validation Loss: 0.10877005010843277\n",
      "Epoch: 776/2000, Train Loss: 0.09100786596536636, Validation Loss: 0.10900864005088806\n",
      "Epoch: 777/2000, Train Loss: 0.09103845059871674, Validation Loss: 0.10874699056148529\n",
      "Epoch: 778/2000, Train Loss: 0.09109017997980118, Validation Loss: 0.1091664656996727\n",
      "Epoch: 779/2000, Train Loss: 0.09107258915901184, Validation Loss: 0.10867030173540115\n",
      "Epoch: 780/2000, Train Loss: 0.09097183495759964, Validation Loss: 0.1088729128241539\n",
      "Epoch: 781/2000, Train Loss: 0.09079606086015701, Validation Loss: 0.10849588364362717\n",
      "Epoch: 782/2000, Train Loss: 0.09062840789556503, Validation Loss: 0.10849540680646896\n",
      "Epoch: 783/2000, Train Loss: 0.09052154421806335, Validation Loss: 0.108550064265728\n",
      "Epoch: 784/2000, Train Loss: 0.09048453718423843, Validation Loss: 0.10837715119123459\n",
      "Epoch: 785/2000, Train Loss: 0.09047971665859222, Validation Loss: 0.10864449292421341\n",
      "Epoch: 786/2000, Train Loss: 0.09045419096946716, Validation Loss: 0.10830499976873398\n",
      "Epoch: 787/2000, Train Loss: 0.09038503468036652, Validation Loss: 0.10847379267215729\n",
      "Epoch: 788/2000, Train Loss: 0.09027210623025894, Validation Loss: 0.10818782448768616\n",
      "Epoch: 789/2000, Train Loss: 0.0901532843708992, Validation Loss: 0.10819772630929947\n",
      "Epoch: 790/2000, Train Loss: 0.09005845338106155, Validation Loss: 0.10817849636077881\n",
      "Epoch: 791/2000, Train Loss: 0.0899989902973175, Validation Loss: 0.10805042088031769\n",
      "Epoch: 792/2000, Train Loss: 0.08996129781007767, Validation Loss: 0.10820822417736053\n",
      "Epoch: 793/2000, Train Loss: 0.08992068469524384, Validation Loss: 0.10796895623207092\n",
      "Epoch: 794/2000, Train Loss: 0.08986140787601471, Validation Loss: 0.10810288041830063\n",
      "Epoch: 795/2000, Train Loss: 0.08977926522493362, Validation Loss: 0.10787858814001083\n",
      "Epoch: 796/2000, Train Loss: 0.0896882489323616, Validation Loss: 0.10790460556745529\n",
      "Epoch: 797/2000, Train Loss: 0.0896027684211731, Validation Loss: 0.10783203691244125\n",
      "Epoch: 798/2000, Train Loss: 0.08953262865543365, Validation Loss: 0.10775113105773926\n",
      "Epoch: 799/2000, Train Loss: 0.08947636187076569, Validation Loss: 0.10782041400671005\n",
      "Epoch: 800/2000, Train Loss: 0.08942496031522751, Validation Loss: 0.10765652358531952\n",
      "Epoch: 801/2000, Train Loss: 0.08936921507120132, Validation Loss: 0.10775551199913025\n",
      "Epoch: 802/2000, Train Loss: 0.08930346369743347, Validation Loss: 0.1075742244720459\n",
      "Epoch: 803/2000, Train Loss: 0.08922990411520004, Validation Loss: 0.10761889070272446\n",
      "Epoch: 804/2000, Train Loss: 0.08915335685014725, Validation Loss: 0.10750516504049301\n",
      "Epoch: 805/2000, Train Loss: 0.08908016979694366, Validation Loss: 0.10747310519218445\n",
      "Epoch: 806/2000, Train Loss: 0.08901336044073105, Validation Loss: 0.10745926201343536\n",
      "Epoch: 807/2000, Train Loss: 0.08895225822925568, Validation Loss: 0.10735999792814255\n",
      "Epoch: 808/2000, Train Loss: 0.08889365941286087, Validation Loss: 0.10740696638822556\n",
      "Epoch: 809/2000, Train Loss: 0.08883388340473175, Validation Loss: 0.1072697713971138\n",
      "Epoch: 810/2000, Train Loss: 0.08877088874578476, Validation Loss: 0.10731775313615799\n",
      "Epoch: 811/2000, Train Loss: 0.0887041762471199, Validation Loss: 0.1071881577372551\n",
      "Epoch: 812/2000, Train Loss: 0.0886354073882103, Validation Loss: 0.10720023512840271\n",
      "Epoch: 813/2000, Train Loss: 0.08856654912233353, Validation Loss: 0.10711802542209625\n",
      "Epoch: 814/2000, Train Loss: 0.08849938213825226, Validation Loss: 0.10708349943161011\n",
      "Epoch: 815/2000, Train Loss: 0.08843456208705902, Validation Loss: 0.10705753415822983\n",
      "Epoch: 816/2000, Train Loss: 0.08837177604436874, Validation Loss: 0.10698077082633972\n",
      "Epoch: 817/2000, Train Loss: 0.08831007033586502, Validation Loss: 0.10699095577001572\n",
      "Epoch: 818/2000, Train Loss: 0.0882483497262001, Validation Loss: 0.10688786953687668\n",
      "Epoch: 819/2000, Train Loss: 0.08818583190441132, Validation Loss: 0.10690705478191376\n",
      "Epoch: 820/2000, Train Loss: 0.0881221815943718, Validation Loss: 0.10680150985717773\n",
      "Epoch: 821/2000, Train Loss: 0.08805760741233826, Validation Loss: 0.10681010782718658\n",
      "Epoch: 822/2000, Train Loss: 0.08799250423908234, Validation Loss: 0.1067231073975563\n",
      "Epoch: 823/2000, Train Loss: 0.08792740106582642, Validation Loss: 0.10671015828847885\n",
      "Epoch: 824/2000, Train Loss: 0.08786269277334213, Validation Loss: 0.10665083676576614\n",
      "Epoch: 825/2000, Train Loss: 0.0877986028790474, Validation Loss: 0.10661270469427109\n",
      "Epoch: 826/2000, Train Loss: 0.08773516863584518, Validation Loss: 0.1065795049071312\n",
      "Epoch: 827/2000, Train Loss: 0.08767227083444595, Validation Loss: 0.10651952773332596\n",
      "Epoch: 828/2000, Train Loss: 0.0876096859574318, Validation Loss: 0.10650550574064255\n",
      "Epoch: 829/2000, Train Loss: 0.08754724264144897, Validation Loss: 0.10643121600151062\n",
      "Epoch: 830/2000, Train Loss: 0.08748476207256317, Validation Loss: 0.10642747581005096\n",
      "Epoch: 831/2000, Train Loss: 0.08742215484380722, Validation Loss: 0.10634671896696091\n",
      "Epoch: 832/2000, Train Loss: 0.08735939860343933, Validation Loss: 0.10634514689445496\n",
      "Epoch: 833/2000, Train Loss: 0.08729647099971771, Validation Loss: 0.10626450926065445\n",
      "Epoch: 834/2000, Train Loss: 0.08723345398902893, Validation Loss: 0.10625975579023361\n",
      "Epoch: 835/2000, Train Loss: 0.08717038482427597, Validation Loss: 0.10618395358324051\n",
      "Epoch: 836/2000, Train Loss: 0.08710732311010361, Validation Loss: 0.1061733067035675\n",
      "Epoch: 837/2000, Train Loss: 0.08704432100057602, Validation Loss: 0.10610441863536835\n",
      "Epoch: 838/2000, Train Loss: 0.0869813859462738, Validation Loss: 0.10608670115470886\n",
      "Epoch: 839/2000, Train Loss: 0.08691854774951935, Validation Loss: 0.10602478682994843\n",
      "Epoch: 840/2000, Train Loss: 0.08685580641031265, Validation Loss: 0.10600025206804276\n",
      "Epoch: 841/2000, Train Loss: 0.08679316937923431, Validation Loss: 0.10594464093446732\n",
      "Epoch: 842/2000, Train Loss: 0.08673061430454254, Validation Loss: 0.10591461509466171\n",
      "Epoch: 843/2000, Train Loss: 0.08666814118623734, Validation Loss: 0.10586433857679367\n",
      "Epoch: 844/2000, Train Loss: 0.0866057351231575, Validation Loss: 0.10583006590604782\n",
      "Epoch: 845/2000, Train Loss: 0.08654339611530304, Validation Loss: 0.10578366369009018\n",
      "Epoch: 846/2000, Train Loss: 0.08648110181093216, Validation Loss: 0.10574605315923691\n",
      "Epoch: 847/2000, Train Loss: 0.08641885966062546, Validation Loss: 0.10570237785577774\n",
      "Epoch: 848/2000, Train Loss: 0.08635665476322174, Validation Loss: 0.10566241294145584\n",
      "Epoch: 849/2000, Train Loss: 0.0862944945693016, Validation Loss: 0.10562095791101456\n",
      "Epoch: 850/2000, Train Loss: 0.08623237907886505, Validation Loss: 0.1055794283747673\n",
      "Epoch: 851/2000, Train Loss: 0.08617029339075089, Validation Loss: 0.10553982108831406\n",
      "Epoch: 852/2000, Train Loss: 0.08610823005437851, Validation Loss: 0.10549689084291458\n",
      "Epoch: 853/2000, Train Loss: 0.08604621887207031, Validation Loss: 0.10545886307954788\n",
      "Epoch: 854/2000, Train Loss: 0.08598422259092331, Validation Loss: 0.10541439801454544\n",
      "Epoch: 855/2000, Train Loss: 0.08592228591442108, Validation Loss: 0.10537828505039215\n",
      "Epoch: 856/2000, Train Loss: 0.08586037158966064, Validation Loss: 0.10533205419778824\n",
      "Epoch: 857/2000, Train Loss: 0.0857984721660614, Validation Loss: 0.105298712849617\n",
      "Epoch: 858/2000, Train Loss: 0.08573663979768753, Validation Loss: 0.10524990409612656\n",
      "Epoch: 859/2000, Train Loss: 0.08567484468221664, Validation Loss: 0.10522051900625229\n",
      "Epoch: 860/2000, Train Loss: 0.08561310172080994, Validation Loss: 0.10516739636659622\n",
      "Epoch: 861/2000, Train Loss: 0.085551418364048, Validation Loss: 0.1051441878080368\n",
      "Epoch: 862/2000, Train Loss: 0.08548983186483383, Validation Loss: 0.1050838977098465\n",
      "Epoch: 863/2000, Train Loss: 0.08542840927839279, Validation Loss: 0.10507111996412277\n",
      "Epoch: 864/2000, Train Loss: 0.08536723256111145, Validation Loss: 0.10499898344278336\n",
      "Epoch: 865/2000, Train Loss: 0.08530648052692413, Validation Loss: 0.10500436276197433\n",
      "Epoch: 866/2000, Train Loss: 0.08524642884731293, Validation Loss: 0.10491224378347397\n",
      "Epoch: 867/2000, Train Loss: 0.0851876363158226, Validation Loss: 0.10495033115148544\n",
      "Epoch: 868/2000, Train Loss: 0.0851309597492218, Validation Loss: 0.10482524335384369\n",
      "Epoch: 869/2000, Train Loss: 0.08507820218801498, Validation Loss: 0.10492587089538574\n",
      "Epoch: 870/2000, Train Loss: 0.08503204584121704, Validation Loss: 0.1047503873705864\n",
      "Epoch: 871/2000, Train Loss: 0.08499851077795029, Validation Loss: 0.10497751832008362\n",
      "Epoch: 872/2000, Train Loss: 0.08498528599739075, Validation Loss: 0.10473903268575668\n",
      "Epoch: 873/2000, Train Loss: 0.08501231670379639, Validation Loss: 0.10522730648517609\n",
      "Epoch: 874/2000, Train Loss: 0.08509513735771179, Validation Loss: 0.10494665056467056\n",
      "Epoch: 875/2000, Train Loss: 0.08528777211904526, Validation Loss: 0.10588784515857697\n",
      "Epoch: 876/2000, Train Loss: 0.0855627954006195, Validation Loss: 0.10554700344800949\n",
      "Epoch: 877/2000, Train Loss: 0.08597675710916519, Validation Loss: 0.10664670169353485\n",
      "Epoch: 878/2000, Train Loss: 0.08614841848611832, Validation Loss: 0.10564756393432617\n",
      "Epoch: 879/2000, Train Loss: 0.08604636788368225, Validation Loss: 0.10568657517433167\n",
      "Epoch: 880/2000, Train Loss: 0.0852755680680275, Validation Loss: 0.10445333272218704\n",
      "Epoch: 881/2000, Train Loss: 0.08452840894460678, Validation Loss: 0.1043451651930809\n",
      "Epoch: 882/2000, Train Loss: 0.08430242538452148, Validation Loss: 0.10496212542057037\n",
      "Epoch: 883/2000, Train Loss: 0.08461768925189972, Validation Loss: 0.10477916896343231\n",
      "Epoch: 884/2000, Train Loss: 0.0849691703915596, Validation Loss: 0.10527495294809341\n",
      "Epoch: 885/2000, Train Loss: 0.0848146378993988, Validation Loss: 0.10431668162345886\n",
      "Epoch: 886/2000, Train Loss: 0.08434540778398514, Validation Loss: 0.10421253740787506\n",
      "Epoch: 887/2000, Train Loss: 0.08399885147809982, Validation Loss: 0.10440506041049957\n",
      "Epoch: 888/2000, Train Loss: 0.08406835049390793, Validation Loss: 0.10429119318723679\n",
      "Epoch: 889/2000, Train Loss: 0.08429303765296936, Validation Loss: 0.10473693907260895\n",
      "Epoch: 890/2000, Train Loss: 0.08424869924783707, Validation Loss: 0.10407703369855881\n",
      "Epoch: 891/2000, Train Loss: 0.0839582160115242, Validation Loss: 0.10404335707426071\n",
      "Epoch: 892/2000, Train Loss: 0.0837145671248436, Validation Loss: 0.10415959358215332\n",
      "Epoch: 893/2000, Train Loss: 0.08373074233531952, Validation Loss: 0.10401997715234756\n",
      "Epoch: 894/2000, Train Loss: 0.08384474366903305, Validation Loss: 0.10434146225452423\n",
      "Epoch: 895/2000, Train Loss: 0.08378959447145462, Validation Loss: 0.10385870933532715\n",
      "Epoch: 896/2000, Train Loss: 0.0835859477519989, Validation Loss: 0.10384166985750198\n",
      "Epoch: 897/2000, Train Loss: 0.08342935889959335, Validation Loss: 0.10393664985895157\n",
      "Epoch: 898/2000, Train Loss: 0.08343092352151871, Validation Loss: 0.10377131402492523\n",
      "Epoch: 899/2000, Train Loss: 0.08347378671169281, Validation Loss: 0.10400047153234482\n",
      "Epoch: 900/2000, Train Loss: 0.08340476453304291, Validation Loss: 0.10365550965070724\n",
      "Epoch: 901/2000, Train Loss: 0.08325554430484772, Validation Loss: 0.1036480963230133\n",
      "Epoch: 902/2000, Train Loss: 0.08315020054578781, Validation Loss: 0.10373040288686752\n",
      "Epoch: 903/2000, Train Loss: 0.08313778787851334, Validation Loss: 0.10356359928846359\n",
      "Epoch: 904/2000, Train Loss: 0.08313777297735214, Validation Loss: 0.1037248969078064\n",
      "Epoch: 905/2000, Train Loss: 0.08306678384542465, Validation Loss: 0.10347156226634979\n",
      "Epoch: 906/2000, Train Loss: 0.08295400440692902, Validation Loss: 0.10346244275569916\n",
      "Epoch: 907/2000, Train Loss: 0.0828738659620285, Validation Loss: 0.10352517664432526\n",
      "Epoch: 908/2000, Train Loss: 0.08284731954336166, Validation Loss: 0.10337680578231812\n",
      "Epoch: 909/2000, Train Loss: 0.08282352238893509, Validation Loss: 0.10349816828966141\n",
      "Epoch: 910/2000, Train Loss: 0.08275695145130157, Validation Loss: 0.10330958664417267\n",
      "Epoch: 911/2000, Train Loss: 0.08266746252775192, Validation Loss: 0.10329816490411758\n",
      "Epoch: 912/2000, Train Loss: 0.08259864896535873, Validation Loss: 0.10333438217639923\n",
      "Epoch: 913/2000, Train Loss: 0.08256109058856964, Validation Loss: 0.10320384800434113\n",
      "Epoch: 914/2000, Train Loss: 0.08252489566802979, Validation Loss: 0.1032911166548729\n",
      "Epoch: 915/2000, Train Loss: 0.08246393501758575, Validation Loss: 0.10313846915960312\n",
      "Epoch: 916/2000, Train Loss: 0.082388736307621, Validation Loss: 0.10312581062316895\n",
      "Epoch: 917/2000, Train Loss: 0.08232450485229492, Validation Loss: 0.10313477367162704\n",
      "Epoch: 918/2000, Train Loss: 0.08227898180484772, Validation Loss: 0.10302688926458359\n",
      "Epoch: 919/2000, Train Loss: 0.08223621547222137, Validation Loss: 0.10309109836816788\n",
      "Epoch: 920/2000, Train Loss: 0.08218000829219818, Validation Loss: 0.1029660776257515\n",
      "Epoch: 921/2000, Train Loss: 0.0821138322353363, Validation Loss: 0.10296010971069336\n",
      "Epoch: 922/2000, Train Loss: 0.08205188810825348, Validation Loss: 0.10294807702302933\n",
      "Epoch: 923/2000, Train Loss: 0.08200109004974365, Validation Loss: 0.10286304354667664\n",
      "Epoch: 924/2000, Train Loss: 0.08195425570011139, Validation Loss: 0.10290654748678207\n",
      "Epoch: 925/2000, Train Loss: 0.08190116286277771, Validation Loss: 0.10279776901006699\n",
      "Epoch: 926/2000, Train Loss: 0.08184096962213516, Validation Loss: 0.10279493033885956\n",
      "Epoch: 927/2000, Train Loss: 0.08178089559078217, Validation Loss: 0.10276103764772415\n",
      "Epoch: 928/2000, Train Loss: 0.08172682672739029, Validation Loss: 0.10269510000944138\n",
      "Epoch: 929/2000, Train Loss: 0.08167695254087448, Validation Loss: 0.10271928459405899\n",
      "Epoch: 930/2000, Train Loss: 0.0816253125667572, Validation Loss: 0.10262635350227356\n",
      "Epoch: 931/2000, Train Loss: 0.08156918734312057, Validation Loss: 0.10262902826070786\n",
      "Epoch: 932/2000, Train Loss: 0.08151119202375412, Validation Loss: 0.10258051753044128\n",
      "Epoch: 933/2000, Train Loss: 0.0814555436372757, Validation Loss: 0.10253395885229111\n",
      "Epoch: 934/2000, Train Loss: 0.08140319585800171, Validation Loss: 0.10253935307264328\n",
      "Epoch: 935/2000, Train Loss: 0.08135157078504562, Validation Loss: 0.10246217995882034\n",
      "Epoch: 936/2000, Train Loss: 0.08129794895648956, Validation Loss: 0.10246676951646805\n",
      "Epoch: 937/2000, Train Loss: 0.08124218881130219, Validation Loss: 0.10240843147039413\n",
      "Epoch: 938/2000, Train Loss: 0.08118642866611481, Validation Loss: 0.1023775115609169\n",
      "Epoch: 939/2000, Train Loss: 0.0811324194073677, Validation Loss: 0.1023625060915947\n",
      "Epoch: 940/2000, Train Loss: 0.0810798704624176, Validation Loss: 0.10230013728141785\n",
      "Epoch: 941/2000, Train Loss: 0.08102720230817795, Validation Loss: 0.10229996591806412\n",
      "Epoch: 942/2000, Train Loss: 0.08097328990697861, Validation Loss: 0.10223778337240219\n",
      "Epoch: 943/2000, Train Loss: 0.08091846108436584, Validation Loss: 0.10221853107213974\n",
      "Epoch: 944/2000, Train Loss: 0.08086390048265457, Validation Loss: 0.10218564420938492\n",
      "Epoch: 945/2000, Train Loss: 0.0808103010058403, Validation Loss: 0.10213922709226608\n",
      "Epoch: 946/2000, Train Loss: 0.08075740933418274, Validation Loss: 0.10212956368923187\n",
      "Epoch: 947/2000, Train Loss: 0.08070439100265503, Validation Loss: 0.1020716056227684\n",
      "Epoch: 948/2000, Train Loss: 0.0806507095694542, Validation Loss: 0.10205889493227005\n",
      "Epoch: 949/2000, Train Loss: 0.08059658110141754, Validation Loss: 0.10201378911733627\n",
      "Epoch: 950/2000, Train Loss: 0.08054258674383163, Validation Loss: 0.10198180377483368\n",
      "Epoch: 951/2000, Train Loss: 0.08048907667398453, Validation Loss: 0.10195830464363098\n",
      "Epoch: 952/2000, Train Loss: 0.08043598383665085, Validation Loss: 0.10190954059362411\n",
      "Epoch: 953/2000, Train Loss: 0.08038288354873657, Validation Loss: 0.10189495235681534\n",
      "Epoch: 954/2000, Train Loss: 0.08032950758934021, Validation Loss: 0.10184508562088013\n",
      "Epoch: 955/2000, Train Loss: 0.08027587085962296, Validation Loss: 0.1018228605389595\n",
      "Epoch: 956/2000, Train Loss: 0.0802222192287445, Validation Loss: 0.10178631544113159\n",
      "Epoch: 957/2000, Train Loss: 0.08016878366470337, Validation Loss: 0.10174985975027084\n",
      "Epoch: 958/2000, Train Loss: 0.08011562377214432, Validation Loss: 0.10172710567712784\n",
      "Epoch: 959/2000, Train Loss: 0.08006257563829422, Validation Loss: 0.10168146342039108\n",
      "Epoch: 960/2000, Train Loss: 0.08000944554805756, Validation Loss: 0.10166191309690475\n",
      "Epoch: 961/2000, Train Loss: 0.07995618879795074, Validation Loss: 0.10161826759576797\n",
      "Epoch: 962/2000, Train Loss: 0.07990287244319916, Validation Loss: 0.1015918105840683\n",
      "Epoch: 963/2000, Train Loss: 0.07984961569309235, Validation Loss: 0.101558156311512\n",
      "Epoch: 964/2000, Train Loss: 0.07979650050401688, Validation Loss: 0.10152191668748856\n",
      "Epoch: 965/2000, Train Loss: 0.07974350452423096, Validation Loss: 0.10149721801280975\n",
      "Epoch: 966/2000, Train Loss: 0.07969056069850922, Validation Loss: 0.10145547240972519\n",
      "Epoch: 967/2000, Train Loss: 0.07963757961988449, Validation Loss: 0.1014326810836792\n",
      "Epoch: 968/2000, Train Loss: 0.07958455383777618, Validation Loss: 0.10139264911413193\n",
      "Epoch: 969/2000, Train Loss: 0.07953149825334549, Validation Loss: 0.10136527568101883\n",
      "Epoch: 970/2000, Train Loss: 0.07947847992181778, Validation Loss: 0.10133185237646103\n",
      "Epoch: 971/2000, Train Loss: 0.07942552119493484, Validation Loss: 0.10129785537719727\n",
      "Epoch: 972/2000, Train Loss: 0.07937263697385788, Validation Loss: 0.10127073526382446\n",
      "Epoch: 973/2000, Train Loss: 0.0793197751045227, Validation Loss: 0.10123256593942642\n",
      "Epoch: 974/2000, Train Loss: 0.07926691323518753, Validation Loss: 0.10120773315429688\n",
      "Epoch: 975/2000, Train Loss: 0.07921403646469116, Validation Loss: 0.10116999596357346\n",
      "Epoch: 976/2000, Train Loss: 0.0791611447930336, Validation Loss: 0.1011432409286499\n",
      "Epoch: 977/2000, Train Loss: 0.07910826057195663, Validation Loss: 0.10110945999622345\n",
      "Epoch: 978/2000, Train Loss: 0.07905540615320206, Validation Loss: 0.10107866674661636\n",
      "Epoch: 979/2000, Train Loss: 0.07900258153676987, Validation Loss: 0.10104946047067642\n",
      "Epoch: 980/2000, Train Loss: 0.07894978672266006, Validation Loss: 0.10101509839296341\n",
      "Epoch: 981/2000, Train Loss: 0.07889701426029205, Validation Loss: 0.10098858177661896\n",
      "Epoch: 982/2000, Train Loss: 0.07884424179792404, Validation Loss: 0.10095296800136566\n",
      "Epoch: 983/2000, Train Loss: 0.07879148423671722, Validation Loss: 0.1009264662861824\n",
      "Epoch: 984/2000, Train Loss: 0.07873871922492981, Validation Loss: 0.10089205205440521\n",
      "Epoch: 985/2000, Train Loss: 0.07868597656488419, Validation Loss: 0.10086352378129959\n",
      "Epoch: 986/2000, Train Loss: 0.07863324880599976, Validation Loss: 0.10083167254924774\n",
      "Epoch: 987/2000, Train Loss: 0.07858054339885712, Validation Loss: 0.10080043971538544\n",
      "Epoch: 988/2000, Train Loss: 0.07852787524461746, Validation Loss: 0.10077112168073654\n",
      "Epoch: 989/2000, Train Loss: 0.0784752294421196, Validation Loss: 0.10073789954185486\n",
      "Epoch: 990/2000, Train Loss: 0.07842259854078293, Validation Loss: 0.10071013867855072\n",
      "Epoch: 991/2000, Train Loss: 0.07836999744176865, Validation Loss: 0.10067638009786606\n",
      "Epoch: 992/2000, Train Loss: 0.07831742614507675, Validation Loss: 0.1006489023566246\n",
      "Epoch: 993/2000, Train Loss: 0.07826486974954605, Validation Loss: 0.10061593353748322\n",
      "Epoch: 994/2000, Train Loss: 0.07821234315633774, Validation Loss: 0.10058768093585968\n",
      "Epoch: 995/2000, Train Loss: 0.07815983891487122, Validation Loss: 0.10055630654096603\n",
      "Epoch: 996/2000, Train Loss: 0.07810737192630768, Validation Loss: 0.10052680969238281\n",
      "Epoch: 997/2000, Train Loss: 0.07805492728948593, Validation Loss: 0.10049712657928467\n",
      "Epoch: 998/2000, Train Loss: 0.07800250500440598, Validation Loss: 0.10046643018722534\n",
      "Epoch: 999/2000, Train Loss: 0.077950119972229, Validation Loss: 0.10043800622224808\n",
      "Epoch: 1000/2000, Train Loss: 0.07789774984121323, Validation Loss: 0.10040648281574249\n",
      "Epoch: 1001/2000, Train Loss: 0.07784540951251984, Validation Loss: 0.10037866234779358\n",
      "Epoch: 1002/2000, Train Loss: 0.07779308408498764, Validation Loss: 0.10034685581922531\n",
      "Epoch: 1003/2000, Train Loss: 0.07774077355861664, Validation Loss: 0.10031906515359879\n",
      "Epoch: 1004/2000, Train Loss: 0.07768848538398743, Validation Loss: 0.1002875342965126\n",
      "Epoch: 1005/2000, Train Loss: 0.07763620465993881, Validation Loss: 0.10025942325592041\n",
      "Epoch: 1006/2000, Train Loss: 0.07758394628763199, Validation Loss: 0.10022855550050735\n",
      "Epoch: 1007/2000, Train Loss: 0.07753168791532516, Validation Loss: 0.10019996017217636\n",
      "Epoch: 1008/2000, Train Loss: 0.07747945934534073, Validation Loss: 0.10016994178295135\n",
      "Epoch: 1009/2000, Train Loss: 0.07742723822593689, Validation Loss: 0.10014091432094574\n",
      "Epoch: 1010/2000, Train Loss: 0.07737505435943604, Validation Loss: 0.10011176764965057\n",
      "Epoch: 1011/2000, Train Loss: 0.07732287049293518, Validation Loss: 0.1000823825597763\n",
      "Epoch: 1012/2000, Train Loss: 0.07727070897817612, Validation Loss: 0.10005390644073486\n",
      "Epoch: 1013/2000, Train Loss: 0.07721856236457825, Validation Loss: 0.10002423077821732\n",
      "Epoch: 1014/2000, Train Loss: 0.07716643065214157, Validation Loss: 0.09999622404575348\n",
      "Epoch: 1015/2000, Train Loss: 0.0771142989397049, Validation Loss: 0.09996632486581802\n",
      "Epoch: 1016/2000, Train Loss: 0.07706218212842941, Validation Loss: 0.09993863105773926\n",
      "Epoch: 1017/2000, Train Loss: 0.07701006531715393, Validation Loss: 0.09990864992141724\n",
      "Epoch: 1018/2000, Train Loss: 0.07695796340703964, Validation Loss: 0.09988121688365936\n",
      "Epoch: 1019/2000, Train Loss: 0.07690587639808655, Validation Loss: 0.09985129535198212\n",
      "Epoch: 1020/2000, Train Loss: 0.07685380429029465, Validation Loss: 0.09982416033744812\n",
      "Epoch: 1021/2000, Train Loss: 0.07680173963308334, Validation Loss: 0.09979438781738281\n",
      "Epoch: 1022/2000, Train Loss: 0.07674971222877502, Validation Loss: 0.09976761788129807\n",
      "Epoch: 1023/2000, Train Loss: 0.0766976922750473, Validation Loss: 0.09973803162574768\n",
      "Epoch: 1024/2000, Train Loss: 0.07664570957422256, Validation Loss: 0.09971165657043457\n",
      "Epoch: 1025/2000, Train Loss: 0.07659374922513962, Validation Loss: 0.09968212991952896\n",
      "Epoch: 1026/2000, Train Loss: 0.07654182612895966, Validation Loss: 0.09965615719556808\n",
      "Epoch: 1027/2000, Train Loss: 0.07648993283510208, Validation Loss: 0.09962650388479233\n",
      "Epoch: 1028/2000, Train Loss: 0.0764380693435669, Validation Loss: 0.09960095584392548\n",
      "Epoch: 1029/2000, Train Loss: 0.07638625055551529, Validation Loss: 0.09957097470760345\n",
      "Epoch: 1030/2000, Train Loss: 0.07633446902036667, Validation Loss: 0.09954600036144257\n",
      "Epoch: 1031/2000, Train Loss: 0.07628271728754044, Validation Loss: 0.0995153859257698\n",
      "Epoch: 1032/2000, Train Loss: 0.07623100280761719, Validation Loss: 0.09949126094579697\n",
      "Epoch: 1033/2000, Train Loss: 0.07617933303117752, Validation Loss: 0.09945966303348541\n",
      "Epoch: 1034/2000, Train Loss: 0.07612770050764084, Validation Loss: 0.09943679720163345\n",
      "Epoch: 1035/2000, Train Loss: 0.07607611268758774, Validation Loss: 0.09940364211797714\n",
      "Epoch: 1036/2000, Train Loss: 0.07602456957101822, Validation Loss: 0.09938272833824158\n",
      "Epoch: 1037/2000, Train Loss: 0.07597307115793228, Validation Loss: 0.09934704750776291\n",
      "Epoch: 1038/2000, Train Loss: 0.07592163234949112, Validation Loss: 0.09932923316955566\n",
      "Epoch: 1039/2000, Train Loss: 0.07587024569511414, Validation Loss: 0.09928955882787704\n",
      "Epoch: 1040/2000, Train Loss: 0.07581895589828491, Validation Loss: 0.09927678108215332\n",
      "Epoch: 1041/2000, Train Loss: 0.07576776295900345, Validation Loss: 0.09923069924116135\n",
      "Epoch: 1042/2000, Train Loss: 0.07571673393249512, Validation Loss: 0.0992264598608017\n",
      "Epoch: 1043/2000, Train Loss: 0.07566593587398529, Validation Loss: 0.09917010366916656\n",
      "Epoch: 1044/2000, Train Loss: 0.07561551034450531, Validation Loss: 0.09918072074651718\n",
      "Epoch: 1045/2000, Train Loss: 0.07556569576263428, Validation Loss: 0.09910768270492554\n",
      "Epoch: 1046/2000, Train Loss: 0.07551688700914383, Validation Loss: 0.09914496541023254\n",
      "Epoch: 1047/2000, Train Loss: 0.07546976208686829, Validation Loss: 0.09904512017965317\n",
      "Epoch: 1048/2000, Train Loss: 0.0754256471991539, Validation Loss: 0.09913225471973419\n",
      "Epoch: 1049/2000, Train Loss: 0.07538646459579468, Validation Loss: 0.09899165481328964\n",
      "Epoch: 1050/2000, Train Loss: 0.07535658776760101, Validation Loss: 0.09917684644460678\n",
      "Epoch: 1051/2000, Train Loss: 0.07534142583608627, Validation Loss: 0.09898355603218079\n",
      "Epoch: 1052/2000, Train Loss: 0.07535523176193237, Validation Loss: 0.09936902672052383\n",
      "Epoch: 1053/2000, Train Loss: 0.07541022449731827, Validation Loss: 0.099136583507061\n",
      "Epoch: 1054/2000, Train Loss: 0.0755486711859703, Validation Loss: 0.09990264475345612\n",
      "Epoch: 1055/2000, Train Loss: 0.07576949149370193, Validation Loss: 0.09967073798179626\n",
      "Epoch: 1056/2000, Train Loss: 0.07615375518798828, Validation Loss: 0.10082357376813889\n",
      "Epoch: 1057/2000, Train Loss: 0.07648776471614838, Validation Loss: 0.10027045011520386\n",
      "Epoch: 1058/2000, Train Loss: 0.07678240537643433, Validation Loss: 0.10078044980764389\n",
      "Epoch: 1059/2000, Train Loss: 0.07639188319444656, Validation Loss: 0.09933220595121384\n",
      "Epoch: 1060/2000, Train Loss: 0.07566417753696442, Validation Loss: 0.09901702404022217\n",
      "Epoch: 1061/2000, Train Loss: 0.07490471005439758, Validation Loss: 0.09883933514356613\n",
      "Epoch: 1062/2000, Train Loss: 0.07476130872964859, Validation Loss: 0.09894455969333649\n",
      "Epoch: 1063/2000, Train Loss: 0.0751444548368454, Validation Loss: 0.0998215302824974\n",
      "Epoch: 1064/2000, Train Loss: 0.07544752955436707, Validation Loss: 0.09912125021219254\n",
      "Epoch: 1065/2000, Train Loss: 0.0753309428691864, Validation Loss: 0.09908617287874222\n",
      "Epoch: 1066/2000, Train Loss: 0.07480866461992264, Validation Loss: 0.09855917096138\n",
      "Epoch: 1067/2000, Train Loss: 0.07447521388530731, Validation Loss: 0.09856519848108292\n",
      "Epoch: 1068/2000, Train Loss: 0.07456475496292114, Validation Loss: 0.09916488826274872\n",
      "Epoch: 1069/2000, Train Loss: 0.07479386031627655, Validation Loss: 0.09874767065048218\n",
      "Epoch: 1070/2000, Train Loss: 0.07479744404554367, Validation Loss: 0.09885386377573013\n",
      "Epoch: 1071/2000, Train Loss: 0.07449381798505783, Validation Loss: 0.09843916445970535\n",
      "Epoch: 1072/2000, Train Loss: 0.07424259930849075, Validation Loss: 0.09842539578676224\n",
      "Epoch: 1073/2000, Train Loss: 0.074251189827919, Validation Loss: 0.0988280400633812\n",
      "Epoch: 1074/2000, Train Loss: 0.07437577098608017, Validation Loss: 0.09850116074085236\n",
      "Epoch: 1075/2000, Train Loss: 0.07436853647232056, Validation Loss: 0.0986180528998375\n",
      "Epoch: 1076/2000, Train Loss: 0.07417242228984833, Validation Loss: 0.09831531345844269\n",
      "Epoch: 1077/2000, Train Loss: 0.07400205731391907, Validation Loss: 0.09828177839517593\n",
      "Epoch: 1078/2000, Train Loss: 0.07398898899555206, Validation Loss: 0.09855683892965317\n",
      "Epoch: 1079/2000, Train Loss: 0.07404538244009018, Validation Loss: 0.09828798472881317\n",
      "Epoch: 1080/2000, Train Loss: 0.0740189328789711, Validation Loss: 0.09839987009763718\n",
      "Epoch: 1081/2000, Train Loss: 0.07388227432966232, Validation Loss: 0.09819141775369644\n",
      "Epoch: 1082/2000, Train Loss: 0.07376357167959213, Validation Loss: 0.09815575182437897\n",
      "Epoch: 1083/2000, Train Loss: 0.0737391784787178, Validation Loss: 0.09834866225719452\n",
      "Epoch: 1084/2000, Train Loss: 0.0737541988492012, Validation Loss: 0.0981311947107315\n",
      "Epoch: 1085/2000, Train Loss: 0.07371831685304642, Validation Loss: 0.09822928160429001\n",
      "Epoch: 1086/2000, Train Loss: 0.07361814379692078, Validation Loss: 0.0980733186006546\n",
      "Epoch: 1087/2000, Train Loss: 0.07352810353040695, Validation Loss: 0.09803647547960281\n",
      "Epoch: 1088/2000, Train Loss: 0.07349365949630737, Validation Loss: 0.09816647320985794\n",
      "Epoch: 1089/2000, Train Loss: 0.0734839215874672, Validation Loss: 0.09799302369356155\n",
      "Epoch: 1090/2000, Train Loss: 0.0734451487660408, Validation Loss: 0.09807679057121277\n",
      "Epoch: 1091/2000, Train Loss: 0.07336794584989548, Validation Loss: 0.09795276820659637\n",
      "Epoch: 1092/2000, Train Loss: 0.07329423725605011, Validation Loss: 0.0979200005531311\n",
      "Epoch: 1093/2000, Train Loss: 0.07325255125761032, Validation Loss: 0.09800256788730621\n",
      "Epoch: 1094/2000, Train Loss: 0.07322796434164047, Validation Loss: 0.0978640466928482\n",
      "Epoch: 1095/2000, Train Loss: 0.07318876683712006, Validation Loss: 0.09793475270271301\n",
      "Epoch: 1096/2000, Train Loss: 0.07312610745429993, Validation Loss: 0.09782694280147552\n",
      "Epoch: 1097/2000, Train Loss: 0.0730617418885231, Validation Loss: 0.09780396521091461\n",
      "Epoch: 1098/2000, Train Loss: 0.07301469147205353, Validation Loss: 0.09784935414791107\n",
      "Epoch: 1099/2000, Train Loss: 0.07298058271408081, Validation Loss: 0.0977432131767273\n",
      "Epoch: 1100/2000, Train Loss: 0.07294127345085144, Validation Loss: 0.09780395776033401\n",
      "Epoch: 1101/2000, Train Loss: 0.07288790494203568, Validation Loss: 0.09770848602056503\n",
      "Epoch: 1102/2000, Train Loss: 0.07282997667789459, Validation Loss: 0.09769769757986069\n",
      "Epoch: 1103/2000, Train Loss: 0.07278004288673401, Validation Loss: 0.09771176427602768\n",
      "Epoch: 1104/2000, Train Loss: 0.07273952662944794, Validation Loss: 0.09763059765100479\n",
      "Epoch: 1105/2000, Train Loss: 0.07269924879074097, Validation Loss: 0.09767612814903259\n",
      "Epoch: 1106/2000, Train Loss: 0.07265154272317886, Validation Loss: 0.09758665412664413\n",
      "Epoch: 1107/2000, Train Loss: 0.07259867340326309, Validation Loss: 0.09758399426937103\n",
      "Epoch: 1108/2000, Train Loss: 0.07254793494939804, Validation Loss: 0.09757106751203537\n",
      "Epoch: 1109/2000, Train Loss: 0.07250309735536575, Validation Loss: 0.09751347452402115\n",
      "Epoch: 1110/2000, Train Loss: 0.07246096432209015, Validation Loss: 0.09754642099142075\n",
      "Epoch: 1111/2000, Train Loss: 0.07241616398096085, Validation Loss: 0.09746991842985153\n",
      "Epoch: 1112/2000, Train Loss: 0.07236732542514801, Validation Loss: 0.09747733175754547\n",
      "Epoch: 1113/2000, Train Loss: 0.07231748104095459, Validation Loss: 0.09744485467672348\n",
      "Epoch: 1114/2000, Train Loss: 0.07227024435997009, Validation Loss: 0.09740648418664932\n",
      "Epoch: 1115/2000, Train Loss: 0.07222586870193481, Validation Loss: 0.09742061048746109\n",
      "Epoch: 1116/2000, Train Loss: 0.07218175381422043, Validation Loss: 0.09735605865716934\n",
      "Epoch: 1117/2000, Train Loss: 0.07213561981916428, Validation Loss: 0.09736667573451996\n",
      "Epoch: 1118/2000, Train Loss: 0.07208764553070068, Validation Loss: 0.09732118248939514\n",
      "Epoch: 1119/2000, Train Loss: 0.07203983515501022, Validation Loss: 0.09729927033185959\n",
      "Epoch: 1120/2000, Train Loss: 0.07199365645647049, Validation Loss: 0.0972939059138298\n",
      "Epoch: 1121/2000, Train Loss: 0.07194878160953522, Validation Loss: 0.09724435955286026\n",
      "Epoch: 1122/2000, Train Loss: 0.07190374284982681, Validation Loss: 0.09725262224674225\n",
      "Epoch: 1123/2000, Train Loss: 0.07185754925012589, Validation Loss: 0.09720338135957718\n",
      "Epoch: 1124/2000, Train Loss: 0.0718105211853981, Validation Loss: 0.09719403833150864\n",
      "Epoch: 1125/2000, Train Loss: 0.07176368683576584, Validation Loss: 0.09717081487178802\n",
      "Epoch: 1126/2000, Train Loss: 0.07171773910522461, Validation Loss: 0.09713638573884964\n",
      "Epoch: 1127/2000, Train Loss: 0.0716724619269371, Validation Loss: 0.09713472425937653\n",
      "Epoch: 1128/2000, Train Loss: 0.07162710279226303, Validation Loss: 0.09708918631076813\n",
      "Epoch: 1129/2000, Train Loss: 0.071581169962883, Validation Loss: 0.09708584100008011\n",
      "Epoch: 1130/2000, Train Loss: 0.07153478264808655, Validation Loss: 0.09705107659101486\n",
      "Epoch: 1131/2000, Train Loss: 0.07148844003677368, Validation Loss: 0.09703074395656586\n",
      "Epoch: 1132/2000, Train Loss: 0.07144252955913544, Validation Loss: 0.09701572358608246\n",
      "Epoch: 1133/2000, Train Loss: 0.07139702141284943, Validation Loss: 0.09697972238063812\n",
      "Epoch: 1134/2000, Train Loss: 0.0713515654206276, Validation Loss: 0.09697404503822327\n",
      "Epoch: 1135/2000, Train Loss: 0.07130587100982666, Validation Loss: 0.09693563729524612\n",
      "Epoch: 1136/2000, Train Loss: 0.07125990837812424, Validation Loss: 0.09692398458719254\n",
      "Epoch: 1137/2000, Train Loss: 0.07121388614177704, Validation Loss: 0.09689656645059586\n",
      "Epoch: 1138/2000, Train Loss: 0.07116801291704178, Validation Loss: 0.09687214344739914\n",
      "Epoch: 1139/2000, Train Loss: 0.07112239301204681, Validation Loss: 0.09685776382684708\n",
      "Epoch: 1140/2000, Train Loss: 0.0710768923163414, Validation Loss: 0.09682432562112808\n",
      "Epoch: 1141/2000, Train Loss: 0.0710313469171524, Validation Loss: 0.09681416302919388\n",
      "Epoch: 1142/2000, Train Loss: 0.07098568230867386, Validation Loss: 0.09678123146295547\n",
      "Epoch: 1143/2000, Train Loss: 0.07093992084264755, Validation Loss: 0.09676565229892731\n",
      "Epoch: 1144/2000, Train Loss: 0.07089417427778244, Validation Loss: 0.09674099087715149\n",
      "Epoch: 1145/2000, Train Loss: 0.07084853202104568, Validation Loss: 0.09671662002801895\n",
      "Epoch: 1146/2000, Train Loss: 0.07080299407243729, Validation Loss: 0.096700519323349\n",
      "Epoch: 1147/2000, Train Loss: 0.07075753062963486, Validation Loss: 0.09667041897773743\n",
      "Epoch: 1148/2000, Train Loss: 0.07071204483509064, Validation Loss: 0.09665701538324356\n",
      "Epoch: 1149/2000, Train Loss: 0.07066651433706284, Validation Loss: 0.0966273620724678\n",
      "Epoch: 1150/2000, Train Loss: 0.07062094658613205, Validation Loss: 0.09661050885915756\n",
      "Epoch: 1151/2000, Train Loss: 0.07057540118694305, Validation Loss: 0.09658628702163696\n",
      "Epoch: 1152/2000, Train Loss: 0.07052990794181824, Validation Loss: 0.09656357020139694\n",
      "Epoch: 1153/2000, Train Loss: 0.07048448175191879, Validation Loss: 0.09654521197080612\n",
      "Epoch: 1154/2000, Train Loss: 0.07043911516666412, Validation Loss: 0.0965181514620781\n",
      "Epoch: 1155/2000, Train Loss: 0.07039374858140945, Validation Loss: 0.09650234133005142\n",
      "Epoch: 1156/2000, Train Loss: 0.07034838944673538, Validation Loss: 0.09647475183010101\n",
      "Epoch: 1157/2000, Train Loss: 0.07030302286148071, Validation Loss: 0.09645766019821167\n",
      "Epoch: 1158/2000, Train Loss: 0.07025767117738724, Validation Loss: 0.09643305838108063\n",
      "Epoch: 1159/2000, Train Loss: 0.07021234184503555, Validation Loss: 0.09641242772340775\n",
      "Epoch: 1160/2000, Train Loss: 0.07016705721616745, Validation Loss: 0.09639181941747665\n",
      "Epoch: 1161/2000, Train Loss: 0.07012181729078293, Validation Loss: 0.09636770188808441\n",
      "Epoch: 1162/2000, Train Loss: 0.0700766071677208, Validation Loss: 0.09634972363710403\n",
      "Epoch: 1163/2000, Train Loss: 0.07003141194581985, Validation Loss: 0.09632403403520584\n",
      "Epoch: 1164/2000, Train Loss: 0.0699862390756607, Validation Loss: 0.09630642831325531\n",
      "Epoch: 1165/2000, Train Loss: 0.06994107365608215, Validation Loss: 0.09628152847290039\n",
      "Epoch: 1166/2000, Train Loss: 0.0698959231376648, Validation Loss: 0.09626240283250809\n",
      "Epoch: 1167/2000, Train Loss: 0.06985079497098923, Validation Loss: 0.09623973816633224\n",
      "Epoch: 1168/2000, Train Loss: 0.06980568915605545, Validation Loss: 0.09621834754943848\n",
      "Epoch: 1169/2000, Train Loss: 0.06976061314344406, Validation Loss: 0.09619807451963425\n",
      "Epoch: 1170/2000, Train Loss: 0.06971555948257446, Validation Loss: 0.09617485105991364\n",
      "Epoch: 1171/2000, Train Loss: 0.06967050582170486, Validation Loss: 0.09615606814622879\n",
      "Epoch: 1172/2000, Train Loss: 0.06962547451257706, Validation Loss: 0.09613209217786789\n",
      "Epoch: 1173/2000, Train Loss: 0.06958043575286865, Validation Loss: 0.09611347317695618\n",
      "Epoch: 1174/2000, Train Loss: 0.06953538209199905, Validation Loss: 0.0960899069905281\n",
      "Epoch: 1175/2000, Train Loss: 0.06949029862880707, Validation Loss: 0.09607043862342834\n",
      "Epoch: 1176/2000, Train Loss: 0.06944519281387329, Validation Loss: 0.09604804217815399\n",
      "Epoch: 1177/2000, Train Loss: 0.06940004229545593, Validation Loss: 0.09602727741003036\n",
      "Epoch: 1178/2000, Train Loss: 0.06935489177703857, Validation Loss: 0.09600625932216644\n",
      "Epoch: 1179/2000, Train Loss: 0.0693097934126854, Validation Loss: 0.0959843248128891\n",
      "Epoch: 1180/2000, Train Loss: 0.06926478445529938, Validation Loss: 0.09596437960863113\n",
      "Epoch: 1181/2000, Train Loss: 0.06921985745429993, Validation Loss: 0.09594173729419708\n",
      "Epoch: 1182/2000, Train Loss: 0.06917499750852585, Validation Loss: 0.09592234343290329\n",
      "Epoch: 1183/2000, Train Loss: 0.06913018971681595, Validation Loss: 0.09589958190917969\n",
      "Epoch: 1184/2000, Train Loss: 0.06908541917800903, Validation Loss: 0.0958801805973053\n",
      "Epoch: 1185/2000, Train Loss: 0.0690406858921051, Validation Loss: 0.09585774689912796\n",
      "Epoch: 1186/2000, Train Loss: 0.06899599730968475, Validation Loss: 0.09583791345357895\n",
      "Epoch: 1187/2000, Train Loss: 0.06895133852958679, Validation Loss: 0.09581603109836578\n",
      "Epoch: 1188/2000, Train Loss: 0.06890667974948883, Validation Loss: 0.09579546004533768\n",
      "Epoch: 1189/2000, Train Loss: 0.06886199861764908, Validation Loss: 0.09577406197786331\n",
      "Epoch: 1190/2000, Train Loss: 0.06881728768348694, Validation Loss: 0.09575265645980835\n",
      "Epoch: 1191/2000, Train Loss: 0.06877249479293823, Validation Loss: 0.09573156386613846\n",
      "Epoch: 1192/2000, Train Loss: 0.06872760504484177, Validation Loss: 0.09570939093828201\n",
      "Epoch: 1193/2000, Train Loss: 0.06868255138397217, Validation Loss: 0.0956883504986763\n",
      "Epoch: 1194/2000, Train Loss: 0.06863725930452347, Validation Loss: 0.0956653505563736\n",
      "Epoch: 1195/2000, Train Loss: 0.0685916319489479, Validation Loss: 0.09564365446567535\n",
      "Epoch: 1196/2000, Train Loss: 0.06854581087827682, Validation Loss: 0.09561900794506073\n",
      "Epoch: 1197/2000, Train Loss: 0.0685001015663147, Validation Loss: 0.09559538215398788\n",
      "Epoch: 1198/2000, Train Loss: 0.06845454126596451, Validation Loss: 0.09557008743286133\n",
      "Epoch: 1199/2000, Train Loss: 0.06840860843658447, Validation Loss: 0.09554913640022278\n",
      "Epoch: 1200/2000, Train Loss: 0.06836257129907608, Validation Loss: 0.09552807360887527\n",
      "Epoch: 1201/2000, Train Loss: 0.06831692159175873, Validation Loss: 0.09550902247428894\n",
      "Epoch: 1202/2000, Train Loss: 0.06827156245708466, Validation Loss: 0.09548746049404144\n",
      "Epoch: 1203/2000, Train Loss: 0.06822623312473297, Validation Loss: 0.09546611458063126\n",
      "Epoch: 1204/2000, Train Loss: 0.06818090379238129, Validation Loss: 0.09544200450181961\n",
      "Epoch: 1205/2000, Train Loss: 0.06813562661409378, Validation Loss: 0.09541834890842438\n",
      "Epoch: 1206/2000, Train Loss: 0.06809046864509583, Validation Loss: 0.09539327025413513\n",
      "Epoch: 1207/2000, Train Loss: 0.06804537028074265, Validation Loss: 0.0953696146607399\n",
      "Epoch: 1208/2000, Train Loss: 0.06800026446580887, Validation Loss: 0.09534528851509094\n",
      "Epoch: 1209/2000, Train Loss: 0.06795510649681091, Validation Loss: 0.09532201290130615\n",
      "Epoch: 1210/2000, Train Loss: 0.06790993362665176, Validation Loss: 0.0952971801161766\n",
      "Epoch: 1211/2000, Train Loss: 0.06786476820707321, Validation Loss: 0.09527193009853363\n",
      "Epoch: 1212/2000, Train Loss: 0.06781960278749466, Validation Loss: 0.09524375945329666\n",
      "Epoch: 1213/2000, Train Loss: 0.06777441501617432, Validation Loss: 0.09521424770355225\n",
      "Epoch: 1214/2000, Train Loss: 0.06772920489311218, Validation Loss: 0.09518174827098846\n",
      "Epoch: 1215/2000, Train Loss: 0.06768397241830826, Validation Loss: 0.09514953196048737\n",
      "Epoch: 1216/2000, Train Loss: 0.06763873249292374, Validation Loss: 0.09511669725179672\n",
      "Epoch: 1217/2000, Train Loss: 0.06759344041347504, Validation Loss: 0.0950864851474762\n",
      "Epoch: 1218/2000, Train Loss: 0.06754807382822037, Validation Loss: 0.09505575895309448\n",
      "Epoch: 1219/2000, Train Loss: 0.06750266253948212, Validation Loss: 0.09502670913934708\n",
      "Epoch: 1220/2000, Train Loss: 0.06745722889900208, Validation Loss: 0.09499502927064896\n",
      "Epoch: 1221/2000, Train Loss: 0.06741173565387726, Validation Loss: 0.0949653834104538\n",
      "Epoch: 1222/2000, Train Loss: 0.06736621260643005, Validation Loss: 0.09493377804756165\n",
      "Epoch: 1223/2000, Train Loss: 0.06732065230607986, Validation Loss: 0.09490739554166794\n",
      "Epoch: 1224/2000, Train Loss: 0.0672750398516655, Validation Loss: 0.0948791578412056\n",
      "Epoch: 1225/2000, Train Loss: 0.06722936779260635, Validation Loss: 0.09485732018947601\n",
      "Epoch: 1226/2000, Train Loss: 0.06718368083238602, Validation Loss: 0.09483018517494202\n",
      "Epoch: 1227/2000, Train Loss: 0.06713799387216568, Validation Loss: 0.09481079131364822\n",
      "Epoch: 1228/2000, Train Loss: 0.06709235906600952, Validation Loss: 0.09478243440389633\n",
      "Epoch: 1229/2000, Train Loss: 0.06704676896333694, Validation Loss: 0.09476625919342041\n",
      "Epoch: 1230/2000, Train Loss: 0.06700120121240616, Validation Loss: 0.0947355180978775\n",
      "Epoch: 1231/2000, Train Loss: 0.06695562601089478, Validation Loss: 0.09472351521253586\n",
      "Epoch: 1232/2000, Train Loss: 0.0669100284576416, Validation Loss: 0.09468688070774078\n",
      "Epoch: 1233/2000, Train Loss: 0.06686440855264664, Validation Loss: 0.09468145668506622\n",
      "Epoch: 1234/2000, Train Loss: 0.06681879609823227, Validation Loss: 0.09463537484407425\n",
      "Epoch: 1235/2000, Train Loss: 0.06677331775426865, Validation Loss: 0.09464407712221146\n",
      "Epoch: 1236/2000, Train Loss: 0.06672818958759308, Validation Loss: 0.09458257257938385\n",
      "Epoch: 1237/2000, Train Loss: 0.06668393313884735, Validation Loss: 0.09461832791566849\n",
      "Epoch: 1238/2000, Train Loss: 0.06664139032363892, Validation Loss: 0.09452994167804718\n",
      "Epoch: 1239/2000, Train Loss: 0.06660250574350357, Validation Loss: 0.09462354332208633\n",
      "Epoch: 1240/2000, Train Loss: 0.06657053530216217, Validation Loss: 0.09449462592601776\n",
      "Epoch: 1241/2000, Train Loss: 0.06655276566743851, Validation Loss: 0.09471727907657623\n",
      "Epoch: 1242/2000, Train Loss: 0.06656105071306229, Validation Loss: 0.0945490375161171\n",
      "Epoch: 1243/2000, Train Loss: 0.06662146002054214, Validation Loss: 0.09507182240486145\n",
      "Epoch: 1244/2000, Train Loss: 0.06676526367664337, Validation Loss: 0.09494556486606598\n",
      "Epoch: 1245/2000, Train Loss: 0.06707458198070526, Validation Loss: 0.09606854617595673\n",
      "Epoch: 1246/2000, Train Loss: 0.06753870844841003, Validation Loss: 0.09607352316379547\n",
      "Epoch: 1247/2000, Train Loss: 0.06827782839536667, Validation Loss: 0.09742213785648346\n",
      "\n",
      "Early stopping at epoch 1247 due to validation loss increasing for 3 consecutive epochs.\n",
      "\n",
      "Accuracy: 0.9655\n",
      "\n",
      "Class 0 - Precision: 0.9615, Recall: 0.9704, F1 Score: 0.9659\n",
      "Class 1 - Precision: 0.9697, Recall: 0.9606, F1 Score: 0.9651\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3148   96]\n",
      " [ 126 3073]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "optimizer_lstm = optim.Adam(lstm.parameters(), lr=0.001)\n",
    "lstm, lstm_train_losses, lstm_val_losses = train_model(lstm, optimizer_lstm, criterion, X_train, y_train, X_val, y_val)\n",
    "\n",
    "accuracy, report, conf_matrix = evaluate_model(lstm, X_test, y_test)\n",
    "print_evaluation_metrics(accuracy, report, conf_matrix, lstm_train_losses, lstm_val_losses, \"LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "70aabc7e1d3be7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-16T02:15:03.956577300Z",
     "start_time": "2023-08-16T02:14:16.606533500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2000, Train Loss: 0.6929253339767456, Validation Loss: 0.6764959096908569\n",
      "Epoch: 2/2000, Train Loss: 0.6755416989326477, Validation Loss: 0.6630359888076782\n",
      "Epoch: 3/2000, Train Loss: 0.6615344285964966, Validation Loss: 0.65004962682724\n",
      "Epoch: 4/2000, Train Loss: 0.6483249664306641, Validation Loss: 0.6370728611946106\n",
      "Epoch: 5/2000, Train Loss: 0.6352864503860474, Validation Loss: 0.6237024068832397\n",
      "Epoch: 6/2000, Train Loss: 0.6218580603599548, Validation Loss: 0.6096261143684387\n",
      "Epoch: 7/2000, Train Loss: 0.6076511740684509, Validation Loss: 0.5954881310462952\n",
      "Epoch: 8/2000, Train Loss: 0.5933177471160889, Validation Loss: 0.5819429159164429\n",
      "Epoch: 9/2000, Train Loss: 0.5795716047286987, Validation Loss: 0.5686237812042236\n",
      "Epoch: 10/2000, Train Loss: 0.5661202073097229, Validation Loss: 0.5551929473876953\n",
      "Epoch: 11/2000, Train Loss: 0.5526320934295654, Validation Loss: 0.5419567227363586\n",
      "Epoch: 12/2000, Train Loss: 0.539366602897644, Validation Loss: 0.5288493633270264\n",
      "Epoch: 13/2000, Train Loss: 0.5262429714202881, Validation Loss: 0.5161346793174744\n",
      "Epoch: 14/2000, Train Loss: 0.513540506362915, Validation Loss: 0.5039030909538269\n",
      "Epoch: 15/2000, Train Loss: 0.5013689398765564, Validation Loss: 0.49204134941101074\n",
      "Epoch: 16/2000, Train Loss: 0.48959672451019287, Validation Loss: 0.48071545362472534\n",
      "Epoch: 17/2000, Train Loss: 0.4783407747745514, Validation Loss: 0.46972915530204773\n",
      "Epoch: 18/2000, Train Loss: 0.46739110350608826, Validation Loss: 0.459238201379776\n",
      "Epoch: 19/2000, Train Loss: 0.4569160044193268, Validation Loss: 0.44909852743148804\n",
      "Epoch: 20/2000, Train Loss: 0.44679802656173706, Validation Loss: 0.4393605589866638\n",
      "Epoch: 21/2000, Train Loss: 0.43709322810173035, Validation Loss: 0.42996758222579956\n",
      "Epoch: 22/2000, Train Loss: 0.42773866653442383, Validation Loss: 0.4209403097629547\n",
      "Epoch: 23/2000, Train Loss: 0.41873693466186523, Validation Loss: 0.412324458360672\n",
      "Epoch: 24/2000, Train Loss: 0.4101017713546753, Validation Loss: 0.40417617559432983\n",
      "Epoch: 25/2000, Train Loss: 0.4018978476524353, Validation Loss: 0.39629536867141724\n",
      "Epoch: 26/2000, Train Loss: 0.39400726556777954, Validation Loss: 0.38873258233070374\n",
      "Epoch: 27/2000, Train Loss: 0.3864920735359192, Validation Loss: 0.3814414441585541\n",
      "Epoch: 28/2000, Train Loss: 0.37928059697151184, Validation Loss: 0.3744543492794037\n",
      "Epoch: 29/2000, Train Loss: 0.3723953068256378, Validation Loss: 0.36772972345352173\n",
      "Epoch: 30/2000, Train Loss: 0.3657831847667694, Validation Loss: 0.36133110523223877\n",
      "Epoch: 31/2000, Train Loss: 0.35945990681648254, Validation Loss: 0.35523515939712524\n",
      "Epoch: 32/2000, Train Loss: 0.35338935256004333, Validation Loss: 0.34936878085136414\n",
      "Epoch: 33/2000, Train Loss: 0.3475412130355835, Validation Loss: 0.34373176097869873\n",
      "Epoch: 34/2000, Train Loss: 0.34191426634788513, Validation Loss: 0.3382827341556549\n",
      "Epoch: 35/2000, Train Loss: 0.3364522457122803, Validation Loss: 0.3329905569553375\n",
      "Epoch: 36/2000, Train Loss: 0.3311654031276703, Validation Loss: 0.32789087295532227\n",
      "Epoch: 37/2000, Train Loss: 0.3260881006717682, Validation Loss: 0.3230574429035187\n",
      "Epoch: 38/2000, Train Loss: 0.32125088572502136, Validation Loss: 0.3184695839881897\n",
      "Epoch: 39/2000, Train Loss: 0.3166652023792267, Validation Loss: 0.3140755295753479\n",
      "Epoch: 40/2000, Train Loss: 0.31228896975517273, Validation Loss: 0.3098823130130768\n",
      "Epoch: 41/2000, Train Loss: 0.3080812692642212, Validation Loss: 0.3058435916900635\n",
      "Epoch: 42/2000, Train Loss: 0.3040359318256378, Validation Loss: 0.3019489645957947\n",
      "Epoch: 43/2000, Train Loss: 0.30015313625335693, Validation Loss: 0.29824697971343994\n",
      "Epoch: 44/2000, Train Loss: 0.29643481969833374, Validation Loss: 0.29469600319862366\n",
      "Epoch: 45/2000, Train Loss: 0.292880117893219, Validation Loss: 0.29129084944725037\n",
      "Epoch: 46/2000, Train Loss: 0.28946706652641296, Validation Loss: 0.28803518414497375\n",
      "Epoch: 47/2000, Train Loss: 0.2861657738685608, Validation Loss: 0.28486767411231995\n",
      "Epoch: 48/2000, Train Loss: 0.28296613693237305, Validation Loss: 0.28182560205459595\n",
      "Epoch: 49/2000, Train Loss: 0.2798728346824646, Validation Loss: 0.27890273928642273\n",
      "Epoch: 50/2000, Train Loss: 0.27688929438591003, Validation Loss: 0.27606678009033203\n",
      "Epoch: 51/2000, Train Loss: 0.2740122377872467, Validation Loss: 0.2733592689037323\n",
      "Epoch: 52/2000, Train Loss: 0.27123695611953735, Validation Loss: 0.2707277536392212\n",
      "Epoch: 53/2000, Train Loss: 0.2685573399066925, Validation Loss: 0.26819393038749695\n",
      "Epoch: 54/2000, Train Loss: 0.26596516370773315, Validation Loss: 0.2657424509525299\n",
      "Epoch: 55/2000, Train Loss: 0.26345279812812805, Validation Loss: 0.2633494734764099\n",
      "Epoch: 56/2000, Train Loss: 0.2610168755054474, Validation Loss: 0.2610473334789276\n",
      "Epoch: 57/2000, Train Loss: 0.25865602493286133, Validation Loss: 0.258781373500824\n",
      "Epoch: 58/2000, Train Loss: 0.25636792182922363, Validation Loss: 0.25661057233810425\n",
      "Epoch: 59/2000, Train Loss: 0.2541499435901642, Validation Loss: 0.2544705271720886\n",
      "Epoch: 60/2000, Train Loss: 0.2520005404949188, Validation Loss: 0.2524440586566925\n",
      "Epoch: 61/2000, Train Loss: 0.24991825222969055, Validation Loss: 0.2504417598247528\n",
      "Epoch: 62/2000, Train Loss: 0.24790434539318085, Validation Loss: 0.2485932558774948\n",
      "Epoch: 63/2000, Train Loss: 0.24596291780471802, Validation Loss: 0.24673114717006683\n",
      "Epoch: 64/2000, Train Loss: 0.2441059947013855, Validation Loss: 0.245133638381958\n",
      "Epoch: 65/2000, Train Loss: 0.24235375225543976, Validation Loss: 0.2434554249048233\n",
      "Epoch: 66/2000, Train Loss: 0.24075421690940857, Validation Loss: 0.2421821653842926\n",
      "Epoch: 67/2000, Train Loss: 0.23922783136367798, Validation Loss: 0.24043869972229004\n",
      "Epoch: 68/2000, Train Loss: 0.2376372516155243, Validation Loss: 0.23883309960365295\n",
      "Epoch: 69/2000, Train Loss: 0.23579952120780945, Validation Loss: 0.23718054592609406\n",
      "Epoch: 70/2000, Train Loss: 0.23414967954158783, Validation Loss: 0.23587064445018768\n",
      "Epoch: 71/2000, Train Loss: 0.23285908997058868, Validation Loss: 0.23479345440864563\n",
      "Epoch: 72/2000, Train Loss: 0.2315739095211029, Validation Loss: 0.23317576944828033\n",
      "Epoch: 73/2000, Train Loss: 0.23006896674633026, Validation Loss: 0.2318001091480255\n",
      "Epoch: 74/2000, Train Loss: 0.22856895625591278, Validation Loss: 0.23066850006580353\n",
      "Epoch: 75/2000, Train Loss: 0.227344810962677, Validation Loss: 0.2294364720582962\n",
      "Epoch: 76/2000, Train Loss: 0.226191908121109, Validation Loss: 0.22830577194690704\n",
      "Epoch: 77/2000, Train Loss: 0.22485730051994324, Validation Loss: 0.22693529725074768\n",
      "Epoch: 78/2000, Train Loss: 0.22350646555423737, Validation Loss: 0.22580888867378235\n",
      "Epoch: 79/2000, Train Loss: 0.2223404198884964, Validation Loss: 0.22489646077156067\n",
      "Epoch: 80/2000, Train Loss: 0.22125431895256042, Validation Loss: 0.22364424169063568\n",
      "Epoch: 81/2000, Train Loss: 0.22007480263710022, Validation Loss: 0.22256653010845184\n",
      "Epoch: 82/2000, Train Loss: 0.2188381403684616, Validation Loss: 0.22148843109607697\n",
      "Epoch: 83/2000, Train Loss: 0.21770653128623962, Validation Loss: 0.22044311463832855\n",
      "Epoch: 84/2000, Train Loss: 0.21667566895484924, Validation Loss: 0.21957427263259888\n",
      "Epoch: 85/2000, Train Loss: 0.21562090516090393, Validation Loss: 0.2184005230665207\n",
      "Epoch: 86/2000, Train Loss: 0.2145065814256668, Validation Loss: 0.2174202799797058\n",
      "Epoch: 87/2000, Train Loss: 0.2134048491716385, Validation Loss: 0.21646353602409363\n",
      "Epoch: 88/2000, Train Loss: 0.21238155663013458, Validation Loss: 0.2154579907655716\n",
      "Epoch: 89/2000, Train Loss: 0.21140888333320618, Validation Loss: 0.21463601291179657\n",
      "Epoch: 90/2000, Train Loss: 0.2104230672121048, Validation Loss: 0.2135472297668457\n",
      "Epoch: 91/2000, Train Loss: 0.20940878987312317, Validation Loss: 0.21266014873981476\n",
      "Epoch: 92/2000, Train Loss: 0.20839613676071167, Validation Loss: 0.21171213686466217\n",
      "Epoch: 93/2000, Train Loss: 0.2074263095855713, Validation Loss: 0.2107919305562973\n",
      "Epoch: 94/2000, Train Loss: 0.20649956166744232, Validation Loss: 0.2100183069705963\n",
      "Epoch: 95/2000, Train Loss: 0.20559294521808624, Validation Loss: 0.209043949842453\n",
      "Epoch: 96/2000, Train Loss: 0.20468662679195404, Validation Loss: 0.2083013653755188\n",
      "Epoch: 97/2000, Train Loss: 0.2037724256515503, Validation Loss: 0.20732325315475464\n",
      "Epoch: 98/2000, Train Loss: 0.20285819470882416, Validation Loss: 0.20655085146427155\n",
      "Epoch: 99/2000, Train Loss: 0.2019532322883606, Validation Loss: 0.20565935969352722\n",
      "Epoch: 100/2000, Train Loss: 0.20106682181358337, Validation Loss: 0.20485247671604156\n",
      "Epoch: 101/2000, Train Loss: 0.20020054280757904, Validation Loss: 0.20407256484031677\n",
      "Epoch: 102/2000, Train Loss: 0.19935183227062225, Validation Loss: 0.20323024690151215\n",
      "Epoch: 103/2000, Train Loss: 0.19851811230182648, Validation Loss: 0.2025459110736847\n",
      "Epoch: 104/2000, Train Loss: 0.19769960641860962, Validation Loss: 0.2016681581735611\n",
      "Epoch: 105/2000, Train Loss: 0.1969003826379776, Validation Loss: 0.20110464096069336\n",
      "Epoch: 106/2000, Train Loss: 0.19612768292427063, Validation Loss: 0.20020683109760284\n",
      "Epoch: 107/2000, Train Loss: 0.19539864361286163, Validation Loss: 0.19985587894916534\n",
      "Epoch: 108/2000, Train Loss: 0.19471792876720428, Validation Loss: 0.1989375203847885\n",
      "Epoch: 109/2000, Train Loss: 0.19409285485744476, Validation Loss: 0.19871065020561218\n",
      "Epoch: 110/2000, Train Loss: 0.1934141367673874, Validation Loss: 0.1975633203983307\n",
      "Epoch: 111/2000, Train Loss: 0.1926218420267105, Validation Loss: 0.19698092341423035\n",
      "Epoch: 112/2000, Train Loss: 0.19165591895580292, Validation Loss: 0.1959376186132431\n",
      "Epoch: 113/2000, Train Loss: 0.19074782729148865, Validation Loss: 0.19528000056743622\n",
      "Epoch: 114/2000, Train Loss: 0.19004659354686737, Validation Loss: 0.19495917856693268\n",
      "Epoch: 115/2000, Train Loss: 0.18949666619300842, Validation Loss: 0.19414693117141724\n",
      "Epoch: 116/2000, Train Loss: 0.18893221020698547, Validation Loss: 0.19379684329032898\n",
      "Epoch: 117/2000, Train Loss: 0.1882067322731018, Validation Loss: 0.19276879727840424\n",
      "Epoch: 118/2000, Train Loss: 0.18738846480846405, Validation Loss: 0.19217856228351593\n",
      "Epoch: 119/2000, Train Loss: 0.18662229180335999, Validation Loss: 0.19164910912513733\n",
      "Epoch: 120/2000, Train Loss: 0.1860019415616989, Validation Loss: 0.19097958505153656\n",
      "Epoch: 121/2000, Train Loss: 0.1854633241891861, Validation Loss: 0.1907147914171219\n",
      "Epoch: 122/2000, Train Loss: 0.18488359451293945, Validation Loss: 0.18981309235095978\n",
      "Epoch: 123/2000, Train Loss: 0.18421728909015656, Validation Loss: 0.1893579512834549\n",
      "Epoch: 124/2000, Train Loss: 0.18348927795886993, Validation Loss: 0.18859970569610596\n",
      "Epoch: 125/2000, Train Loss: 0.18279853463172913, Validation Loss: 0.18802587687969208\n",
      "Epoch: 126/2000, Train Loss: 0.18218739330768585, Validation Loss: 0.1876543015241623\n",
      "Epoch: 127/2000, Train Loss: 0.18162865936756134, Validation Loss: 0.1869398057460785\n",
      "Epoch: 128/2000, Train Loss: 0.1810731589794159, Validation Loss: 0.18665295839309692\n",
      "Epoch: 129/2000, Train Loss: 0.18047113716602325, Validation Loss: 0.18582534790039062\n",
      "Epoch: 130/2000, Train Loss: 0.17983782291412354, Validation Loss: 0.18542127311229706\n",
      "Epoch: 131/2000, Train Loss: 0.1791824996471405, Validation Loss: 0.1846821904182434\n",
      "Epoch: 132/2000, Train Loss: 0.17854899168014526, Validation Loss: 0.18417872488498688\n",
      "Epoch: 133/2000, Train Loss: 0.1779337376356125, Validation Loss: 0.18362493813037872\n",
      "Epoch: 134/2000, Train Loss: 0.17734071612358093, Validation Loss: 0.18303349614143372\n",
      "Epoch: 135/2000, Train Loss: 0.1767701953649521, Validation Loss: 0.1826404184103012\n",
      "Epoch: 136/2000, Train Loss: 0.1762172132730484, Validation Loss: 0.1819641888141632\n",
      "Epoch: 137/2000, Train Loss: 0.1756802350282669, Validation Loss: 0.18172819912433624\n",
      "Epoch: 138/2000, Train Loss: 0.1751651167869568, Validation Loss: 0.18098321557044983\n",
      "Epoch: 139/2000, Train Loss: 0.17469076812267303, Validation Loss: 0.18098804354667664\n",
      "Epoch: 140/2000, Train Loss: 0.17426025867462158, Validation Loss: 0.18016403913497925\n",
      "Epoch: 141/2000, Train Loss: 0.17387919127941132, Validation Loss: 0.18033412098884583\n",
      "Epoch: 142/2000, Train Loss: 0.17344971001148224, Validation Loss: 0.17925356328487396\n",
      "Epoch: 143/2000, Train Loss: 0.17291100323200226, Validation Loss: 0.17904655635356903\n",
      "Epoch: 144/2000, Train Loss: 0.17215541005134583, Validation Loss: 0.1779801845550537\n",
      "Epoch: 145/2000, Train Loss: 0.1713884174823761, Validation Loss: 0.1775190532207489\n",
      "Epoch: 146/2000, Train Loss: 0.170796737074852, Validation Loss: 0.17731505632400513\n",
      "Epoch: 147/2000, Train Loss: 0.17041033506393433, Validation Loss: 0.17669256031513214\n",
      "Epoch: 148/2000, Train Loss: 0.170094296336174, Validation Loss: 0.17675480246543884\n",
      "Epoch: 149/2000, Train Loss: 0.1696736067533493, Validation Loss: 0.17578484117984772\n",
      "Epoch: 150/2000, Train Loss: 0.1691095530986786, Validation Loss: 0.17550873756408691\n",
      "Epoch: 151/2000, Train Loss: 0.16844826936721802, Validation Loss: 0.17479756474494934\n",
      "Epoch: 152/2000, Train Loss: 0.16785715520381927, Validation Loss: 0.17435519397258759\n",
      "Epoch: 153/2000, Train Loss: 0.16740281879901886, Validation Loss: 0.17424717545509338\n",
      "Epoch: 154/2000, Train Loss: 0.16703318059444427, Validation Loss: 0.17358285188674927\n",
      "Epoch: 155/2000, Train Loss: 0.16665445268154144, Validation Loss: 0.17353719472885132\n",
      "Epoch: 156/2000, Train Loss: 0.16618530452251434, Validation Loss: 0.1726950705051422\n",
      "Epoch: 157/2000, Train Loss: 0.16564439237117767, Validation Loss: 0.17241699993610382\n",
      "Epoch: 158/2000, Train Loss: 0.1650823950767517, Validation Loss: 0.17184701561927795\n",
      "Epoch: 159/2000, Train Loss: 0.16457496583461761, Validation Loss: 0.17140695452690125\n",
      "Epoch: 160/2000, Train Loss: 0.16413895785808563, Validation Loss: 0.17122779786586761\n",
      "Epoch: 161/2000, Train Loss: 0.1637464016675949, Validation Loss: 0.17060701549053192\n",
      "Epoch: 162/2000, Train Loss: 0.16336044669151306, Validation Loss: 0.1705528199672699\n",
      "Epoch: 163/2000, Train Loss: 0.16294610500335693, Validation Loss: 0.16979233920574188\n",
      "Epoch: 164/2000, Train Loss: 0.16250135004520416, Validation Loss: 0.16966313123703003\n",
      "Epoch: 165/2000, Train Loss: 0.16201724112033844, Validation Loss: 0.1689419001340866\n",
      "Epoch: 166/2000, Train Loss: 0.16152425110340118, Validation Loss: 0.16867923736572266\n",
      "Epoch: 167/2000, Train Loss: 0.1610412895679474, Validation Loss: 0.1681738793849945\n",
      "Epoch: 168/2000, Train Loss: 0.1605863869190216, Validation Loss: 0.16777993738651276\n",
      "Epoch: 169/2000, Train Loss: 0.16016075015068054, Validation Loss: 0.1675005555152893\n",
      "Epoch: 170/2000, Train Loss: 0.15975765883922577, Validation Loss: 0.16697728633880615\n",
      "Epoch: 171/2000, Train Loss: 0.15937110781669617, Validation Loss: 0.1668701022863388\n",
      "Epoch: 172/2000, Train Loss: 0.1589965969324112, Validation Loss: 0.16623957455158234\n",
      "Epoch: 173/2000, Train Loss: 0.1586385816335678, Validation Loss: 0.16629303991794586\n",
      "Epoch: 174/2000, Train Loss: 0.15828794240951538, Validation Loss: 0.1655658632516861\n",
      "Epoch: 175/2000, Train Loss: 0.15795522928237915, Validation Loss: 0.16573812067508698\n",
      "Epoch: 176/2000, Train Loss: 0.15760108828544617, Validation Loss: 0.16489045321941376\n",
      "Epoch: 177/2000, Train Loss: 0.15723305940628052, Validation Loss: 0.16500075161457062\n",
      "Epoch: 178/2000, Train Loss: 0.15678420662879944, Validation Loss: 0.1640893965959549\n",
      "Epoch: 179/2000, Train Loss: 0.15629805624485016, Validation Loss: 0.16398340463638306\n",
      "Epoch: 180/2000, Train Loss: 0.1557808220386505, Validation Loss: 0.16332295536994934\n",
      "Epoch: 181/2000, Train Loss: 0.15530633926391602, Validation Loss: 0.16303013265132904\n",
      "Epoch: 182/2000, Train Loss: 0.15489396452903748, Validation Loss: 0.16278386116027832\n",
      "Epoch: 183/2000, Train Loss: 0.1545388549566269, Validation Loss: 0.16229717433452606\n",
      "Epoch: 184/2000, Train Loss: 0.1542198210954666, Validation Loss: 0.16233216226100922\n",
      "Epoch: 185/2000, Train Loss: 0.15391433238983154, Validation Loss: 0.161666139960289\n",
      "Epoch: 186/2000, Train Loss: 0.1536141186952591, Validation Loss: 0.16181418299674988\n",
      "Epoch: 187/2000, Train Loss: 0.1532832235097885, Validation Loss: 0.16101504862308502\n",
      "Epoch: 188/2000, Train Loss: 0.15292444825172424, Validation Loss: 0.1610826551914215\n",
      "Epoch: 189/2000, Train Loss: 0.15250541269779205, Validation Loss: 0.1602824330329895\n",
      "Epoch: 190/2000, Train Loss: 0.1520635336637497, Validation Loss: 0.1601683497428894\n",
      "Epoch: 191/2000, Train Loss: 0.15161554515361786, Validation Loss: 0.15960583090782166\n",
      "Epoch: 192/2000, Train Loss: 0.1511971801519394, Validation Loss: 0.15932336449623108\n",
      "Epoch: 193/2000, Train Loss: 0.15081700682640076, Validation Loss: 0.15908005833625793\n",
      "Epoch: 194/2000, Train Loss: 0.15047107636928558, Validation Loss: 0.15864123404026031\n",
      "Epoch: 195/2000, Train Loss: 0.15015047788619995, Validation Loss: 0.1586378663778305\n",
      "Epoch: 196/2000, Train Loss: 0.14984354376792908, Validation Loss: 0.15804994106292725\n",
      "Epoch: 197/2000, Train Loss: 0.1495518535375595, Validation Loss: 0.15822403132915497\n",
      "Epoch: 198/2000, Train Loss: 0.14926204085350037, Validation Loss: 0.15752199292182922\n",
      "Epoch: 199/2000, Train Loss: 0.14900465309619904, Validation Loss: 0.1578451544046402\n",
      "Epoch: 200/2000, Train Loss: 0.14873497188091278, Validation Loss: 0.15697604417800903\n",
      "Epoch: 201/2000, Train Loss: 0.14845512807369232, Validation Loss: 0.15725719928741455\n",
      "Epoch: 202/2000, Train Loss: 0.1480812430381775, Validation Loss: 0.1562623828649521\n",
      "Epoch: 203/2000, Train Loss: 0.14764507114887238, Validation Loss: 0.15626904368400574\n",
      "Epoch: 204/2000, Train Loss: 0.14713986217975616, Validation Loss: 0.15552935004234314\n",
      "Epoch: 205/2000, Train Loss: 0.1466677486896515, Validation Loss: 0.1552952080965042\n",
      "Epoch: 206/2000, Train Loss: 0.14627717435359955, Validation Loss: 0.15511997044086456\n",
      "Epoch: 207/2000, Train Loss: 0.14597325026988983, Validation Loss: 0.15466482937335968\n",
      "Epoch: 208/2000, Train Loss: 0.14572840929031372, Validation Loss: 0.15486730635166168\n",
      "Epoch: 209/2000, Train Loss: 0.1454988420009613, Validation Loss: 0.15417422354221344\n",
      "Epoch: 210/2000, Train Loss: 0.14526484906673431, Validation Loss: 0.15447533130645752\n",
      "Epoch: 211/2000, Train Loss: 0.14497730135917664, Validation Loss: 0.15360267460346222\n",
      "Epoch: 212/2000, Train Loss: 0.14464031159877777, Validation Loss: 0.1537470668554306\n",
      "Epoch: 213/2000, Train Loss: 0.14422014355659485, Validation Loss: 0.15293067693710327\n",
      "Epoch: 214/2000, Train Loss: 0.14378002285957336, Validation Loss: 0.1528228521347046\n",
      "Epoch: 215/2000, Train Loss: 0.14336074888706207, Validation Loss: 0.15240950882434845\n",
      "Epoch: 216/2000, Train Loss: 0.14300285279750824, Validation Loss: 0.15207824110984802\n",
      "Epoch: 217/2000, Train Loss: 0.14270560443401337, Validation Loss: 0.15207920968532562\n",
      "Epoch: 218/2000, Train Loss: 0.14244697988033295, Validation Loss: 0.1515318602323532\n",
      "Epoch: 219/2000, Train Loss: 0.14221036434173584, Validation Loss: 0.15176527202129364\n",
      "Epoch: 220/2000, Train Loss: 0.14196550846099854, Validation Loss: 0.1510394960641861\n",
      "Epoch: 221/2000, Train Loss: 0.1417289525270462, Validation Loss: 0.15139776468276978\n",
      "Epoch: 222/2000, Train Loss: 0.14146782457828522, Validation Loss: 0.1505393236875534\n",
      "Epoch: 223/2000, Train Loss: 0.14118392765522003, Validation Loss: 0.1508190929889679\n",
      "Epoch: 224/2000, Train Loss: 0.14082437753677368, Validation Loss: 0.14990228414535522\n",
      "Epoch: 225/2000, Train Loss: 0.140421524643898, Validation Loss: 0.14993245899677277\n",
      "Epoch: 226/2000, Train Loss: 0.13998757302761078, Validation Loss: 0.14927956461906433\n",
      "Epoch: 227/2000, Train Loss: 0.1395871937274933, Validation Loss: 0.14907535910606384\n",
      "Epoch: 228/2000, Train Loss: 0.13924361765384674, Validation Loss: 0.14887376129627228\n",
      "Epoch: 229/2000, Train Loss: 0.13895587623119354, Validation Loss: 0.1484500616788864\n",
      "Epoch: 230/2000, Train Loss: 0.13870522379875183, Validation Loss: 0.14858968555927277\n",
      "Epoch: 231/2000, Train Loss: 0.13847246766090393, Validation Loss: 0.1479775756597519\n",
      "Epoch: 232/2000, Train Loss: 0.1382543295621872, Validation Loss: 0.1483246088027954\n",
      "Epoch: 233/2000, Train Loss: 0.13802789151668549, Validation Loss: 0.14755657315254211\n",
      "Epoch: 234/2000, Train Loss: 0.1377963274717331, Validation Loss: 0.1479407101869583\n",
      "Epoch: 235/2000, Train Loss: 0.1375105232000351, Validation Loss: 0.14705093204975128\n",
      "Epoch: 236/2000, Train Loss: 0.13718724250793457, Validation Loss: 0.1472683548927307\n",
      "Epoch: 237/2000, Train Loss: 0.13679981231689453, Validation Loss: 0.14644722640514374\n",
      "Epoch: 238/2000, Train Loss: 0.13640327751636505, Validation Loss: 0.14642545580863953\n",
      "Epoch: 239/2000, Train Loss: 0.13601695001125336, Validation Loss: 0.14592528343200684\n",
      "Epoch: 240/2000, Train Loss: 0.13567160069942474, Validation Loss: 0.14568930864334106\n",
      "Epoch: 241/2000, Train Loss: 0.13536898791790009, Validation Loss: 0.14554926753044128\n",
      "Epoch: 242/2000, Train Loss: 0.1351015716791153, Validation Loss: 0.14512120187282562\n",
      "Epoch: 243/2000, Train Loss: 0.13486112654209137, Validation Loss: 0.14525891840457916\n",
      "Epoch: 244/2000, Train Loss: 0.13463962078094482, Validation Loss: 0.14466075599193573\n",
      "Epoch: 245/2000, Train Loss: 0.13444092869758606, Validation Loss: 0.14503712952136993\n",
      "Epoch: 246/2000, Train Loss: 0.13425347208976746, Validation Loss: 0.1442776471376419\n",
      "Epoch: 247/2000, Train Loss: 0.13408684730529785, Validation Loss: 0.14482218027114868\n",
      "Epoch: 248/2000, Train Loss: 0.13388749957084656, Validation Loss: 0.14386999607086182\n",
      "Epoch: 249/2000, Train Loss: 0.13365235924720764, Validation Loss: 0.14429537951946259\n",
      "Epoch: 250/2000, Train Loss: 0.1332920789718628, Validation Loss: 0.14325390756130219\n",
      "Epoch: 251/2000, Train Loss: 0.13286232948303223, Validation Loss: 0.1433270126581192\n",
      "Epoch: 252/2000, Train Loss: 0.13239581882953644, Validation Loss: 0.14269836246967316\n",
      "Epoch: 253/2000, Train Loss: 0.13199813663959503, Validation Loss: 0.14249871671199799\n",
      "Epoch: 254/2000, Train Loss: 0.131703719496727, Validation Loss: 0.14249730110168457\n",
      "Epoch: 255/2000, Train Loss: 0.13149578869342804, Validation Loss: 0.14202716946601868\n",
      "Epoch: 256/2000, Train Loss: 0.1313323974609375, Validation Loss: 0.14236126840114594\n",
      "Epoch: 257/2000, Train Loss: 0.13116411864757538, Validation Loss: 0.14162246882915497\n",
      "Epoch: 258/2000, Train Loss: 0.13096708059310913, Validation Loss: 0.14197632670402527\n",
      "Epoch: 259/2000, Train Loss: 0.13069124519824982, Validation Loss: 0.14111341536045074\n",
      "Epoch: 260/2000, Train Loss: 0.13035766780376434, Validation Loss: 0.1412569284439087\n",
      "Epoch: 261/2000, Train Loss: 0.12997165322303772, Validation Loss: 0.14060242474079132\n",
      "Epoch: 262/2000, Train Loss: 0.12959840893745422, Validation Loss: 0.1405046135187149\n",
      "Epoch: 263/2000, Train Loss: 0.12926867604255676, Validation Loss: 0.14027713239192963\n",
      "Epoch: 264/2000, Train Loss: 0.1289953738451004, Validation Loss: 0.1399514079093933\n",
      "Epoch: 265/2000, Train Loss: 0.1287679672241211, Validation Loss: 0.14006654918193817\n",
      "Epoch: 266/2000, Train Loss: 0.12856756150722504, Validation Loss: 0.13953319191932678\n",
      "Epoch: 267/2000, Train Loss: 0.12838825583457947, Validation Loss: 0.13987880945205688\n",
      "Epoch: 268/2000, Train Loss: 0.1282106190919876, Validation Loss: 0.13915550708770752\n",
      "Epoch: 269/2000, Train Loss: 0.12803451716899872, Validation Loss: 0.1396292895078659\n",
      "Epoch: 270/2000, Train Loss: 0.12781263887882233, Validation Loss: 0.1387515813112259\n",
      "Epoch: 271/2000, Train Loss: 0.12756577134132385, Validation Loss: 0.13912375271320343\n",
      "Epoch: 272/2000, Train Loss: 0.1272394359111786, Validation Loss: 0.13821591436862946\n",
      "Epoch: 273/2000, Train Loss: 0.12687990069389343, Validation Loss: 0.13832204043865204\n",
      "Epoch: 274/2000, Train Loss: 0.1265006810426712, Validation Loss: 0.13770176470279694\n",
      "Epoch: 275/2000, Train Loss: 0.12615333497524261, Validation Loss: 0.13755938410758972\n",
      "Epoch: 276/2000, Train Loss: 0.125858873128891, Validation Loss: 0.13739775121212006\n",
      "Epoch: 277/2000, Train Loss: 0.1256173998117447, Validation Loss: 0.13703830540180206\n",
      "Epoch: 278/2000, Train Loss: 0.1254129856824875, Validation Loss: 0.1372205913066864\n",
      "Epoch: 279/2000, Train Loss: 0.12522560358047485, Validation Loss: 0.13666068017482758\n",
      "Epoch: 280/2000, Train Loss: 0.12504631280899048, Validation Loss: 0.13701137900352478\n",
      "Epoch: 281/2000, Train Loss: 0.12485411763191223, Validation Loss: 0.13628320395946503\n",
      "Epoch: 282/2000, Train Loss: 0.12464939802885056, Validation Loss: 0.13664962351322174\n",
      "Epoch: 283/2000, Train Loss: 0.12440527975559235, Validation Loss: 0.13584010303020477\n",
      "Epoch: 284/2000, Train Loss: 0.12413575500249863, Validation Loss: 0.13609333336353302\n",
      "Epoch: 285/2000, Train Loss: 0.12382771819829941, Validation Loss: 0.13537126779556274\n",
      "Epoch: 286/2000, Train Loss: 0.12351330369710922, Validation Loss: 0.13544054329395294\n",
      "Epoch: 287/2000, Train Loss: 0.12320317327976227, Validation Loss: 0.13496628403663635\n",
      "Epoch: 288/2000, Train Loss: 0.12291639298200607, Validation Loss: 0.1348385214805603\n",
      "Epoch: 289/2000, Train Loss: 0.12265542149543762, Validation Loss: 0.1346418857574463\n",
      "Epoch: 290/2000, Train Loss: 0.12241756170988083, Validation Loss: 0.13433809578418732\n",
      "Epoch: 291/2000, Train Loss: 0.12219735980033875, Validation Loss: 0.13437391817569733\n",
      "Epoch: 292/2000, Train Loss: 0.12198977172374725, Validation Loss: 0.13392488658428192\n",
      "Epoch: 293/2000, Train Loss: 0.12179394066333771, Validation Loss: 0.13415732979774475\n",
      "Epoch: 294/2000, Train Loss: 0.12160835415124893, Validation Loss: 0.1335678994655609\n",
      "Epoch: 295/2000, Train Loss: 0.12144078314304352, Validation Loss: 0.13399767875671387\n",
      "Epoch: 296/2000, Train Loss: 0.12128543853759766, Validation Loss: 0.13325342535972595\n",
      "Epoch: 297/2000, Train Loss: 0.12115528434515, Validation Loss: 0.13387517631053925\n",
      "Epoch: 298/2000, Train Loss: 0.12101603299379349, Validation Loss: 0.13295331597328186\n",
      "Epoch: 299/2000, Train Loss: 0.12087385356426239, Validation Loss: 0.1335994303226471\n",
      "Epoch: 300/2000, Train Loss: 0.12064433097839355, Validation Loss: 0.1325192153453827\n",
      "Epoch: 301/2000, Train Loss: 0.12034915387630463, Validation Loss: 0.13288478553295135\n",
      "Epoch: 302/2000, Train Loss: 0.11995236575603485, Validation Loss: 0.13197600841522217\n",
      "Epoch: 303/2000, Train Loss: 0.11955089867115021, Validation Loss: 0.13199271261692047\n",
      "Epoch: 304/2000, Train Loss: 0.11920171231031418, Validation Loss: 0.13169872760772705\n",
      "Epoch: 305/2000, Train Loss: 0.11894863098859787, Validation Loss: 0.13141268491744995\n",
      "Epoch: 306/2000, Train Loss: 0.11877898871898651, Validation Loss: 0.1316622942686081\n",
      "Epoch: 307/2000, Train Loss: 0.11865244060754776, Validation Loss: 0.13107243180274963\n",
      "Epoch: 308/2000, Train Loss: 0.11853215843439102, Validation Loss: 0.13153260946273804\n",
      "Epoch: 309/2000, Train Loss: 0.11837416887283325, Validation Loss: 0.13072320818901062\n",
      "Epoch: 310/2000, Train Loss: 0.11817371845245361, Validation Loss: 0.13110294938087463\n",
      "Epoch: 311/2000, Train Loss: 0.11790559440851212, Validation Loss: 0.13029426336288452\n",
      "Epoch: 312/2000, Train Loss: 0.11760685592889786, Validation Loss: 0.13044323027133942\n",
      "Epoch: 313/2000, Train Loss: 0.11729704588651657, Validation Loss: 0.1299329698085785\n",
      "Epoch: 314/2000, Train Loss: 0.11701560020446777, Validation Loss: 0.12983748316764832\n",
      "Epoch: 315/2000, Train Loss: 0.1167755126953125, Validation Loss: 0.12972773611545563\n",
      "Epoch: 316/2000, Train Loss: 0.11657451093196869, Validation Loss: 0.1293998658657074\n",
      "Epoch: 317/2000, Train Loss: 0.11640014499425888, Validation Loss: 0.12958058714866638\n",
      "Epoch: 318/2000, Train Loss: 0.11623793840408325, Validation Loss: 0.12905506789684296\n",
      "Epoch: 319/2000, Train Loss: 0.1160803809762001, Validation Loss: 0.12939970195293427\n",
      "Epoch: 320/2000, Train Loss: 0.11591532826423645, Validation Loss: 0.1287330538034439\n",
      "Epoch: 321/2000, Train Loss: 0.1157459169626236, Validation Loss: 0.12914548814296722\n",
      "Epoch: 322/2000, Train Loss: 0.1155562624335289, Validation Loss: 0.12840037047863007\n",
      "Epoch: 323/2000, Train Loss: 0.11535628885030746, Validation Loss: 0.12878698110580444\n",
      "Epoch: 324/2000, Train Loss: 0.1151295080780983, Validation Loss: 0.12804502248764038\n",
      "Epoch: 325/2000, Train Loss: 0.1148924008011818, Validation Loss: 0.12832704186439514\n",
      "Epoch: 326/2000, Train Loss: 0.11463871598243713, Validation Loss: 0.127690851688385\n",
      "Epoch: 327/2000, Train Loss: 0.11438655853271484, Validation Loss: 0.12783178687095642\n",
      "Epoch: 328/2000, Train Loss: 0.11413773149251938, Validation Loss: 0.12737196683883667\n",
      "Epoch: 329/2000, Train Loss: 0.1139010414481163, Validation Loss: 0.12736986577510834\n",
      "Epoch: 330/2000, Train Loss: 0.11367607861757278, Validation Loss: 0.1270858347415924\n",
      "Epoch: 331/2000, Train Loss: 0.11346213519573212, Validation Loss: 0.1269567310810089\n",
      "Epoch: 332/2000, Train Loss: 0.1132565587759018, Validation Loss: 0.12681370973587036\n",
      "Epoch: 333/2000, Train Loss: 0.11305717378854752, Validation Loss: 0.1265801191329956\n",
      "Epoch: 334/2000, Train Loss: 0.11286274343729019, Validation Loss: 0.12655910849571228\n",
      "Epoch: 335/2000, Train Loss: 0.11267343163490295, Validation Loss: 0.12622977793216705\n",
      "Epoch: 336/2000, Train Loss: 0.11249145120382309, Validation Loss: 0.1263548880815506\n",
      "Epoch: 337/2000, Train Loss: 0.11232122033834457, Validation Loss: 0.12591058015823364\n",
      "Epoch: 338/2000, Train Loss: 0.11217367649078369, Validation Loss: 0.1262894868850708\n",
      "Epoch: 339/2000, Train Loss: 0.1120639368891716, Validation Loss: 0.12569686770439148\n",
      "Epoch: 340/2000, Train Loss: 0.11202883720397949, Validation Loss: 0.1265878975391388\n",
      "Epoch: 341/2000, Train Loss: 0.11209512501955032, Validation Loss: 0.12582744657993317\n",
      "Epoch: 342/2000, Train Loss: 0.11232815682888031, Validation Loss: 0.1273833066225052\n",
      "Epoch: 343/2000, Train Loss: 0.11257617920637131, Validation Loss: 0.12610428035259247\n",
      "Epoch: 344/2000, Train Loss: 0.11273133009672165, Validation Loss: 0.12704221904277802\n",
      "Epoch: 345/2000, Train Loss: 0.11216988414525986, Validation Loss: 0.12496766448020935\n",
      "Epoch: 346/2000, Train Loss: 0.11122119426727295, Validation Loss: 0.12481807917356491\n",
      "Epoch: 347/2000, Train Loss: 0.11041159927845001, Validation Loss: 0.12487971782684326\n",
      "Epoch: 348/2000, Train Loss: 0.11032283306121826, Validation Loss: 0.12459473311901093\n",
      "Epoch: 349/2000, Train Loss: 0.11067551374435425, Validation Loss: 0.12574519217014313\n",
      "Epoch: 350/2000, Train Loss: 0.11075016111135483, Validation Loss: 0.12430412322282791\n",
      "Epoch: 351/2000, Train Loss: 0.11031613498926163, Validation Loss: 0.12433771789073944\n",
      "Epoch: 352/2000, Train Loss: 0.10962827503681183, Validation Loss: 0.1238776296377182\n",
      "Epoch: 353/2000, Train Loss: 0.10931062698364258, Validation Loss: 0.12363207340240479\n",
      "Epoch: 354/2000, Train Loss: 0.10941454768180847, Validation Loss: 0.12447760254144669\n",
      "Epoch: 355/2000, Train Loss: 0.10948552191257477, Validation Loss: 0.1234498918056488\n",
      "Epoch: 356/2000, Train Loss: 0.1092289462685585, Validation Loss: 0.12361056357622147\n",
      "Epoch: 357/2000, Train Loss: 0.10874386876821518, Validation Loss: 0.12315215915441513\n",
      "Epoch: 358/2000, Train Loss: 0.10843859612941742, Validation Loss: 0.1229308545589447\n",
      "Epoch: 359/2000, Train Loss: 0.1084119975566864, Validation Loss: 0.12352316081523895\n",
      "Epoch: 360/2000, Train Loss: 0.10841519385576248, Validation Loss: 0.12273617833852768\n",
      "Epoch: 361/2000, Train Loss: 0.10823027044534683, Validation Loss: 0.12293864041566849\n",
      "Epoch: 362/2000, Train Loss: 0.10787831991910934, Validation Loss: 0.12247786670923233\n",
      "Epoch: 363/2000, Train Loss: 0.10759681463241577, Validation Loss: 0.1222928836941719\n",
      "Epoch: 364/2000, Train Loss: 0.1074863150715828, Validation Loss: 0.12266592681407928\n",
      "Epoch: 365/2000, Train Loss: 0.10743647068738937, Validation Loss: 0.12206096947193146\n",
      "Epoch: 366/2000, Train Loss: 0.10729998350143433, Validation Loss: 0.12229493260383606\n",
      "Epoch: 367/2000, Train Loss: 0.1070438027381897, Validation Loss: 0.12179654091596603\n",
      "Epoch: 368/2000, Train Loss: 0.10678381472826004, Validation Loss: 0.12168396264314651\n",
      "Epoch: 369/2000, Train Loss: 0.1066112145781517, Validation Loss: 0.12183394283056259\n",
      "Epoch: 370/2000, Train Loss: 0.10651111602783203, Validation Loss: 0.12140973657369614\n",
      "Epoch: 371/2000, Train Loss: 0.1064007431268692, Validation Loss: 0.12166918814182281\n",
      "Epoch: 372/2000, Train Loss: 0.1062217578291893, Validation Loss: 0.12117470800876617\n",
      "Epoch: 373/2000, Train Loss: 0.10600101947784424, Validation Loss: 0.12117302417755127\n",
      "Epoch: 374/2000, Train Loss: 0.1057952418923378, Validation Loss: 0.12107095867395401\n",
      "Epoch: 375/2000, Train Loss: 0.10563889145851135, Validation Loss: 0.12080586701631546\n",
      "Epoch: 376/2000, Train Loss: 0.10551479458808899, Validation Loss: 0.12098657339811325\n",
      "Epoch: 377/2000, Train Loss: 0.10538167506456375, Validation Loss: 0.12055467814207077\n",
      "Epoch: 378/2000, Train Loss: 0.10521748661994934, Validation Loss: 0.12066731601953506\n",
      "Epoch: 379/2000, Train Loss: 0.10502766817808151, Validation Loss: 0.12035694718360901\n",
      "Epoch: 380/2000, Train Loss: 0.10483978688716888, Validation Loss: 0.12027901411056519\n",
      "Epoch: 381/2000, Train Loss: 0.10467254370450974, Validation Loss: 0.12025082111358643\n",
      "Epoch: 382/2000, Train Loss: 0.10452646762132645, Validation Loss: 0.11998794972896576\n",
      "Epoch: 383/2000, Train Loss: 0.10438786447048187, Validation Loss: 0.12010374665260315\n",
      "Epoch: 384/2000, Train Loss: 0.10424142330884933, Validation Loss: 0.11974799633026123\n",
      "Epoch: 385/2000, Train Loss: 0.10408161580562592, Validation Loss: 0.1198258176445961\n",
      "Epoch: 386/2000, Train Loss: 0.103910893201828, Validation Loss: 0.11953752487897873\n",
      "Epoch: 387/2000, Train Loss: 0.1037386804819107, Validation Loss: 0.11949808895587921\n",
      "Epoch: 388/2000, Train Loss: 0.10357227921485901, Validation Loss: 0.1193724051117897\n",
      "Epoch: 389/2000, Train Loss: 0.10341477394104004, Validation Loss: 0.11920727789402008\n",
      "Epoch: 390/2000, Train Loss: 0.10326441377401352, Validation Loss: 0.11921658366918564\n",
      "Epoch: 391/2000, Train Loss: 0.10311707854270935, Validation Loss: 0.11895694583654404\n",
      "Epoch: 392/2000, Train Loss: 0.10296909511089325, Validation Loss: 0.11902416497468948\n",
      "Epoch: 393/2000, Train Loss: 0.10281788557767868, Validation Loss: 0.11872579157352448\n",
      "Epoch: 394/2000, Train Loss: 0.10266343504190445, Validation Loss: 0.11879140883684158\n",
      "Epoch: 395/2000, Train Loss: 0.10250594466924667, Validation Loss: 0.11850582808256149\n",
      "Epoch: 396/2000, Train Loss: 0.10234715789556503, Validation Loss: 0.11853981018066406\n",
      "Epoch: 397/2000, Train Loss: 0.10218796133995056, Validation Loss: 0.11829382926225662\n",
      "Epoch: 398/2000, Train Loss: 0.10202950984239578, Validation Loss: 0.11828776448965073\n",
      "Epoch: 399/2000, Train Loss: 0.10187200456857681, Validation Loss: 0.11808466911315918\n",
      "Epoch: 400/2000, Train Loss: 0.10171566158533096, Validation Loss: 0.11804423481225967\n",
      "Epoch: 401/2000, Train Loss: 0.10156028717756271, Validation Loss: 0.11787506192922592\n",
      "Epoch: 402/2000, Train Loss: 0.10140577703714371, Validation Loss: 0.11781162023544312\n",
      "Epoch: 403/2000, Train Loss: 0.10125192254781723, Validation Loss: 0.11766207963228226\n",
      "Epoch: 404/2000, Train Loss: 0.10109858959913254, Validation Loss: 0.11758619546890259\n",
      "Epoch: 405/2000, Train Loss: 0.10094562917947769, Validation Loss: 0.11744184046983719\n",
      "Epoch: 406/2000, Train Loss: 0.10079304873943329, Validation Loss: 0.11736543476581573\n",
      "Epoch: 407/2000, Train Loss: 0.10064084827899933, Validation Loss: 0.11721514910459518\n",
      "Epoch: 408/2000, Train Loss: 0.10048910230398178, Validation Loss: 0.11715488135814667\n",
      "Epoch: 409/2000, Train Loss: 0.10033798962831497, Validation Loss: 0.11698448657989502\n",
      "Epoch: 410/2000, Train Loss: 0.1001879870891571, Validation Loss: 0.11696558445692062\n",
      "Epoch: 411/2000, Train Loss: 0.10004009306430817, Validation Loss: 0.11674609780311584\n",
      "Epoch: 412/2000, Train Loss: 0.09989674389362335, Validation Loss: 0.11682584136724472\n",
      "Epoch: 413/2000, Train Loss: 0.09976330399513245, Validation Loss: 0.11650633066892624\n",
      "Epoch: 414/2000, Train Loss: 0.0996529832482338, Validation Loss: 0.11686182022094727\n",
      "Epoch: 415/2000, Train Loss: 0.09959553182125092, Validation Loss: 0.11640449613332748\n",
      "Epoch: 416/2000, Train Loss: 0.09966208785772324, Validation Loss: 0.11760615557432175\n",
      "Epoch: 417/2000, Train Loss: 0.09998133033514023, Validation Loss: 0.11720843613147736\n",
      "Epoch: 418/2000, Train Loss: 0.1008007749915123, Validation Loss: 0.12015610933303833\n",
      "Epoch: 419/2000, Train Loss: 0.10198656469583511, Validation Loss: 0.11918237805366516\n",
      "Epoch: 420/2000, Train Loss: 0.10315065830945969, Validation Loss: 0.11997057497501373\n",
      "Epoch: 421/2000, Train Loss: 0.10173633694648743, Validation Loss: 0.11602271348237991\n",
      "Epoch: 422/2000, Train Loss: 0.09926923364400864, Validation Loss: 0.11558756977319717\n",
      "Epoch: 423/2000, Train Loss: 0.09844400733709335, Validation Loss: 0.11797742545604706\n",
      "Epoch: 424/2000, Train Loss: 0.09981893002986908, Validation Loss: 0.11708106100559235\n",
      "Epoch: 425/2000, Train Loss: 0.10044357925653458, Validation Loss: 0.1168285459280014\n",
      "Epoch: 426/2000, Train Loss: 0.09874068200588226, Validation Loss: 0.11549754440784454\n",
      "Epoch: 427/2000, Train Loss: 0.09781063348054886, Validation Loss: 0.11573605239391327\n",
      "Epoch: 428/2000, Train Loss: 0.09870423376560211, Validation Loss: 0.1172960102558136\n",
      "Epoch: 429/2000, Train Loss: 0.09896917641162872, Validation Loss: 0.11513218283653259\n",
      "Epoch: 430/2000, Train Loss: 0.09789783507585526, Validation Loss: 0.11486209183931351\n",
      "Epoch: 431/2000, Train Loss: 0.09730010479688644, Validation Loss: 0.11609300225973129\n",
      "Epoch: 432/2000, Train Loss: 0.0978926420211792, Validation Loss: 0.11516429483890533\n",
      "Epoch: 433/2000, Train Loss: 0.09799528121948242, Validation Loss: 0.11515264213085175\n",
      "Epoch: 434/2000, Train Loss: 0.09710473567247391, Validation Loss: 0.11483936011791229\n",
      "Epoch: 435/2000, Train Loss: 0.09684926271438599, Validation Loss: 0.11467524617910385\n",
      "Epoch: 436/2000, Train Loss: 0.09727029502391815, Validation Loss: 0.11537472903728485\n",
      "Epoch: 437/2000, Train Loss: 0.09704805165529251, Validation Loss: 0.11425057053565979\n",
      "Epoch: 438/2000, Train Loss: 0.09644018858671188, Validation Loss: 0.1141732856631279\n",
      "Epoch: 439/2000, Train Loss: 0.0963984802365303, Validation Loss: 0.11501598358154297\n",
      "Epoch: 440/2000, Train Loss: 0.09659389406442642, Validation Loss: 0.11407562345266342\n",
      "Epoch: 441/2000, Train Loss: 0.09631363302469254, Validation Loss: 0.11406107991933823\n",
      "Epoch: 442/2000, Train Loss: 0.09589719772338867, Validation Loss: 0.1142830029129982\n",
      "Epoch: 443/2000, Train Loss: 0.09591351449489594, Validation Loss: 0.11382639408111572\n",
      "Epoch: 444/2000, Train Loss: 0.09598060697317123, Validation Loss: 0.11411740630865097\n",
      "Epoch: 445/2000, Train Loss: 0.0956815704703331, Validation Loss: 0.1136290654540062\n",
      "Epoch: 446/2000, Train Loss: 0.09539896249771118, Validation Loss: 0.11346378177404404\n",
      "Epoch: 447/2000, Train Loss: 0.09540407359600067, Validation Loss: 0.11394651234149933\n",
      "Epoch: 448/2000, Train Loss: 0.09537280350923538, Validation Loss: 0.11329828947782516\n",
      "Epoch: 449/2000, Train Loss: 0.0951220914721489, Validation Loss: 0.11328940838575363\n",
      "Epoch: 450/2000, Train Loss: 0.09491218626499176, Validation Loss: 0.11350381374359131\n",
      "Epoch: 451/2000, Train Loss: 0.09488575160503387, Validation Loss: 0.11308211088180542\n",
      "Epoch: 452/2000, Train Loss: 0.09482038766145706, Validation Loss: 0.11326048523187637\n",
      "Epoch: 453/2000, Train Loss: 0.09460530430078506, Validation Loss: 0.11299657076597214\n",
      "Epoch: 454/2000, Train Loss: 0.09442903846502304, Validation Loss: 0.11280826479196548\n",
      "Epoch: 455/2000, Train Loss: 0.09437208622694016, Validation Loss: 0.11307915300130844\n",
      "Epoch: 456/2000, Train Loss: 0.09428845345973969, Validation Loss: 0.11263378709554672\n",
      "Epoch: 457/2000, Train Loss: 0.09411141276359558, Validation Loss: 0.11262283474206924\n",
      "Epoch: 458/2000, Train Loss: 0.0939486175775528, Validation Loss: 0.11268554627895355\n",
      "Epoch: 459/2000, Train Loss: 0.0938652828335762, Validation Loss: 0.11238165944814682\n",
      "Epoch: 460/2000, Train Loss: 0.09377989917993546, Validation Loss: 0.11253128945827484\n",
      "Epoch: 461/2000, Train Loss: 0.09362995624542236, Validation Loss: 0.11227972060441971\n",
      "Epoch: 462/2000, Train Loss: 0.09347427636384964, Validation Loss: 0.11216457188129425\n",
      "Epoch: 463/2000, Train Loss: 0.09336915612220764, Validation Loss: 0.11229762434959412\n",
      "Epoch: 464/2000, Train Loss: 0.0932793989777565, Validation Loss: 0.11198404431343079\n",
      "Epoch: 465/2000, Train Loss: 0.09315360337495804, Validation Loss: 0.11202506721019745\n",
      "Epoch: 466/2000, Train Loss: 0.09300701320171356, Validation Loss: 0.11192303895950317\n",
      "Epoch: 467/2000, Train Loss: 0.09288541227579117, Validation Loss: 0.11174788326025009\n",
      "Epoch: 468/2000, Train Loss: 0.0927874743938446, Validation Loss: 0.11185155808925629\n",
      "Epoch: 469/2000, Train Loss: 0.09267684817314148, Validation Loss: 0.1115928366780281\n",
      "Epoch: 470/2000, Train Loss: 0.09254424273967743, Validation Loss: 0.1115708127617836\n",
      "Epoch: 471/2000, Train Loss: 0.09241433441638947, Validation Loss: 0.11152998358011246\n",
      "Epoch: 472/2000, Train Loss: 0.09230374544858932, Validation Loss: 0.11134159564971924\n",
      "Epoch: 473/2000, Train Loss: 0.09219890832901001, Validation Loss: 0.11140758544206619\n",
      "Epoch: 474/2000, Train Loss: 0.09208127856254578, Validation Loss: 0.11120517551898956\n",
      "Epoch: 475/2000, Train Loss: 0.0919540673494339, Validation Loss: 0.11116054654121399\n",
      "Epoch: 476/2000, Train Loss: 0.0918327271938324, Validation Loss: 0.11113076657056808\n",
      "Epoch: 477/2000, Train Loss: 0.09172230213880539, Validation Loss: 0.11095714569091797\n",
      "Epoch: 478/2000, Train Loss: 0.09161311388015747, Validation Loss: 0.11099174618721008\n",
      "Epoch: 479/2000, Train Loss: 0.09149599075317383, Validation Loss: 0.11081676930189133\n",
      "Epoch: 480/2000, Train Loss: 0.09137380123138428, Validation Loss: 0.11076734215021133\n",
      "Epoch: 481/2000, Train Loss: 0.09125494956970215, Validation Loss: 0.11072281748056412\n",
      "Epoch: 482/2000, Train Loss: 0.09114246070384979, Validation Loss: 0.11057810485363007\n",
      "Epoch: 483/2000, Train Loss: 0.09103167057037354, Validation Loss: 0.1105964183807373\n",
      "Epoch: 484/2000, Train Loss: 0.09091713279485703, Validation Loss: 0.11044005304574966\n",
      "Epoch: 485/2000, Train Loss: 0.09079921245574951, Validation Loss: 0.1104019433259964\n",
      "Epoch: 486/2000, Train Loss: 0.09068194776773453, Validation Loss: 0.11032653599977493\n",
      "Epoch: 487/2000, Train Loss: 0.0905681625008583, Validation Loss: 0.11020613461732864\n",
      "Epoch: 488/2000, Train Loss: 0.0904565155506134, Validation Loss: 0.11019489169120789\n",
      "Epoch: 489/2000, Train Loss: 0.09034392237663269, Validation Loss: 0.1100444570183754\n",
      "Epoch: 490/2000, Train Loss: 0.09022898226976395, Validation Loss: 0.11001867800951004\n",
      "Epoch: 491/2000, Train Loss: 0.09011294692754745, Validation Loss: 0.1099126935005188\n",
      "Epoch: 492/2000, Train Loss: 0.08999805152416229, Validation Loss: 0.10983072221279144\n",
      "Epoch: 493/2000, Train Loss: 0.08988530188798904, Validation Loss: 0.10978799313306808\n",
      "Epoch: 494/2000, Train Loss: 0.08977390080690384, Validation Loss: 0.1096622571349144\n",
      "Epoch: 495/2000, Train Loss: 0.08966227620840073, Validation Loss: 0.10964007675647736\n",
      "Epoch: 496/2000, Train Loss: 0.08954957872629166, Validation Loss: 0.10951638966798782\n",
      "Epoch: 497/2000, Train Loss: 0.0894361212849617, Validation Loss: 0.10947033762931824\n",
      "Epoch: 498/2000, Train Loss: 0.08932284265756607, Validation Loss: 0.10938671976327896\n",
      "Epoch: 499/2000, Train Loss: 0.08921057730913162, Validation Loss: 0.10930157452821732\n",
      "Epoch: 500/2000, Train Loss: 0.08909939974546432, Validation Loss: 0.10925637185573578\n",
      "Epoch: 501/2000, Train Loss: 0.0889887809753418, Validation Loss: 0.10914386808872223\n",
      "Epoch: 502/2000, Train Loss: 0.08887803554534912, Validation Loss: 0.10910981148481369\n",
      "Epoch: 503/2000, Train Loss: 0.08876686543226242, Validation Loss: 0.10899657011032104\n",
      "Epoch: 504/2000, Train Loss: 0.08865542709827423, Validation Loss: 0.10894972085952759\n",
      "Epoch: 505/2000, Train Loss: 0.0885440781712532, Validation Loss: 0.10885853320360184\n",
      "Epoch: 506/2000, Train Loss: 0.0884331539273262, Validation Loss: 0.10878905653953552\n",
      "Epoch: 507/2000, Train Loss: 0.08832275867462158, Validation Loss: 0.10872513055801392\n",
      "Epoch: 508/2000, Train Loss: 0.08821278810501099, Validation Loss: 0.10863479226827621\n",
      "Epoch: 509/2000, Train Loss: 0.0881030261516571, Validation Loss: 0.1085880696773529\n",
      "Epoch: 510/2000, Train Loss: 0.08799334615468979, Validation Loss: 0.108486108481884\n",
      "Epoch: 511/2000, Train Loss: 0.08788366615772247, Validation Loss: 0.10844320803880692\n",
      "Epoch: 512/2000, Train Loss: 0.08777400106191635, Validation Loss: 0.10834141075611115\n",
      "Epoch: 513/2000, Train Loss: 0.08766444772481918, Validation Loss: 0.10829335451126099\n",
      "Epoch: 514/2000, Train Loss: 0.08755502849817276, Validation Loss: 0.10820065438747406\n",
      "Epoch: 515/2000, Train Loss: 0.08744578063488007, Validation Loss: 0.10814333707094193\n",
      "Epoch: 516/2000, Train Loss: 0.0873367190361023, Validation Loss: 0.10806278884410858\n",
      "Epoch: 517/2000, Train Loss: 0.08722785115242004, Validation Loss: 0.10799518972635269\n",
      "Epoch: 518/2000, Train Loss: 0.0871191993355751, Validation Loss: 0.10792551189661026\n",
      "Epoch: 519/2000, Train Loss: 0.08701078593730927, Validation Loss: 0.10784843564033508\n",
      "Epoch: 520/2000, Train Loss: 0.08690260350704193, Validation Loss: 0.10778700560331345\n",
      "Epoch: 521/2000, Train Loss: 0.08679458498954773, Validation Loss: 0.10770200937986374\n",
      "Epoch: 522/2000, Train Loss: 0.08668671548366547, Validation Loss: 0.10764680802822113\n",
      "Epoch: 523/2000, Train Loss: 0.08657901734113693, Validation Loss: 0.10755505412817001\n",
      "Epoch: 524/2000, Train Loss: 0.08647150546312332, Validation Loss: 0.10750576108694077\n",
      "Epoch: 525/2000, Train Loss: 0.08636418730020523, Validation Loss: 0.10740762948989868\n",
      "Epoch: 526/2000, Train Loss: 0.08625710010528564, Validation Loss: 0.10736645758152008\n",
      "Epoch: 527/2000, Train Loss: 0.08615022897720337, Validation Loss: 0.1072605550289154\n",
      "Epoch: 528/2000, Train Loss: 0.08604361116886139, Validation Loss: 0.10723207890987396\n",
      "Epoch: 529/2000, Train Loss: 0.08593733608722687, Validation Loss: 0.10711316019296646\n",
      "Epoch: 530/2000, Train Loss: 0.08583150058984756, Validation Loss: 0.10710475593805313\n",
      "Epoch: 531/2000, Train Loss: 0.08572633564472198, Validation Loss: 0.10696201771497726\n",
      "Epoch: 532/2000, Train Loss: 0.08562218397855759, Validation Loss: 0.10698851943016052\n",
      "Epoch: 533/2000, Train Loss: 0.08551964908838272, Validation Loss: 0.10680457949638367\n",
      "Epoch: 534/2000, Train Loss: 0.08541996031999588, Validation Loss: 0.10689909011125565\n",
      "Epoch: 535/2000, Train Loss: 0.08532515168190002, Validation Loss: 0.1066468209028244\n",
      "Epoch: 536/2000, Train Loss: 0.08523932099342346, Validation Loss: 0.10688020288944244\n",
      "Epoch: 537/2000, Train Loss: 0.08516931533813477, Validation Loss: 0.10652001947164536\n",
      "Epoch: 538/2000, Train Loss: 0.08512920141220093, Validation Loss: 0.10704466700553894\n",
      "Epoch: 539/2000, Train Loss: 0.08513915538787842, Validation Loss: 0.10654207319021225\n",
      "Epoch: 540/2000, Train Loss: 0.08524009585380554, Validation Loss: 0.10763926804065704\n",
      "Epoch: 541/2000, Train Loss: 0.0854557603597641, Validation Loss: 0.10697022080421448\n",
      "Epoch: 542/2000, Train Loss: 0.08584195375442505, Validation Loss: 0.10870260000228882\n",
      "Epoch: 543/2000, Train Loss: 0.08620548993349075, Validation Loss: 0.10747700929641724\n",
      "Epoch: 544/2000, Train Loss: 0.08646021038293839, Validation Loss: 0.10854146629571915\n",
      "Epoch: 545/2000, Train Loss: 0.08597781509160995, Validation Loss: 0.10644105076789856\n",
      "Epoch: 546/2000, Train Loss: 0.0850951224565506, Validation Loss: 0.10635833442211151\n",
      "Epoch: 547/2000, Train Loss: 0.08422556519508362, Validation Loss: 0.10615723580121994\n",
      "Epoch: 548/2000, Train Loss: 0.08403890579938889, Validation Loss: 0.10606516152620316\n",
      "Epoch: 549/2000, Train Loss: 0.08442679792642593, Validation Loss: 0.10741084814071655\n",
      "Epoch: 550/2000, Train Loss: 0.08476033806800842, Validation Loss: 0.10623308271169662\n",
      "Epoch: 551/2000, Train Loss: 0.08465705066919327, Validation Loss: 0.1065768301486969\n",
      "Epoch: 552/2000, Train Loss: 0.08405845612287521, Validation Loss: 0.10555744916200638\n",
      "Epoch: 553/2000, Train Loss: 0.08354468643665314, Validation Loss: 0.10547828674316406\n",
      "Epoch: 554/2000, Train Loss: 0.08346248418092728, Validation Loss: 0.10621210932731628\n",
      "Epoch: 555/2000, Train Loss: 0.0836755782365799, Validation Loss: 0.1055944636464119\n",
      "Epoch: 556/2000, Train Loss: 0.083785779774189, Validation Loss: 0.1061779111623764\n",
      "Epoch: 557/2000, Train Loss: 0.08353319764137268, Validation Loss: 0.10526644438505173\n",
      "Epoch: 558/2000, Train Loss: 0.08314595371484756, Validation Loss: 0.10527820885181427\n",
      "Epoch: 559/2000, Train Loss: 0.08292172104120255, Validation Loss: 0.10555677860975266\n",
      "Epoch: 560/2000, Train Loss: 0.08295010775327682, Validation Loss: 0.10517634451389313\n",
      "Epoch: 561/2000, Train Loss: 0.08303628861904144, Validation Loss: 0.10575070232152939\n",
      "Epoch: 562/2000, Train Loss: 0.08295156061649323, Validation Loss: 0.10499802231788635\n",
      "Epoch: 563/2000, Train Loss: 0.08271251618862152, Validation Loss: 0.10510428249835968\n",
      "Epoch: 564/2000, Train Loss: 0.08247213810682297, Validation Loss: 0.10502564907073975\n",
      "Epoch: 565/2000, Train Loss: 0.08237221837043762, Validation Loss: 0.10480216890573502\n",
      "Epoch: 566/2000, Train Loss: 0.0823797881603241, Validation Loss: 0.10525087267160416\n",
      "Epoch: 567/2000, Train Loss: 0.08235900849103928, Validation Loss: 0.10468665510416031\n",
      "Epoch: 568/2000, Train Loss: 0.08223796635866165, Validation Loss: 0.1048956885933876\n",
      "Epoch: 569/2000, Train Loss: 0.08204761147499084, Validation Loss: 0.10459668189287186\n",
      "Epoch: 570/2000, Train Loss: 0.0818915143609047, Validation Loss: 0.10450895875692368\n",
      "Epoch: 571/2000, Train Loss: 0.08181554824113846, Validation Loss: 0.10475435107946396\n",
      "Epoch: 572/2000, Train Loss: 0.08178143948316574, Validation Loss: 0.104390449821949\n",
      "Epoch: 573/2000, Train Loss: 0.08172248303890228, Validation Loss: 0.10466740280389786\n",
      "Epoch: 574/2000, Train Loss: 0.08160510659217834, Validation Loss: 0.10428217053413391\n",
      "Epoch: 575/2000, Train Loss: 0.08146033436059952, Validation Loss: 0.1043190285563469\n",
      "Epoch: 576/2000, Train Loss: 0.08133330196142197, Validation Loss: 0.10428481549024582\n",
      "Epoch: 577/2000, Train Loss: 0.08124695718288422, Validation Loss: 0.1040881797671318\n",
      "Epoch: 578/2000, Train Loss: 0.08118528127670288, Validation Loss: 0.10429754108190536\n",
      "Epoch: 579/2000, Train Loss: 0.08111528307199478, Validation Loss: 0.103961281478405\n",
      "Epoch: 580/2000, Train Loss: 0.08101949095726013, Validation Loss: 0.10411594063043594\n",
      "Epoch: 581/2000, Train Loss: 0.08090230077505112, Validation Loss: 0.10388532280921936\n",
      "Epoch: 582/2000, Train Loss: 0.08078577369451523, Validation Loss: 0.10387048125267029\n",
      "Epoch: 583/2000, Train Loss: 0.08068566769361496, Validation Loss: 0.10388513654470444\n",
      "Epoch: 584/2000, Train Loss: 0.0806032121181488, Validation Loss: 0.10370385646820068\n",
      "Epoch: 585/2000, Train Loss: 0.08052744716405869, Validation Loss: 0.10384847223758698\n",
      "Epoch: 586/2000, Train Loss: 0.0804453119635582, Validation Loss: 0.10358451306819916\n",
      "Epoch: 587/2000, Train Loss: 0.08035160601139069, Validation Loss: 0.10369394719600677\n",
      "Epoch: 588/2000, Train Loss: 0.0802486315369606, Validation Loss: 0.1034930944442749\n",
      "Epoch: 589/2000, Train Loss: 0.08014466613531113, Validation Loss: 0.10349224507808685\n",
      "Epoch: 590/2000, Train Loss: 0.08004626631736755, Validation Loss: 0.10344033688306808\n",
      "Epoch: 591/2000, Train Loss: 0.07995570451021194, Validation Loss: 0.1033250093460083\n",
      "Epoch: 592/2000, Train Loss: 0.07987066358327866, Validation Loss: 0.10338924080133438\n",
      "Epoch: 593/2000, Train Loss: 0.0797867402434349, Validation Loss: 0.10319830477237701\n",
      "Epoch: 594/2000, Train Loss: 0.07970038056373596, Validation Loss: 0.10329452902078629\n",
      "Epoch: 595/2000, Train Loss: 0.07960980385541916, Validation Loss: 0.10309307277202606\n",
      "Epoch: 596/2000, Train Loss: 0.07951581478118896, Validation Loss: 0.10315576195716858\n",
      "Epoch: 597/2000, Train Loss: 0.07941997051239014, Validation Loss: 0.10300210863351822\n",
      "Epoch: 598/2000, Train Loss: 0.07932446897029877, Validation Loss: 0.10300183296203613\n",
      "Epoch: 599/2000, Train Loss: 0.0792306438088417, Validation Loss: 0.1029212549328804\n",
      "Epoch: 600/2000, Train Loss: 0.07913900911808014, Validation Loss: 0.10285598039627075\n",
      "Epoch: 601/2000, Train Loss: 0.07904932647943497, Validation Loss: 0.10284214466810226\n",
      "Epoch: 602/2000, Train Loss: 0.07896099239587784, Validation Loss: 0.10272490978240967\n",
      "Epoch: 603/2000, Train Loss: 0.07887335121631622, Validation Loss: 0.10275761783123016\n",
      "Epoch: 604/2000, Train Loss: 0.07878583669662476, Validation Loss: 0.10260520875453949\n",
      "Epoch: 605/2000, Train Loss: 0.07869821786880493, Validation Loss: 0.10266527533531189\n",
      "Epoch: 606/2000, Train Loss: 0.07861033082008362, Validation Loss: 0.1024903804063797\n",
      "Epoch: 607/2000, Train Loss: 0.07852233946323395, Validation Loss: 0.10256757587194443\n",
      "Epoch: 608/2000, Train Loss: 0.07843436300754547, Validation Loss: 0.10237572342157364\n",
      "Epoch: 609/2000, Train Loss: 0.07834675163030624, Validation Loss: 0.10247086733579636\n",
      "Epoch: 610/2000, Train Loss: 0.0782596692442894, Validation Loss: 0.10225945711135864\n",
      "Epoch: 611/2000, Train Loss: 0.07817371189594269, Validation Loss: 0.1023838222026825\n",
      "Epoch: 612/2000, Train Loss: 0.07808926701545715, Validation Loss: 0.10214189440011978\n",
      "Epoch: 613/2000, Train Loss: 0.07800745218992233, Validation Loss: 0.10231923311948776\n",
      "Epoch: 614/2000, Train Loss: 0.07792928069829941, Validation Loss: 0.10202699154615402\n",
      "Epoch: 615/2000, Train Loss: 0.07785724103450775, Validation Loss: 0.10229937732219696\n",
      "Epoch: 616/2000, Train Loss: 0.07779378443956375, Validation Loss: 0.10192796587944031\n",
      "Epoch: 617/2000, Train Loss: 0.0777444913983345, Validation Loss: 0.10236338526010513\n",
      "Epoch: 618/2000, Train Loss: 0.07771339267492294, Validation Loss: 0.10187835246324539\n",
      "Epoch: 619/2000, Train Loss: 0.07771173119544983, Validation Loss: 0.10256505012512207\n",
      "Epoch: 620/2000, Train Loss: 0.0777377337217331, Validation Loss: 0.10192929953336716\n",
      "Epoch: 621/2000, Train Loss: 0.07780804485082626, Validation Loss: 0.10290282964706421\n",
      "Epoch: 622/2000, Train Loss: 0.07788094878196716, Validation Loss: 0.10205380618572235\n",
      "Epoch: 623/2000, Train Loss: 0.07797011733055115, Validation Loss: 0.10311182588338852\n",
      "Epoch: 624/2000, Train Loss: 0.07794319093227386, Validation Loss: 0.1019739881157875\n",
      "Epoch: 625/2000, Train Loss: 0.07783163338899612, Validation Loss: 0.10267718136310577\n",
      "Epoch: 626/2000, Train Loss: 0.07751653343439102, Validation Loss: 0.10153447836637497\n",
      "Epoch: 627/2000, Train Loss: 0.07714513689279556, Validation Loss: 0.10176687687635422\n",
      "Epoch: 628/2000, Train Loss: 0.0767921581864357, Validation Loss: 0.10136502981185913\n",
      "Epoch: 629/2000, Train Loss: 0.07658391445875168, Validation Loss: 0.10128482431173325\n",
      "Epoch: 630/2000, Train Loss: 0.0765359103679657, Validation Loss: 0.1017264798283577\n",
      "Epoch: 631/2000, Train Loss: 0.07658891379833221, Validation Loss: 0.10127667337656021\n",
      "Epoch: 632/2000, Train Loss: 0.07666190713644028, Validation Loss: 0.10199175029993057\n",
      "Epoch: 633/2000, Train Loss: 0.0766645148396492, Validation Loss: 0.10120713710784912\n",
      "Epoch: 634/2000, Train Loss: 0.0765833631157875, Validation Loss: 0.10171572118997574\n",
      "Epoch: 635/2000, Train Loss: 0.0763910636305809, Validation Loss: 0.10096967965364456\n",
      "Epoch: 636/2000, Train Loss: 0.07616526633501053, Validation Loss: 0.10115475952625275\n",
      "Epoch: 637/2000, Train Loss: 0.07595893740653992, Validation Loss: 0.10092968493700027\n",
      "Epoch: 638/2000, Train Loss: 0.07582671195268631, Validation Loss: 0.10083749145269394\n",
      "Epoch: 639/2000, Train Loss: 0.07577113807201385, Validation Loss: 0.101141057908535\n",
      "Epoch: 640/2000, Train Loss: 0.07575896382331848, Validation Loss: 0.10076586902141571\n",
      "Epoch: 641/2000, Train Loss: 0.07574938982725143, Validation Loss: 0.10124403983354568\n",
      "Epoch: 642/2000, Train Loss: 0.07570337504148483, Validation Loss: 0.10067879408597946\n",
      "Epoch: 643/2000, Train Loss: 0.07561734318733215, Validation Loss: 0.10105292499065399\n",
      "Epoch: 644/2000, Train Loss: 0.07548695057630539, Validation Loss: 0.10054158419370651\n",
      "Epoch: 645/2000, Train Loss: 0.07534241676330566, Validation Loss: 0.10071256011724472\n",
      "Epoch: 646/2000, Train Loss: 0.07520310580730438, Validation Loss: 0.10048796981573105\n",
      "Epoch: 647/2000, Train Loss: 0.07508954405784607, Validation Loss: 0.10045018047094345\n",
      "Epoch: 648/2000, Train Loss: 0.07500586658716202, Validation Loss: 0.10054188966751099\n",
      "Epoch: 649/2000, Train Loss: 0.07494483143091202, Validation Loss: 0.10031277686357498\n",
      "Epoch: 650/2000, Train Loss: 0.07489375025033951, Validation Loss: 0.10058626532554626\n",
      "Epoch: 651/2000, Train Loss: 0.07483917474746704, Validation Loss: 0.10022206604480743\n",
      "Epoch: 652/2000, Train Loss: 0.07477497309446335, Validation Loss: 0.10053122043609619\n",
      "Epoch: 653/2000, Train Loss: 0.0746949091553688, Validation Loss: 0.10012856125831604\n",
      "Epoch: 654/2000, Train Loss: 0.07460379600524902, Validation Loss: 0.10037734359502792\n",
      "Epoch: 655/2000, Train Loss: 0.07450259476900101, Validation Loss: 0.10003849118947983\n",
      "Epoch: 656/2000, Train Loss: 0.07439956068992615, Validation Loss: 0.10018163174390793\n",
      "Epoch: 657/2000, Train Loss: 0.07429817318916321, Validation Loss: 0.09997357428073883\n",
      "Epoch: 658/2000, Train Loss: 0.07420278340578079, Validation Loss: 0.10000207275152206\n",
      "Epoch: 659/2000, Train Loss: 0.07411429286003113, Validation Loss: 0.09993711113929749\n",
      "Epoch: 660/2000, Train Loss: 0.07403229176998138, Validation Loss: 0.09986092895269394\n",
      "Epoch: 661/2000, Train Loss: 0.07395534962415695, Validation Loss: 0.09991349279880524\n",
      "Epoch: 662/2000, Train Loss: 0.07388179749250412, Validation Loss: 0.09974928945302963\n",
      "Epoch: 663/2000, Train Loss: 0.07381026446819305, Validation Loss: 0.09988445788621902\n",
      "Epoch: 664/2000, Train Loss: 0.07373946905136108, Validation Loss: 0.09964995831251144\n",
      "Epoch: 665/2000, Train Loss: 0.0736691877245903, Validation Loss: 0.09984253346920013\n",
      "Epoch: 666/2000, Train Loss: 0.07359858602285385, Validation Loss: 0.09955460578203201\n",
      "Epoch: 667/2000, Train Loss: 0.07352878898382187, Validation Loss: 0.09979556500911713\n",
      "Epoch: 668/2000, Train Loss: 0.07345890998840332, Validation Loss: 0.09946474432945251\n",
      "Epoch: 669/2000, Train Loss: 0.07339110225439072, Validation Loss: 0.09975767135620117\n",
      "Epoch: 670/2000, Train Loss: 0.07332395762205124, Validation Loss: 0.09938329458236694\n",
      "Epoch: 671/2000, Train Loss: 0.07326095551252365, Validation Loss: 0.09974014759063721\n",
      "Epoch: 672/2000, Train Loss: 0.0731998160481453, Validation Loss: 0.09931131452322006\n",
      "Epoch: 673/2000, Train Loss: 0.07314594835042953, Validation Loss: 0.09975171834230423\n",
      "Epoch: 674/2000, Train Loss: 0.07309523224830627, Validation Loss: 0.09925254434347153\n",
      "Epoch: 675/2000, Train Loss: 0.07305625826120377, Validation Loss: 0.09980268776416779\n",
      "Epoch: 676/2000, Train Loss: 0.07302066683769226, Validation Loss: 0.09921631962060928\n",
      "Epoch: 677/2000, Train Loss: 0.07300175726413727, Validation Loss: 0.09989646077156067\n",
      "Epoch: 678/2000, Train Loss: 0.07298097759485245, Validation Loss: 0.09920578449964523\n",
      "Epoch: 679/2000, Train Loss: 0.07297707349061966, Validation Loss: 0.09999654442071915\n",
      "Epoch: 680/2000, Train Loss: 0.07295171171426773, Validation Loss: 0.09918948262929916\n",
      "Epoch: 681/2000, Train Loss: 0.07292955368757248, Validation Loss: 0.09999220818281174\n",
      "Epoch: 682/2000, Train Loss: 0.07285135984420776, Validation Loss: 0.09909462183713913\n",
      "Epoch: 683/2000, Train Loss: 0.07275288552045822, Validation Loss: 0.0997529923915863\n",
      "Epoch: 684/2000, Train Loss: 0.07258478552103043, Validation Loss: 0.09890168160200119\n",
      "Epoch: 685/2000, Train Loss: 0.07239978015422821, Validation Loss: 0.09931188821792603\n",
      "Epoch: 686/2000, Train Loss: 0.07219386100769043, Validation Loss: 0.0987429991364479\n",
      "Epoch: 687/2000, Train Loss: 0.07201287895441055, Validation Loss: 0.0988953560590744\n",
      "Epoch: 688/2000, Train Loss: 0.0718693658709526, Validation Loss: 0.09874607622623444\n",
      "Epoch: 689/2000, Train Loss: 0.07177156955003738, Validation Loss: 0.09865351766347885\n",
      "Epoch: 690/2000, Train Loss: 0.07171221822500229, Validation Loss: 0.09886220842599869\n",
      "Epoch: 691/2000, Train Loss: 0.07167770713567734, Validation Loss: 0.09854734688997269\n",
      "Epoch: 692/2000, Train Loss: 0.07165498286485672, Validation Loss: 0.09897145628929138\n",
      "Epoch: 693/2000, Train Loss: 0.07162905484437943, Validation Loss: 0.09848769009113312\n",
      "Epoch: 694/2000, Train Loss: 0.07159697264432907, Validation Loss: 0.0989958643913269\n",
      "Epoch: 695/2000, Train Loss: 0.07154355198144913, Validation Loss: 0.09841997176408768\n",
      "Epoch: 696/2000, Train Loss: 0.07147733122110367, Validation Loss: 0.09890501201152802\n",
      "Epoch: 697/2000, Train Loss: 0.07138553261756897, Validation Loss: 0.09833014756441116\n",
      "Epoch: 698/2000, Train Loss: 0.07128413021564484, Validation Loss: 0.09871652722358704\n",
      "Epoch: 699/2000, Train Loss: 0.07116834074258804, Validation Loss: 0.09823911637067795\n",
      "Epoch: 700/2000, Train Loss: 0.07105347514152527, Validation Loss: 0.09848985075950623\n",
      "Epoch: 701/2000, Train Loss: 0.07094060629606247, Validation Loss: 0.09817595034837723\n",
      "Epoch: 702/2000, Train Loss: 0.07083722949028015, Validation Loss: 0.09828326106071472\n",
      "Epoch: 703/2000, Train Loss: 0.07074379920959473, Validation Loss: 0.09814614802598953\n",
      "Epoch: 704/2000, Train Loss: 0.0706605613231659, Validation Loss: 0.0981195941567421\n",
      "Epoch: 705/2000, Train Loss: 0.07058579474687576, Validation Loss: 0.09813471138477325\n",
      "Epoch: 706/2000, Train Loss: 0.0705174133181572, Validation Loss: 0.0979938805103302\n",
      "Epoch: 707/2000, Train Loss: 0.0704534724354744, Validation Loss: 0.09812813997268677\n",
      "Epoch: 708/2000, Train Loss: 0.07039236277341843, Validation Loss: 0.09789475053548813\n",
      "Epoch: 709/2000, Train Loss: 0.07033353298902512, Validation Loss: 0.09812403470277786\n",
      "Epoch: 710/2000, Train Loss: 0.07027588784694672, Validation Loss: 0.09781339019536972\n",
      "Epoch: 711/2000, Train Loss: 0.07022083550691605, Validation Loss: 0.09812748432159424\n",
      "Epoch: 712/2000, Train Loss: 0.07016696780920029, Validation Loss: 0.09774427860975266\n",
      "Epoch: 713/2000, Train Loss: 0.0701180100440979, Validation Loss: 0.09814754873514175\n",
      "Epoch: 714/2000, Train Loss: 0.07007118314504623, Validation Loss: 0.09768687188625336\n",
      "Epoch: 715/2000, Train Loss: 0.07003337889909744, Validation Loss: 0.09819714725017548\n",
      "Epoch: 716/2000, Train Loss: 0.06999856233596802, Validation Loss: 0.09764821082353592\n",
      "Epoch: 717/2000, Train Loss: 0.06997822970151901, Validation Loss: 0.09828846901655197\n",
      "Epoch: 718/2000, Train Loss: 0.06995885819196701, Validation Loss: 0.09763757139444351\n",
      "Epoch: 719/2000, Train Loss: 0.06995825469493866, Validation Loss: 0.09841260313987732\n",
      "Epoch: 720/2000, Train Loss: 0.06994732469320297, Validation Loss: 0.0976458340883255\n",
      "Epoch: 721/2000, Train Loss: 0.069951131939888, Validation Loss: 0.09850253909826279\n",
      "Epoch: 722/2000, Train Loss: 0.06991659849882126, Validation Loss: 0.09761878103017807\n",
      "Epoch: 723/2000, Train Loss: 0.06987743079662323, Validation Loss: 0.09842251986265182\n",
      "Epoch: 724/2000, Train Loss: 0.06976698338985443, Validation Loss: 0.09748519957065582\n",
      "Epoch: 725/2000, Train Loss: 0.06963346898555756, Validation Loss: 0.09808267652988434\n",
      "Epoch: 726/2000, Train Loss: 0.06943942606449127, Validation Loss: 0.09728547930717468\n",
      "Epoch: 727/2000, Train Loss: 0.06924355030059814, Validation Loss: 0.09761248528957367\n",
      "Epoch: 728/2000, Train Loss: 0.0690535232424736, Validation Loss: 0.09718894213438034\n",
      "Epoch: 729/2000, Train Loss: 0.06890365481376648, Validation Loss: 0.09725382924079895\n",
      "Epoch: 730/2000, Train Loss: 0.06880008429288864, Validation Loss: 0.09726497530937195\n",
      "Epoch: 731/2000, Train Loss: 0.06873898208141327, Validation Loss: 0.0970822125673294\n",
      "Epoch: 732/2000, Train Loss: 0.0687074139714241, Validation Loss: 0.09741217643022537\n",
      "Epoch: 733/2000, Train Loss: 0.06868943572044373, Validation Loss: 0.09701583534479141\n",
      "Epoch: 734/2000, Train Loss: 0.06867467612028122, Validation Loss: 0.0975121259689331\n",
      "Epoch: 735/2000, Train Loss: 0.06864737719297409, Validation Loss: 0.09696809202432632\n",
      "Epoch: 736/2000, Train Loss: 0.06860988587141037, Validation Loss: 0.09750201553106308\n",
      "Epoch: 737/2000, Train Loss: 0.06854618340730667, Validation Loss: 0.09689843654632568\n",
      "Epoch: 738/2000, Train Loss: 0.06846950203180313, Validation Loss: 0.09737059473991394\n",
      "Epoch: 739/2000, Train Loss: 0.06836899369955063, Validation Loss: 0.09681079536676407\n",
      "Epoch: 740/2000, Train Loss: 0.06826256215572357, Validation Loss: 0.09715932607650757\n",
      "Epoch: 741/2000, Train Loss: 0.06814832240343094, Validation Loss: 0.09673760831356049\n",
      "Epoch: 742/2000, Train Loss: 0.0680396631360054, Validation Loss: 0.0969376415014267\n",
      "Epoch: 743/2000, Train Loss: 0.06793814897537231, Validation Loss: 0.09670176357030869\n",
      "Epoch: 744/2000, Train Loss: 0.06784795224666595, Validation Loss: 0.09675296396017075\n",
      "Epoch: 745/2000, Train Loss: 0.06776850670576096, Validation Loss: 0.09669613838195801\n",
      "Epoch: 746/2000, Train Loss: 0.06769838184118271, Validation Loss: 0.096613310277462\n",
      "Epoch: 747/2000, Train Loss: 0.06763530522584915, Validation Loss: 0.09670119732618332\n",
      "Epoch: 748/2000, Train Loss: 0.06757703423500061, Validation Loss: 0.09650762379169464\n",
      "Epoch: 749/2000, Train Loss: 0.06752195954322815, Validation Loss: 0.09670591354370117\n",
      "Epoch: 750/2000, Train Loss: 0.06746846437454224, Validation Loss: 0.09642433375120163\n",
      "Epoch: 751/2000, Train Loss: 0.06741676479578018, Validation Loss: 0.09671039879322052\n",
      "Epoch: 752/2000, Train Loss: 0.06736533343791962, Validation Loss: 0.09635554254055023\n",
      "Epoch: 753/2000, Train Loss: 0.06731658428907394, Validation Loss: 0.09672027081251144\n",
      "Epoch: 754/2000, Train Loss: 0.06726811081171036, Validation Loss: 0.09629635512828827\n",
      "Epoch: 755/2000, Train Loss: 0.06722482293844223, Validation Loss: 0.09674359858036041\n",
      "Epoch: 756/2000, Train Loss: 0.0671822726726532, Validation Loss: 0.09624657034873962\n",
      "Epoch: 757/2000, Train Loss: 0.06714858114719391, Validation Loss: 0.09679022431373596\n",
      "Epoch: 758/2000, Train Loss: 0.06711532920598984, Validation Loss: 0.09621134400367737\n",
      "Epoch: 759/2000, Train Loss: 0.06709502637386322, Validation Loss: 0.09686535596847534\n",
      "Epoch: 760/2000, Train Loss: 0.0670715793967247, Validation Loss: 0.09619442373514175\n",
      "Epoch: 761/2000, Train Loss: 0.06706321239471436, Validation Loss: 0.09695298224687576\n",
      "Epoch: 762/2000, Train Loss: 0.06704063713550568, Validation Loss: 0.09618350863456726\n",
      "Epoch: 763/2000, Train Loss: 0.06702882796525955, Validation Loss: 0.09699531644582748\n",
      "Epoch: 764/2000, Train Loss: 0.06698198616504669, Validation Loss: 0.0961390882730484\n",
      "Epoch: 765/2000, Train Loss: 0.0669327899813652, Validation Loss: 0.0968996211886406\n",
      "Epoch: 766/2000, Train Loss: 0.0668293684720993, Validation Loss: 0.09602143615484238\n",
      "Epoch: 767/2000, Train Loss: 0.06671323627233505, Validation Loss: 0.09661620110273361\n",
      "Epoch: 768/2000, Train Loss: 0.06655300408601761, Validation Loss: 0.09586221724748611\n",
      "Epoch: 769/2000, Train Loss: 0.0663924515247345, Validation Loss: 0.09623068571090698\n",
      "Epoch: 770/2000, Train Loss: 0.0662304013967514, Validation Loss: 0.0957661047577858\n",
      "Epoch: 771/2000, Train Loss: 0.06609312444925308, Validation Loss: 0.09590175747871399\n",
      "Epoch: 772/2000, Train Loss: 0.06598472595214844, Validation Loss: 0.0957871526479721\n",
      "Epoch: 773/2000, Train Loss: 0.06590676307678223, Validation Loss: 0.09570202231407166\n",
      "Epoch: 774/2000, Train Loss: 0.06585335731506348, Validation Loss: 0.09587766230106354\n",
      "Epoch: 775/2000, Train Loss: 0.06581619381904602, Validation Loss: 0.09559884667396545\n",
      "Epoch: 776/2000, Train Loss: 0.06578802317380905, Validation Loss: 0.09596731513738632\n",
      "Epoch: 777/2000, Train Loss: 0.06576064974069595, Validation Loss: 0.09553848206996918\n",
      "Epoch: 778/2000, Train Loss: 0.06573319435119629, Validation Loss: 0.09601623564958572\n",
      "Epoch: 779/2000, Train Loss: 0.06569641083478928, Validation Loss: 0.09548716992139816\n",
      "Epoch: 780/2000, Train Loss: 0.0656564012169838, Validation Loss: 0.09600740671157837\n",
      "Epoch: 781/2000, Train Loss: 0.0656014084815979, Validation Loss: 0.09542825818061829\n",
      "Epoch: 782/2000, Train Loss: 0.06554263830184937, Validation Loss: 0.09593509882688522\n",
      "Epoch: 783/2000, Train Loss: 0.06546803563833237, Validation Loss: 0.09535720944404602\n",
      "Epoch: 784/2000, Train Loss: 0.06539104133844376, Validation Loss: 0.09580696374177933\n",
      "Epoch: 785/2000, Train Loss: 0.0653025284409523, Validation Loss: 0.09528045356273651\n",
      "Epoch: 786/2000, Train Loss: 0.0652146190404892, Validation Loss: 0.09564521163702011\n",
      "Epoch: 787/2000, Train Loss: 0.06512225419282913, Validation Loss: 0.09520938247442245\n",
      "Epoch: 788/2000, Train Loss: 0.06503354758024216, Validation Loss: 0.09547749161720276\n",
      "Epoch: 789/2000, Train Loss: 0.0649462342262268, Validation Loss: 0.09515145421028137\n",
      "Epoch: 790/2000, Train Loss: 0.06486395746469498, Validation Loss: 0.09532465040683746\n",
      "Epoch: 791/2000, Train Loss: 0.06478549540042877, Validation Loss: 0.09510647505521774\n",
      "Epoch: 792/2000, Train Loss: 0.06471150368452072, Validation Loss: 0.09519533812999725\n",
      "Epoch: 793/2000, Train Loss: 0.06464101374149323, Validation Loss: 0.09506934136152267\n",
      "Epoch: 794/2000, Train Loss: 0.0645734965801239, Validation Loss: 0.0950879454612732\n",
      "Epoch: 795/2000, Train Loss: 0.06450819969177246, Validation Loss: 0.09503445774316788\n",
      "Epoch: 796/2000, Train Loss: 0.06444454938173294, Validation Loss: 0.09499569237232208\n",
      "Epoch: 797/2000, Train Loss: 0.06438209861516953, Validation Loss: 0.09499888122081757\n",
      "Epoch: 798/2000, Train Loss: 0.06432054191827774, Validation Loss: 0.0949113667011261\n",
      "Epoch: 799/2000, Train Loss: 0.06425977498292923, Validation Loss: 0.09496398270130157\n",
      "Epoch: 800/2000, Train Loss: 0.06419981271028519, Validation Loss: 0.09483002871274948\n",
      "Epoch: 801/2000, Train Loss: 0.06414090096950531, Validation Loss: 0.09493589401245117\n",
      "Epoch: 802/2000, Train Loss: 0.06408336013555527, Validation Loss: 0.0947493463754654\n",
      "Epoch: 803/2000, Train Loss: 0.0640280544757843, Validation Loss: 0.09492592513561249\n",
      "Epoch: 804/2000, Train Loss: 0.06397580355405807, Validation Loss: 0.0946703627705574\n",
      "Epoch: 805/2000, Train Loss: 0.06392893940210342, Validation Loss: 0.09495515376329422\n",
      "Epoch: 806/2000, Train Loss: 0.0638892874121666, Validation Loss: 0.09460284560918808\n",
      "Epoch: 807/2000, Train Loss: 0.06386319547891617, Validation Loss: 0.09506738930940628\n",
      "Epoch: 808/2000, Train Loss: 0.06385433673858643, Validation Loss: 0.09458222985267639\n",
      "Epoch: 809/2000, Train Loss: 0.06387990713119507, Validation Loss: 0.09535512328147888\n",
      "Epoch: 810/2000, Train Loss: 0.06394389271736145, Validation Loss: 0.09470529854297638\n",
      "Epoch: 811/2000, Train Loss: 0.06409021466970444, Validation Loss: 0.09597835689783096\n",
      "Epoch: 812/2000, Train Loss: 0.06429855525493622, Validation Loss: 0.09513934701681137\n",
      "Epoch: 813/2000, Train Loss: 0.06465843319892883, Validation Loss: 0.09698614478111267\n",
      "Epoch: 814/2000, Train Loss: 0.06499169766902924, Validation Loss: 0.09577538818120956\n",
      "Epoch: 815/2000, Train Loss: 0.06540251523256302, Validation Loss: 0.09749401360750198\n",
      "Epoch: 816/2000, Train Loss: 0.06532185524702072, Validation Loss: 0.09544647485017776\n",
      "Epoch: 817/2000, Train Loss: 0.06495419144630432, Validation Loss: 0.0959264487028122\n",
      "Epoch: 818/2000, Train Loss: 0.0640273243188858, Validation Loss: 0.09429055452346802\n",
      "Epoch: 819/2000, Train Loss: 0.06325080990791321, Validation Loss: 0.09428337961435318\n",
      "Epoch: 820/2000, Train Loss: 0.0629909336566925, Validation Loss: 0.09503616392612457\n",
      "Epoch: 821/2000, Train Loss: 0.0632624626159668, Validation Loss: 0.09460469335317612\n",
      "Epoch: 822/2000, Train Loss: 0.06369365751743317, Validation Loss: 0.09593730419874191\n",
      "Epoch: 823/2000, Train Loss: 0.0638008862733841, Validation Loss: 0.09453793615102768\n",
      "Epoch: 824/2000, Train Loss: 0.0635550394654274, Validation Loss: 0.09494189918041229\n",
      "Epoch: 825/2000, Train Loss: 0.0630299299955368, Validation Loss: 0.09409891813993454\n",
      "Epoch: 826/2000, Train Loss: 0.06265759468078613, Validation Loss: 0.09404963999986649\n",
      "Epoch: 827/2000, Train Loss: 0.06262026727199554, Validation Loss: 0.0947512686252594\n",
      "Epoch: 828/2000, Train Loss: 0.06280691176652908, Validation Loss: 0.09415742009878159\n",
      "Epoch: 829/2000, Train Loss: 0.06295951455831528, Validation Loss: 0.09495484083890915\n",
      "Epoch: 830/2000, Train Loss: 0.06285982578992844, Validation Loss: 0.09397370368242264\n",
      "Epoch: 831/2000, Train Loss: 0.06259634345769882, Validation Loss: 0.0941891223192215\n",
      "Epoch: 832/2000, Train Loss: 0.06232885271310806, Validation Loss: 0.0940278097987175\n",
      "Epoch: 833/2000, Train Loss: 0.062226802110672, Validation Loss: 0.0938510149717331\n",
      "Epoch: 834/2000, Train Loss: 0.062279339879751205, Validation Loss: 0.0944809764623642\n",
      "Epoch: 835/2000, Train Loss: 0.062349215149879456, Validation Loss: 0.09385304898023605\n",
      "Epoch: 836/2000, Train Loss: 0.06232648342847824, Validation Loss: 0.0943310409784317\n",
      "Epoch: 837/2000, Train Loss: 0.06217658519744873, Validation Loss: 0.09374605864286423\n",
      "Epoch: 838/2000, Train Loss: 0.06199761480093002, Validation Loss: 0.09382721036672592\n",
      "Epoch: 839/2000, Train Loss: 0.06187763065099716, Validation Loss: 0.09390437602996826\n",
      "Epoch: 840/2000, Train Loss: 0.061848461627960205, Validation Loss: 0.09363783150911331\n",
      "Epoch: 841/2000, Train Loss: 0.06186413764953613, Validation Loss: 0.09408906102180481\n",
      "Epoch: 842/2000, Train Loss: 0.061851102858781815, Validation Loss: 0.09358083456754684\n",
      "Epoch: 843/2000, Train Loss: 0.061779771000146866, Validation Loss: 0.0938858762383461\n",
      "Epoch: 844/2000, Train Loss: 0.06166188046336174, Validation Loss: 0.09356310218572617\n",
      "Epoch: 845/2000, Train Loss: 0.061550624668598175, Validation Loss: 0.09358272701501846\n",
      "Epoch: 846/2000, Train Loss: 0.061479587107896805, Validation Loss: 0.09370037913322449\n",
      "Epoch: 847/2000, Train Loss: 0.06144797429442406, Validation Loss: 0.0934501588344574\n",
      "Epoch: 848/2000, Train Loss: 0.06142757833003998, Validation Loss: 0.09376754611730576\n",
      "Epoch: 849/2000, Train Loss: 0.061387158930301666, Validation Loss: 0.09338747709989548\n",
      "Epoch: 850/2000, Train Loss: 0.061318881809711456, Validation Loss: 0.09360651671886444\n",
      "Epoch: 851/2000, Train Loss: 0.061231765896081924, Validation Loss: 0.09337331354618073\n",
      "Epoch: 852/2000, Train Loss: 0.06114867702126503, Validation Loss: 0.09339050203561783\n",
      "Epoch: 853/2000, Train Loss: 0.06108368933200836, Validation Loss: 0.09344061464071274\n",
      "Epoch: 854/2000, Train Loss: 0.06103719398379326, Validation Loss: 0.09326519817113876\n",
      "Epoch: 855/2000, Train Loss: 0.06099820137023926, Validation Loss: 0.09347663074731827\n",
      "Epoch: 856/2000, Train Loss: 0.06095331534743309, Validation Loss: 0.09320364892482758\n",
      "Epoch: 857/2000, Train Loss: 0.06089659407734871, Validation Loss: 0.09338942170143127\n",
      "Epoch: 858/2000, Train Loss: 0.06082897260785103, Validation Loss: 0.09317757189273834\n",
      "Epoch: 859/2000, Train Loss: 0.06075873598456383, Validation Loss: 0.0932365357875824\n",
      "Epoch: 860/2000, Train Loss: 0.060693010687828064, Validation Loss: 0.09319040924310684\n",
      "Epoch: 861/2000, Train Loss: 0.06063548102974892, Validation Loss: 0.09310806542634964\n",
      "Epoch: 862/2000, Train Loss: 0.06058475747704506, Validation Loss: 0.09320560097694397\n",
      "Epoch: 863/2000, Train Loss: 0.060536351054906845, Validation Loss: 0.0930262953042984\n",
      "Epoch: 864/2000, Train Loss: 0.060485851019620895, Validation Loss: 0.09317020326852798\n",
      "Epoch: 865/2000, Train Loss: 0.06043057516217232, Validation Loss: 0.09297589957714081\n",
      "Epoch: 866/2000, Train Loss: 0.060371167957782745, Validation Loss: 0.09307831525802612\n",
      "Epoch: 867/2000, Train Loss: 0.06030955910682678, Validation Loss: 0.09294909238815308\n",
      "Epoch: 868/2000, Train Loss: 0.060248587280511856, Validation Loss: 0.0929683968424797\n",
      "Epoch: 869/2000, Train Loss: 0.06019006296992302, Validation Loss: 0.09293999522924423\n",
      "Epoch: 870/2000, Train Loss: 0.060134463012218475, Validation Loss: 0.09287524968385696\n",
      "Epoch: 871/2000, Train Loss: 0.06008107587695122, Validation Loss: 0.09292849898338318\n",
      "Epoch: 872/2000, Train Loss: 0.06002851948142052, Validation Loss: 0.09280439466238022\n",
      "Epoch: 873/2000, Train Loss: 0.0599755235016346, Validation Loss: 0.09289155900478363\n",
      "Epoch: 874/2000, Train Loss: 0.05992121621966362, Validation Loss: 0.09274763613939285\n",
      "Epoch: 875/2000, Train Loss: 0.059865519404411316, Validation Loss: 0.09282565861940384\n",
      "Epoch: 876/2000, Train Loss: 0.059808675199747086, Validation Loss: 0.09270261228084564\n",
      "Epoch: 877/2000, Train Loss: 0.05975139141082764, Validation Loss: 0.09274628758430481\n",
      "Epoch: 878/2000, Train Loss: 0.05969420447945595, Validation Loss: 0.09266908466815948\n",
      "Epoch: 879/2000, Train Loss: 0.05963760241866112, Validation Loss: 0.09266787022352219\n",
      "Epoch: 880/2000, Train Loss: 0.059581778943538666, Validation Loss: 0.09263992309570312\n",
      "Epoch: 881/2000, Train Loss: 0.0595267079770565, Validation Loss: 0.09259511530399323\n",
      "Epoch: 882/2000, Train Loss: 0.0594722181558609, Validation Loss: 0.0926068052649498\n",
      "Epoch: 883/2000, Train Loss: 0.05941805616021156, Validation Loss: 0.09252943098545074\n",
      "Epoch: 884/2000, Train Loss: 0.05936400219798088, Validation Loss: 0.09256713837385178\n",
      "Epoch: 885/2000, Train Loss: 0.05930986627936363, Validation Loss: 0.09247138351202011\n",
      "Epoch: 886/2000, Train Loss: 0.05925557762384415, Validation Loss: 0.09252121299505234\n",
      "Epoch: 887/2000, Train Loss: 0.05920107290148735, Validation Loss: 0.09241849929094315\n",
      "Epoch: 888/2000, Train Loss: 0.05914640799164772, Validation Loss: 0.09246936440467834\n",
      "Epoch: 889/2000, Train Loss: 0.05909160152077675, Validation Loss: 0.09236784279346466\n",
      "Epoch: 890/2000, Train Loss: 0.059036727994680405, Validation Loss: 0.0924140214920044\n",
      "Epoch: 891/2000, Train Loss: 0.05898182466626167, Validation Loss: 0.0923188254237175\n",
      "Epoch: 892/2000, Train Loss: 0.05892696604132652, Validation Loss: 0.0923587903380394\n",
      "Epoch: 893/2000, Train Loss: 0.058872152119874954, Validation Loss: 0.09227102249860764\n",
      "Epoch: 894/2000, Train Loss: 0.058817435055971146, Validation Loss: 0.09230510145425797\n",
      "Epoch: 895/2000, Train Loss: 0.0587627999484539, Validation Loss: 0.09222272038459778\n",
      "Epoch: 896/2000, Train Loss: 0.058708272874355316, Validation Loss: 0.09225305169820786\n",
      "Epoch: 897/2000, Train Loss: 0.05865384265780449, Validation Loss: 0.0921727865934372\n",
      "Epoch: 898/2000, Train Loss: 0.05859953165054321, Validation Loss: 0.09220337867736816\n",
      "Epoch: 899/2000, Train Loss: 0.05854532867670059, Validation Loss: 0.09212101250886917\n",
      "Epoch: 900/2000, Train Loss: 0.05849127471446991, Validation Loss: 0.09215685725212097\n",
      "Epoch: 901/2000, Train Loss: 0.05843736603856087, Validation Loss: 0.09206673502922058\n",
      "Epoch: 902/2000, Train Loss: 0.058383673429489136, Validation Loss: 0.09211453050374985\n",
      "Epoch: 903/2000, Train Loss: 0.0583302341401577, Validation Loss: 0.09200941771268845\n",
      "Epoch: 904/2000, Train Loss: 0.05827717483043671, Validation Loss: 0.0920790433883667\n",
      "Epoch: 905/2000, Train Loss: 0.058224644511938095, Validation Loss: 0.09194859117269516\n",
      "Epoch: 906/2000, Train Loss: 0.05817291885614395, Validation Loss: 0.09205490350723267\n",
      "Epoch: 907/2000, Train Loss: 0.05812235176563263, Validation Loss: 0.09188313037157059\n",
      "Epoch: 908/2000, Train Loss: 0.05807371065020561, Validation Loss: 0.092050701379776\n",
      "Epoch: 909/2000, Train Loss: 0.05802782624959946, Validation Loss: 0.09181376546621323\n",
      "Epoch: 910/2000, Train Loss: 0.057986848056316376, Validation Loss: 0.0920867919921875\n",
      "Epoch: 911/2000, Train Loss: 0.057952847331762314, Validation Loss: 0.09175083786249161\n",
      "Epoch: 912/2000, Train Loss: 0.05793190747499466, Validation Loss: 0.09221095591783524\n",
      "Epoch: 913/2000, Train Loss: 0.05792896822094917, Validation Loss: 0.09173266589641571\n",
      "Epoch: 914/2000, Train Loss: 0.057961829006671906, Validation Loss: 0.09253200143575668\n",
      "Epoch: 915/2000, Train Loss: 0.05803924426436424, Validation Loss: 0.09187552332878113\n",
      "Epoch: 916/2000, Train Loss: 0.05821182578802109, Validation Loss: 0.09327251464128494\n",
      "Epoch: 917/2000, Train Loss: 0.0584728978574276, Validation Loss: 0.0924370288848877\n",
      "Epoch: 918/2000, Train Loss: 0.058943577110767365, Validation Loss: 0.09464710205793381\n",
      "Epoch: 919/2000, Train Loss: 0.05944611877202988, Validation Loss: 0.09348084032535553\n",
      "Epoch: 920/2000, Train Loss: 0.060150302946567535, Validation Loss: 0.09574376791715622\n",
      "Epoch: 921/2000, Train Loss: 0.06025531515479088, Validation Loss: 0.09339088946580887\n",
      "Epoch: 922/2000, Train Loss: 0.05999383702874184, Validation Loss: 0.09387914091348648\n",
      "Epoch: 923/2000, Train Loss: 0.05871763452887535, Validation Loss: 0.09157069772481918\n",
      "Epoch: 924/2000, Train Loss: 0.057565849274396896, Validation Loss: 0.09150480479001999\n",
      "Epoch: 925/2000, Train Loss: 0.05720028653740883, Validation Loss: 0.09263820201158524\n",
      "Epoch: 926/2000, Train Loss: 0.05769002065062523, Validation Loss: 0.09217526763677597\n",
      "Epoch: 927/2000, Train Loss: 0.058356720954179764, Validation Loss: 0.09370091557502747\n",
      "Epoch: 928/2000, Train Loss: 0.058360692113637924, Validation Loss: 0.0918300449848175\n",
      "Epoch: 929/2000, Train Loss: 0.05782220885157585, Validation Loss: 0.09198521822690964\n",
      "Epoch: 930/2000, Train Loss: 0.05710696056485176, Validation Loss: 0.09157480299472809\n",
      "Epoch: 931/2000, Train Loss: 0.0568854846060276, Validation Loss: 0.09142392873764038\n",
      "Epoch: 932/2000, Train Loss: 0.0571724958717823, Validation Loss: 0.09268645197153091\n",
      "Epoch: 933/2000, Train Loss: 0.05747072771191597, Validation Loss: 0.09157583862543106\n",
      "Epoch: 934/2000, Train Loss: 0.05743132531642914, Validation Loss: 0.0920880064368248\n",
      "Epoch: 935/2000, Train Loss: 0.05701039358973503, Validation Loss: 0.0912378579378128\n",
      "Epoch: 936/2000, Train Loss: 0.056654222309589386, Validation Loss: 0.09120818227529526\n",
      "Epoch: 937/2000, Train Loss: 0.056616902351379395, Validation Loss: 0.09191495180130005\n",
      "Epoch: 938/2000, Train Loss: 0.05679631605744362, Validation Loss: 0.09128862619400024\n",
      "Epoch: 939/2000, Train Loss: 0.05690556764602661, Validation Loss: 0.09197379648685455\n",
      "Epoch: 940/2000, Train Loss: 0.056753575801849365, Validation Loss: 0.09112962335348129\n",
      "Epoch: 941/2000, Train Loss: 0.056490708142519, Validation Loss: 0.09123995155096054\n",
      "Epoch: 942/2000, Train Loss: 0.05632110685110092, Validation Loss: 0.09142839908599854\n",
      "Epoch: 943/2000, Train Loss: 0.05633513256907463, Validation Loss: 0.0910855308175087\n",
      "Epoch: 944/2000, Train Loss: 0.056421536952257156, Validation Loss: 0.09171010553836823\n",
      "Epoch: 945/2000, Train Loss: 0.05641324818134308, Validation Loss: 0.09101112186908722\n",
      "Epoch: 946/2000, Train Loss: 0.05628495290875435, Validation Loss: 0.09125424176454544\n",
      "Epoch: 947/2000, Train Loss: 0.05611617863178253, Validation Loss: 0.09106769412755966\n",
      "Epoch: 948/2000, Train Loss: 0.05602491274476051, Validation Loss: 0.09093641489744186\n",
      "Epoch: 949/2000, Train Loss: 0.05602717027068138, Validation Loss: 0.09136517345905304\n",
      "Epoch: 950/2000, Train Loss: 0.05604659020900726, Validation Loss: 0.09089389443397522\n",
      "Epoch: 951/2000, Train Loss: 0.05601068213582039, Validation Loss: 0.09123612940311432\n",
      "Epoch: 952/2000, Train Loss: 0.055907171219587326, Validation Loss: 0.09087686985731125\n",
      "Epoch: 953/2000, Train Loss: 0.055797334760427475, Validation Loss: 0.09090042114257812\n",
      "Epoch: 954/2000, Train Loss: 0.055731844156980515, Validation Loss: 0.09104649722576141\n",
      "Epoch: 955/2000, Train Loss: 0.05571302771568298, Validation Loss: 0.09077803045511246\n",
      "Epoch: 956/2000, Train Loss: 0.05570172891020775, Validation Loss: 0.09111294895410538\n",
      "Epoch: 957/2000, Train Loss: 0.05565878003835678, Validation Loss: 0.09074262529611588\n",
      "Epoch: 958/2000, Train Loss: 0.05558373034000397, Validation Loss: 0.09090510755777359\n",
      "Epoch: 959/2000, Train Loss: 0.05550079420208931, Validation Loss: 0.0907944068312645\n",
      "Epoch: 960/2000, Train Loss: 0.055438533425331116, Validation Loss: 0.09071334451436996\n",
      "Epoch: 961/2000, Train Loss: 0.05540185421705246, Validation Loss: 0.09090530127286911\n",
      "Epoch: 962/2000, Train Loss: 0.05537383630871773, Validation Loss: 0.09064459055662155\n",
      "Epoch: 963/2000, Train Loss: 0.05533485487103462, Validation Loss: 0.09086944907903671\n",
      "Epoch: 964/2000, Train Loss: 0.055277079343795776, Validation Loss: 0.0906335785984993\n",
      "Epoch: 965/2000, Train Loss: 0.05521057918667793, Validation Loss: 0.09070635586977005\n",
      "Epoch: 966/2000, Train Loss: 0.055149152874946594, Validation Loss: 0.09068410098552704\n",
      "Epoch: 967/2000, Train Loss: 0.055100612342357635, Validation Loss: 0.09057852625846863\n",
      "Epoch: 968/2000, Train Loss: 0.05506150797009468, Validation Loss: 0.0907275378704071\n",
      "Epoch: 969/2000, Train Loss: 0.055022239685058594, Validation Loss: 0.09051871299743652\n",
      "Epoch: 970/2000, Train Loss: 0.054975736886262894, Validation Loss: 0.09066884964704514\n",
      "Epoch: 971/2000, Train Loss: 0.054921094328165054, Validation Loss: 0.0905032679438591\n",
      "Epoch: 972/2000, Train Loss: 0.054863836616277695, Validation Loss: 0.09054552763700485\n",
      "Epoch: 973/2000, Train Loss: 0.054809775203466415, Validation Loss: 0.09052626043558121\n",
      "Epoch: 974/2000, Train Loss: 0.054761629551649094, Validation Loss: 0.090446338057518\n",
      "Epoch: 975/2000, Train Loss: 0.05471772328019142, Validation Loss: 0.09054483473300934\n",
      "Epoch: 976/2000, Train Loss: 0.05467407405376434, Validation Loss: 0.09039469808340073\n",
      "Epoch: 977/2000, Train Loss: 0.054627545177936554, Validation Loss: 0.0905081182718277\n",
      "Epoch: 978/2000, Train Loss: 0.054577264934778214, Validation Loss: 0.09037528187036514\n",
      "Epoch: 979/2000, Train Loss: 0.054525043815374374, Validation Loss: 0.09042420983314514\n",
      "Epoch: 980/2000, Train Loss: 0.054473280906677246, Validation Loss: 0.09037604928016663\n",
      "Epoch: 981/2000, Train Loss: 0.0544237457215786, Validation Loss: 0.09033887088298798\n",
      "Epoch: 982/2000, Train Loss: 0.05437657982110977, Validation Loss: 0.09037812799215317\n",
      "Epoch: 983/2000, Train Loss: 0.05433065816760063, Validation Loss: 0.09027872234582901\n",
      "Epoch: 984/2000, Train Loss: 0.05428449437022209, Validation Loss: 0.09035652875900269\n",
      "Epoch: 985/2000, Train Loss: 0.05423703044652939, Validation Loss: 0.09024321287870407\n",
      "Epoch: 986/2000, Train Loss: 0.054188162088394165, Validation Loss: 0.09030486643314362\n",
      "Epoch: 987/2000, Train Loss: 0.054138392210006714, Validation Loss: 0.09022543579339981\n",
      "Epoch: 988/2000, Train Loss: 0.054088592529296875, Validation Loss: 0.09024029970169067\n",
      "Epoch: 989/2000, Train Loss: 0.05403940752148628, Validation Loss: 0.09021785110235214\n",
      "Epoch: 990/2000, Train Loss: 0.05399106815457344, Validation Loss: 0.09018179029226303\n",
      "Epoch: 991/2000, Train Loss: 0.053943414241075516, Validation Loss: 0.09020823985338211\n",
      "Epoch: 992/2000, Train Loss: 0.05389605835080147, Validation Loss: 0.09013472497463226\n",
      "Epoch: 993/2000, Train Loss: 0.05384858325123787, Validation Loss: 0.09018527716398239\n",
      "Epoch: 994/2000, Train Loss: 0.05380076542496681, Validation Loss: 0.09009695053100586\n",
      "Epoch: 995/2000, Train Loss: 0.05375254526734352, Validation Loss: 0.09014855325222015\n",
      "Epoch: 996/2000, Train Loss: 0.053703997284173965, Validation Loss: 0.09006612747907639\n",
      "Epoch: 997/2000, Train Loss: 0.05365527421236038, Validation Loss: 0.0901036262512207\n",
      "Epoch: 998/2000, Train Loss: 0.05360652133822441, Validation Loss: 0.09003766626119614\n",
      "Epoch: 999/2000, Train Loss: 0.05355784296989441, Validation Loss: 0.0900556743144989\n",
      "Epoch: 1000/2000, Train Loss: 0.053509291261434555, Validation Loss: 0.09000936150550842\n",
      "Epoch: 1001/2000, Train Loss: 0.05346085503697395, Validation Loss: 0.09001033008098602\n",
      "Epoch: 1002/2000, Train Loss: 0.0534125417470932, Validation Loss: 0.08998052775859833\n",
      "Epoch: 1003/2000, Train Loss: 0.053364310413599014, Validation Loss: 0.0899670273065567\n",
      "Epoch: 1004/2000, Train Loss: 0.0533161498606205, Validation Loss: 0.0899490937590599\n",
      "Epoch: 1005/2000, Train Loss: 0.05326805263757706, Validation Loss: 0.089924655854702\n",
      "Epoch: 1006/2000, Train Loss: 0.05322003737092018, Validation Loss: 0.08991727232933044\n",
      "Epoch: 1007/2000, Train Loss: 0.05317212641239166, Validation Loss: 0.08988232910633087\n",
      "Epoch: 1008/2000, Train Loss: 0.05312435328960419, Validation Loss: 0.08988723158836365\n",
      "Epoch: 1009/2000, Train Loss: 0.05307675898075104, Validation Loss: 0.08983835577964783\n",
      "Epoch: 1010/2000, Train Loss: 0.05302939563989639, Validation Loss: 0.0898628756403923\n",
      "Epoch: 1011/2000, Train Loss: 0.05298236012458801, Validation Loss: 0.08979007601737976\n",
      "Epoch: 1012/2000, Train Loss: 0.05293586477637291, Validation Loss: 0.0898495465517044\n",
      "Epoch: 1013/2000, Train Loss: 0.052890144288539886, Validation Loss: 0.08973504602909088\n",
      "Epoch: 1014/2000, Train Loss: 0.05284593999385834, Validation Loss: 0.08985606580972672\n",
      "Epoch: 1015/2000, Train Loss: 0.052803684026002884, Validation Loss: 0.08967449516057968\n",
      "Epoch: 1016/2000, Train Loss: 0.05276523903012276, Validation Loss: 0.08989410847425461\n",
      "Epoch: 1017/2000, Train Loss: 0.052730776369571686, Validation Loss: 0.08961714804172516\n",
      "Epoch: 1018/2000, Train Loss: 0.052703890949487686, Validation Loss: 0.08997683227062225\n",
      "Epoch: 1019/2000, Train Loss: 0.05268341675400734, Validation Loss: 0.08957765996456146\n",
      "Epoch: 1020/2000, Train Loss: 0.05267604812979698, Validation Loss: 0.09011872112751007\n",
      "Epoch: 1021/2000, Train Loss: 0.05267654359340668, Validation Loss: 0.08957520872354507\n",
      "Epoch: 1022/2000, Train Loss: 0.052696872502565384, Validation Loss: 0.09032943099737167\n",
      "Epoch: 1023/2000, Train Loss: 0.05272199586033821, Validation Loss: 0.08962476998567581\n",
      "Epoch: 1024/2000, Train Loss: 0.05277373269200325, Validation Loss: 0.0905938521027565\n",
      "Epoch: 1025/2000, Train Loss: 0.052815135568380356, Validation Loss: 0.08971487730741501\n",
      "Epoch: 1026/2000, Train Loss: 0.052881620824337006, Validation Loss: 0.09082832932472229\n",
      "Epoch: 1027/2000, Train Loss: 0.05289950594305992, Validation Loss: 0.08977387845516205\n",
      "Epoch: 1028/2000, Train Loss: 0.05292097106575966, Validation Loss: 0.09085185080766678\n",
      "Epoch: 1029/2000, Train Loss: 0.052843332290649414, Validation Loss: 0.0896875411272049\n",
      "Epoch: 1030/2000, Train Loss: 0.05274038761854172, Validation Loss: 0.0905090793967247\n",
      "Epoch: 1031/2000, Train Loss: 0.052540626376867294, Validation Loss: 0.08947509527206421\n",
      "Epoch: 1032/2000, Train Loss: 0.052333440631628036, Validation Loss: 0.08993697166442871\n",
      "Epoch: 1033/2000, Train Loss: 0.05212104693055153, Validation Loss: 0.08937988430261612\n",
      "Epoch: 1034/2000, Train Loss: 0.05196109041571617, Validation Loss: 0.08949580788612366\n",
      "Epoch: 1035/2000, Train Loss: 0.05186561122536659, Validation Loss: 0.089542917907238\n",
      "Epoch: 1036/2000, Train Loss: 0.05183202773332596, Validation Loss: 0.08933161199092865\n",
      "Epoch: 1037/2000, Train Loss: 0.051840927451848984, Validation Loss: 0.08981368690729141\n",
      "Epoch: 1038/2000, Train Loss: 0.05186599865555763, Validation Loss: 0.089316725730896\n",
      "Epoch: 1039/2000, Train Loss: 0.05189042165875435, Validation Loss: 0.08998234570026398\n",
      "Epoch: 1040/2000, Train Loss: 0.051887787878513336, Validation Loss: 0.08930303156375885\n",
      "Epoch: 1041/2000, Train Loss: 0.05186426639556885, Validation Loss: 0.0899466872215271\n",
      "Epoch: 1042/2000, Train Loss: 0.05179924890398979, Validation Loss: 0.0892433226108551\n",
      "Epoch: 1043/2000, Train Loss: 0.051716625690460205, Validation Loss: 0.08973515033721924\n",
      "Epoch: 1044/2000, Train Loss: 0.051611125469207764, Validation Loss: 0.08918676525354385\n",
      "Epoch: 1045/2000, Train Loss: 0.05150765925645828, Validation Loss: 0.08947165310382843\n",
      "Epoch: 1046/2000, Train Loss: 0.051411524415016174, Validation Loss: 0.08920054137706757\n",
      "Epoch: 1047/2000, Train Loss: 0.05133352428674698, Validation Loss: 0.08926964551210403\n",
      "Epoch: 1048/2000, Train Loss: 0.05127475783228874, Validation Loss: 0.08928552269935608\n",
      "Epoch: 1049/2000, Train Loss: 0.05123293772339821, Validation Loss: 0.08915460109710693\n",
      "Epoch: 1050/2000, Train Loss: 0.051202964037656784, Validation Loss: 0.08938544243574142\n",
      "Epoch: 1051/2000, Train Loss: 0.05117879435420036, Validation Loss: 0.089092917740345\n",
      "Epoch: 1052/2000, Train Loss: 0.051156170666217804, Validation Loss: 0.08944910019636154\n",
      "Epoch: 1053/2000, Train Loss: 0.05112973228096962, Validation Loss: 0.08905115723609924\n",
      "Epoch: 1054/2000, Train Loss: 0.05109989643096924, Validation Loss: 0.08946005254983902\n",
      "Epoch: 1055/2000, Train Loss: 0.05106199160218239, Validation Loss: 0.08901701867580414\n",
      "Epoch: 1056/2000, Train Loss: 0.05102026090025902, Validation Loss: 0.0894257053732872\n",
      "Epoch: 1057/2000, Train Loss: 0.05097071826457977, Validation Loss: 0.08898857980966568\n",
      "Epoch: 1058/2000, Train Loss: 0.050919100642204285, Validation Loss: 0.0893598198890686\n",
      "Epoch: 1059/2000, Train Loss: 0.050862547010183334, Validation Loss: 0.08896568417549133\n",
      "Epoch: 1060/2000, Train Loss: 0.05080614238977432, Validation Loss: 0.08927752822637558\n",
      "Epoch: 1061/2000, Train Loss: 0.05074816197156906, Validation Loss: 0.08894769102334976\n",
      "Epoch: 1062/2000, Train Loss: 0.050691910088062286, Validation Loss: 0.08919137716293335\n",
      "Epoch: 1063/2000, Train Loss: 0.05063634365797043, Validation Loss: 0.08893153816461563\n",
      "Epoch: 1064/2000, Train Loss: 0.05058297514915466, Validation Loss: 0.0891106128692627\n",
      "Epoch: 1065/2000, Train Loss: 0.0505310595035553, Validation Loss: 0.08891613036394119\n",
      "Epoch: 1066/2000, Train Loss: 0.050480980426073074, Validation Loss: 0.08904166519641876\n",
      "Epoch: 1067/2000, Train Loss: 0.050432220101356506, Validation Loss: 0.08890119940042496\n",
      "Epoch: 1068/2000, Train Loss: 0.05038468912243843, Validation Loss: 0.0889856219291687\n",
      "Epoch: 1069/2000, Train Loss: 0.050338033586740494, Validation Loss: 0.08888504654169083\n",
      "Epoch: 1070/2000, Train Loss: 0.05029207095503807, Validation Loss: 0.08893956243991852\n",
      "Epoch: 1071/2000, Train Loss: 0.05024658888578415, Validation Loss: 0.08886539191007614\n",
      "Epoch: 1072/2000, Train Loss: 0.05020149052143097, Validation Loss: 0.08889997750520706\n",
      "Epoch: 1073/2000, Train Loss: 0.05015664920210838, Validation Loss: 0.08884190768003464\n",
      "Epoch: 1074/2000, Train Loss: 0.05011199787259102, Validation Loss: 0.08886580914258957\n",
      "Epoch: 1075/2000, Train Loss: 0.0500674694776535, Validation Loss: 0.08881552517414093\n",
      "Epoch: 1076/2000, Train Loss: 0.050023045390844345, Validation Loss: 0.08883534371852875\n",
      "Epoch: 1077/2000, Train Loss: 0.04997871443629265, Validation Loss: 0.08878491818904877\n",
      "Epoch: 1078/2000, Train Loss: 0.049934469163417816, Validation Loss: 0.08880694210529327\n",
      "Epoch: 1079/2000, Train Loss: 0.049890317022800446, Validation Loss: 0.08875026553869247\n",
      "Epoch: 1080/2000, Train Loss: 0.049846287816762924, Validation Loss: 0.08878324180841446\n",
      "Epoch: 1081/2000, Train Loss: 0.049802426248788834, Validation Loss: 0.08871446549892426\n",
      "Epoch: 1082/2000, Train Loss: 0.04975879564881325, Validation Loss: 0.08876907080411911\n",
      "Epoch: 1083/2000, Train Loss: 0.049715518951416016, Validation Loss: 0.08867696672677994\n",
      "Epoch: 1084/2000, Train Loss: 0.04967283084988594, Validation Loss: 0.08876615017652512\n",
      "Epoch: 1085/2000, Train Loss: 0.04963104426860809, Validation Loss: 0.0886320099234581\n",
      "Epoch: 1086/2000, Train Loss: 0.04959079995751381, Validation Loss: 0.08878073841333389\n",
      "Epoch: 1087/2000, Train Loss: 0.04955298453569412, Validation Loss: 0.08857850730419159\n",
      "Epoch: 1088/2000, Train Loss: 0.04951956868171692, Validation Loss: 0.08883693069219589\n",
      "Epoch: 1089/2000, Train Loss: 0.049493059515953064, Validation Loss: 0.088528111577034\n",
      "Epoch: 1090/2000, Train Loss: 0.049479659646749496, Validation Loss: 0.0889919325709343\n",
      "Epoch: 1091/2000, Train Loss: 0.04948634281754494, Validation Loss: 0.08852431923151016\n",
      "Epoch: 1092/2000, Train Loss: 0.04953330010175705, Validation Loss: 0.08938711881637573\n",
      "Epoch: 1093/2000, Train Loss: 0.049637094140052795, Validation Loss: 0.0887262374162674\n",
      "Epoch: 1094/2000, Train Loss: 0.04986325278878212, Validation Loss: 0.09036698192358017\n",
      "Epoch: 1095/2000, Train Loss: 0.05022638291120529, Validation Loss: 0.08957535773515701\n",
      "Epoch: 1096/2000, Train Loss: 0.05091189220547676, Validation Loss: 0.09244777262210846\n",
      "Epoch: 1097/2000, Train Loss: 0.0517263300716877, Validation Loss: 0.09146251529455185\n",
      "Epoch: 1098/2000, Train Loss: 0.052984122186899185, Validation Loss: 0.09458567202091217\n",
      "Epoch: 1099/2000, Train Loss: 0.05336964502930641, Validation Loss: 0.09172791987657547\n",
      "Epoch: 1100/2000, Train Loss: 0.05320552736520767, Validation Loss: 0.09177679568529129\n",
      "Epoch: 1101/2000, Train Loss: 0.05108392611145973, Validation Loss: 0.08849651366472244\n",
      "Epoch: 1102/2000, Train Loss: 0.04925717040896416, Validation Loss: 0.08842385560274124\n",
      "Epoch: 1103/2000, Train Loss: 0.049069277942180634, Validation Loss: 0.09077761322259903\n",
      "Epoch: 1104/2000, Train Loss: 0.05021819844841957, Validation Loss: 0.08998892456293106\n",
      "Epoch: 1105/2000, Train Loss: 0.05109785869717598, Validation Loss: 0.09108927845954895\n",
      "Epoch: 1106/2000, Train Loss: 0.05033846199512482, Validation Loss: 0.08854853361845016\n",
      "Epoch: 1107/2000, Train Loss: 0.049118392169475555, Validation Loss: 0.0884479507803917\n",
      "Epoch: 1108/2000, Train Loss: 0.04869173467159271, Validation Loss: 0.08980534225702286\n",
      "Epoch: 1109/2000, Train Loss: 0.04930848255753517, Validation Loss: 0.08912112563848495\n",
      "Epoch: 1110/2000, Train Loss: 0.049901772290468216, Validation Loss: 0.09014338999986649\n",
      "Epoch: 1111/2000, Train Loss: 0.049498267471790314, Validation Loss: 0.0883411318063736\n",
      "Epoch: 1112/2000, Train Loss: 0.04873817414045334, Validation Loss: 0.08831551671028137\n",
      "Epoch: 1113/2000, Train Loss: 0.04847609996795654, Validation Loss: 0.08928616344928741\n",
      "Epoch: 1114/2000, Train Loss: 0.04885222762823105, Validation Loss: 0.088599793612957\n",
      "Epoch: 1115/2000, Train Loss: 0.04917217418551445, Validation Loss: 0.08941949158906937\n",
      "Epoch: 1116/2000, Train Loss: 0.048882681876420975, Validation Loss: 0.08820564299821854\n",
      "Epoch: 1117/2000, Train Loss: 0.048407845199108124, Validation Loss: 0.08822879195213318\n",
      "Epoch: 1118/2000, Train Loss: 0.04827788099646568, Validation Loss: 0.08899989724159241\n",
      "Epoch: 1119/2000, Train Loss: 0.04850844293832779, Validation Loss: 0.08835722506046295\n",
      "Epoch: 1120/2000, Train Loss: 0.04865390434861183, Validation Loss: 0.0890001654624939\n",
      "Epoch: 1121/2000, Train Loss: 0.0484350323677063, Validation Loss: 0.0881793275475502\n",
      "Epoch: 1122/2000, Train Loss: 0.048140592873096466, Validation Loss: 0.08816873282194138\n",
      "Epoch: 1123/2000, Train Loss: 0.04807651787996292, Validation Loss: 0.08873405307531357\n",
      "Epoch: 1124/2000, Train Loss: 0.048207152634859085, Validation Loss: 0.08814509212970734\n",
      "Epoch: 1125/2000, Train Loss: 0.048262543976306915, Validation Loss: 0.0886504277586937\n",
      "Epoch: 1126/2000, Train Loss: 0.04810566455125809, Validation Loss: 0.08807923644781113\n",
      "Epoch: 1127/2000, Train Loss: 0.04791560396552086, Validation Loss: 0.08807306736707687\n",
      "Epoch: 1128/2000, Train Loss: 0.047867875546216965, Validation Loss: 0.08849387615919113\n",
      "Epoch: 1129/2000, Train Loss: 0.04793104901909828, Validation Loss: 0.08801297843456268\n",
      "Epoch: 1130/2000, Train Loss: 0.04794462397694588, Validation Loss: 0.08842453360557556\n",
      "Epoch: 1131/2000, Train Loss: 0.04783494025468826, Validation Loss: 0.08800549060106277\n",
      "Epoch: 1132/2000, Train Loss: 0.04770534858107567, Validation Loss: 0.08800719678401947\n",
      "Epoch: 1133/2000, Train Loss: 0.04765830934047699, Validation Loss: 0.0883060097694397\n",
      "Epoch: 1134/2000, Train Loss: 0.047678638249635696, Validation Loss: 0.08793403208255768\n",
      "Epoch: 1135/2000, Train Loss: 0.04767478257417679, Validation Loss: 0.08828260749578476\n",
      "Epoch: 1136/2000, Train Loss: 0.04759927839040756, Validation Loss: 0.0879441499710083\n",
      "Epoch: 1137/2000, Train Loss: 0.047503914684057236, Validation Loss: 0.08796658366918564\n",
      "Epoch: 1138/2000, Train Loss: 0.04745117574930191, Validation Loss: 0.08814626932144165\n",
      "Epoch: 1139/2000, Train Loss: 0.04744335263967514, Validation Loss: 0.08786924928426743\n",
      "Epoch: 1140/2000, Train Loss: 0.047431156039237976, Validation Loss: 0.08816539496183395\n",
      "Epoch: 1141/2000, Train Loss: 0.04737906903028488, Validation Loss: 0.08787901699542999\n",
      "Epoch: 1142/2000, Train Loss: 0.04730618745088577, Validation Loss: 0.08793874830007553\n",
      "Epoch: 1143/2000, Train Loss: 0.04724982753396034, Validation Loss: 0.08801610767841339\n",
      "Epoch: 1144/2000, Train Loss: 0.04722273722290993, Validation Loss: 0.08782801032066345\n",
      "Epoch: 1145/2000, Train Loss: 0.047203246504068375, Validation Loss: 0.08807104080915451\n",
      "Epoch: 1146/2000, Train Loss: 0.047165509313344955, Validation Loss: 0.08782431483268738\n",
      "Epoch: 1147/2000, Train Loss: 0.04710955172777176, Validation Loss: 0.08792156726121902\n",
      "Epoch: 1148/2000, Train Loss: 0.047053951770067215, Validation Loss: 0.08790472149848938\n",
      "Epoch: 1149/2000, Train Loss: 0.047013960778713226, Validation Loss: 0.08779297769069672\n",
      "Epoch: 1150/2000, Train Loss: 0.04698565974831581, Validation Loss: 0.08796362578868866\n",
      "Epoch: 1151/2000, Train Loss: 0.04695412144064903, Validation Loss: 0.08775369077920914\n",
      "Epoch: 1152/2000, Train Loss: 0.0469110943377018, Validation Loss: 0.08787615597248077\n",
      "Epoch: 1153/2000, Train Loss: 0.04686105623841286, Validation Loss: 0.08778617531061172\n",
      "Epoch: 1154/2000, Train Loss: 0.04681485518813133, Validation Loss: 0.0877537801861763\n",
      "Epoch: 1155/2000, Train Loss: 0.04677740857005119, Validation Loss: 0.0878443717956543\n",
      "Epoch: 1156/2000, Train Loss: 0.046744395047426224, Validation Loss: 0.08769379556179047\n",
      "Epoch: 1157/2000, Train Loss: 0.04670839011669159, Validation Loss: 0.08781994879245758\n",
      "Epoch: 1158/2000, Train Loss: 0.04666614904999733, Validation Loss: 0.08769156038761139\n",
      "Epoch: 1159/2000, Train Loss: 0.04662097617983818, Validation Loss: 0.08772498369216919\n",
      "Epoch: 1160/2000, Train Loss: 0.04657803475856781, Validation Loss: 0.08772701025009155\n",
      "Epoch: 1161/2000, Train Loss: 0.04653957113623619, Validation Loss: 0.0876464694738388\n",
      "Epoch: 1162/2000, Train Loss: 0.04650354012846947, Validation Loss: 0.08773741126060486\n",
      "Epoch: 1163/2000, Train Loss: 0.04646624997258186, Validation Loss: 0.0876113772392273\n",
      "Epoch: 1164/2000, Train Loss: 0.04642591252923012, Validation Loss: 0.08768351376056671\n",
      "Epoch: 1165/2000, Train Loss: 0.046383537352085114, Validation Loss: 0.087612584233284\n",
      "Epoch: 1166/2000, Train Loss: 0.046341609209775925, Validation Loss: 0.08760446310043335\n",
      "Epoch: 1167/2000, Train Loss: 0.04630172252655029, Validation Loss: 0.08762585371732712\n",
      "Epoch: 1168/2000, Train Loss: 0.04626357555389404, Validation Loss: 0.08754600584506989\n",
      "Epoch: 1169/2000, Train Loss: 0.046225644648075104, Validation Loss: 0.08761357516050339\n",
      "Epoch: 1170/2000, Train Loss: 0.04618655890226364, Validation Loss: 0.08751919120550156\n",
      "Epoch: 1171/2000, Train Loss: 0.046146050095558167, Validation Loss: 0.08756737411022186\n",
      "Epoch: 1172/2000, Train Loss: 0.046104852110147476, Validation Loss: 0.08751717954874039\n",
      "Epoch: 1173/2000, Train Loss: 0.046063974499702454, Validation Loss: 0.08751143515110016\n",
      "Epoch: 1174/2000, Train Loss: 0.04602396860718727, Validation Loss: 0.08752141147851944\n",
      "Epoch: 1175/2000, Train Loss: 0.045984696596860886, Validation Loss: 0.087466761469841\n",
      "Epoch: 1176/2000, Train Loss: 0.04594562575221062, Validation Loss: 0.08751264959573746\n",
      "Epoch: 1177/2000, Train Loss: 0.0459061935544014, Validation Loss: 0.0874403715133667\n",
      "Epoch: 1178/2000, Train Loss: 0.04586618393659592, Validation Loss: 0.08748707175254822\n",
      "Epoch: 1179/2000, Train Loss: 0.045825663954019547, Validation Loss: 0.0874289870262146\n",
      "Epoch: 1180/2000, Train Loss: 0.04578491672873497, Validation Loss: 0.08745196461677551\n",
      "Epoch: 1181/2000, Train Loss: 0.045744214206933975, Validation Loss: 0.08742455393075943\n",
      "Epoch: 1182/2000, Train Loss: 0.045703716576099396, Validation Loss: 0.08741733431816101\n",
      "Epoch: 1183/2000, Train Loss: 0.045663464814424515, Validation Loss: 0.08742111921310425\n",
      "Epoch: 1184/2000, Train Loss: 0.04562339186668396, Validation Loss: 0.08738754689693451\n",
      "Epoch: 1185/2000, Train Loss: 0.045583415776491165, Validation Loss: 0.08741333335638046\n",
      "Epoch: 1186/2000, Train Loss: 0.045543476939201355, Validation Loss: 0.08736056834459305\n",
      "Epoch: 1187/2000, Train Loss: 0.04550352692604065, Validation Loss: 0.0874011218547821\n",
      "Epoch: 1188/2000, Train Loss: 0.04546358063817024, Validation Loss: 0.08733399957418442\n",
      "Epoch: 1189/2000, Train Loss: 0.045423686504364014, Validation Loss: 0.08738761395215988\n",
      "Epoch: 1190/2000, Train Loss: 0.04538394510746002, Validation Loss: 0.08730410784482956\n",
      "Epoch: 1191/2000, Train Loss: 0.04534444212913513, Validation Loss: 0.08737904578447342\n",
      "Epoch: 1192/2000, Train Loss: 0.045305460691452026, Validation Loss: 0.08726947754621506\n",
      "Epoch: 1193/2000, Train Loss: 0.04526720568537712, Validation Loss: 0.08738330751657486\n",
      "Epoch: 1194/2000, Train Loss: 0.045230600982904434, Validation Loss: 0.08722627907991409\n",
      "Epoch: 1195/2000, Train Loss: 0.045195892453193665, Validation Loss: 0.08741259574890137\n",
      "Epoch: 1196/2000, Train Loss: 0.04516526311635971, Validation Loss: 0.08718035370111465\n",
      "Epoch: 1197/2000, Train Loss: 0.045138150453567505, Validation Loss: 0.08747781813144684\n",
      "Epoch: 1198/2000, Train Loss: 0.045117832720279694, Validation Loss: 0.08714333176612854\n",
      "Epoch: 1199/2000, Train Loss: 0.045100145041942596, Validation Loss: 0.08756238222122192\n",
      "Epoch: 1200/2000, Train Loss: 0.04508553817868233, Validation Loss: 0.08712218701839447\n",
      "Epoch: 1201/2000, Train Loss: 0.045066915452480316, Validation Loss: 0.08761025965213776\n",
      "Epoch: 1202/2000, Train Loss: 0.04504098370671272, Validation Loss: 0.08710043877363205\n",
      "Epoch: 1203/2000, Train Loss: 0.04500391706824303, Validation Loss: 0.0875672847032547\n",
      "Epoch: 1204/2000, Train Loss: 0.04495280981063843, Validation Loss: 0.08707129210233688\n",
      "Epoch: 1205/2000, Train Loss: 0.04489203169941902, Validation Loss: 0.0874362587928772\n",
      "Epoch: 1206/2000, Train Loss: 0.04482379928231239, Validation Loss: 0.08705392479896545\n",
      "Epoch: 1207/2000, Train Loss: 0.044755835086107254, Validation Loss: 0.08727481216192245\n",
      "Epoch: 1208/2000, Train Loss: 0.04469194635748863, Validation Loss: 0.08706976473331451\n",
      "Epoch: 1209/2000, Train Loss: 0.0446358323097229, Validation Loss: 0.08714032173156738\n",
      "Epoch: 1210/2000, Train Loss: 0.04458809643983841, Validation Loss: 0.08711642771959305\n",
      "Epoch: 1211/2000, Train Loss: 0.04454787075519562, Validation Loss: 0.08705129474401474\n",
      "Epoch: 1212/2000, Train Loss: 0.044513311237096786, Validation Loss: 0.08717235922813416\n",
      "Epoch: 1213/2000, Train Loss: 0.04448242485523224, Validation Loss: 0.08699574321508408\n",
      "Epoch: 1214/2000, Train Loss: 0.044453613460063934, Validation Loss: 0.08722089976072311\n",
      "Epoch: 1215/2000, Train Loss: 0.04442558065056801, Validation Loss: 0.08695923537015915\n",
      "Epoch: 1216/2000, Train Loss: 0.044397879391908646, Validation Loss: 0.0872592180967331\n",
      "Epoch: 1217/2000, Train Loss: 0.04436967894434929, Validation Loss: 0.08693384379148483\n",
      "Epoch: 1218/2000, Train Loss: 0.04434139281511307, Validation Loss: 0.08728838711977005\n",
      "Epoch: 1219/2000, Train Loss: 0.04431192949414253, Validation Loss: 0.0869140625\n",
      "Epoch: 1220/2000, Train Loss: 0.04428233578801155, Validation Loss: 0.08730758726596832\n",
      "Epoch: 1221/2000, Train Loss: 0.04425076022744179, Validation Loss: 0.08689709007740021\n",
      "Epoch: 1222/2000, Train Loss: 0.044218920171260834, Validation Loss: 0.08731429278850555\n",
      "Epoch: 1223/2000, Train Loss: 0.04418417438864708, Validation Loss: 0.08688090741634369\n",
      "Epoch: 1224/2000, Train Loss: 0.04414905607700348, Validation Loss: 0.08730505406856537\n",
      "Epoch: 1225/2000, Train Loss: 0.04411042481660843, Validation Loss: 0.08686328679323196\n",
      "Epoch: 1226/2000, Train Loss: 0.0440714992582798, Validation Loss: 0.0872785896062851\n",
      "Epoch: 1227/2000, Train Loss: 0.04402903839945793, Validation Loss: 0.08684457093477249\n",
      "Epoch: 1228/2000, Train Loss: 0.04398660734295845, Validation Loss: 0.08723786473274231\n",
      "Epoch: 1229/2000, Train Loss: 0.04394131526350975, Validation Loss: 0.08682636171579361\n",
      "Epoch: 1230/2000, Train Loss: 0.04389661177992821, Validation Loss: 0.08718844503164291\n",
      "Epoch: 1231/2000, Train Loss: 0.04385014623403549, Validation Loss: 0.08680970966815948\n",
      "Epoch: 1232/2000, Train Loss: 0.043804820626974106, Validation Loss: 0.0871371179819107\n",
      "Epoch: 1233/2000, Train Loss: 0.0437588095664978, Validation Loss: 0.08679544180631638\n",
      "Epoch: 1234/2000, Train Loss: 0.04371429979801178, Validation Loss: 0.08709022402763367\n",
      "Epoch: 1235/2000, Train Loss: 0.04366987198591232, Validation Loss: 0.08678290247917175\n",
      "Epoch: 1236/2000, Train Loss: 0.0436270609498024, Validation Loss: 0.0870516300201416\n",
      "Epoch: 1237/2000, Train Loss: 0.043584756553173065, Validation Loss: 0.08676997572183609\n",
      "Epoch: 1238/2000, Train Loss: 0.04354405030608177, Validation Loss: 0.08702369034290314\n",
      "Epoch: 1239/2000, Train Loss: 0.04350404813885689, Validation Loss: 0.08675510436296463\n",
      "Epoch: 1240/2000, Train Loss: 0.04346567764878273, Validation Loss: 0.08700882643461227\n",
      "Epoch: 1241/2000, Train Loss: 0.043428197503089905, Validation Loss: 0.08673754334449768\n",
      "Epoch: 1242/2000, Train Loss: 0.04339262470602989, Validation Loss: 0.08700992912054062\n",
      "Epoch: 1243/2000, Train Loss: 0.043358348309993744, Validation Loss: 0.08671725541353226\n",
      "Epoch: 1244/2000, Train Loss: 0.04332678020000458, Validation Loss: 0.08703204989433289\n",
      "Epoch: 1245/2000, Train Loss: 0.04329735040664673, Validation Loss: 0.08669654279947281\n",
      "Epoch: 1246/2000, Train Loss: 0.043272390961647034, Validation Loss: 0.08708501607179642\n",
      "Epoch: 1247/2000, Train Loss: 0.0432511605322361, Validation Loss: 0.08668173849582672\n",
      "Epoch: 1248/2000, Train Loss: 0.043237850069999695, Validation Loss: 0.08718468993902206\n",
      "Epoch: 1249/2000, Train Loss: 0.04323077201843262, Validation Loss: 0.08668524026870728\n",
      "Epoch: 1250/2000, Train Loss: 0.04323768988251686, Validation Loss: 0.08735288679599762\n",
      "Epoch: 1251/2000, Train Loss: 0.043253254145383835, Validation Loss: 0.08672752231359482\n",
      "Epoch: 1252/2000, Train Loss: 0.04329153150320053, Validation Loss: 0.08760710060596466\n",
      "Epoch: 1253/2000, Train Loss: 0.04333551973104477, Validation Loss: 0.08682670444250107\n",
      "Epoch: 1254/2000, Train Loss: 0.04340919107198715, Validation Loss: 0.08792155981063843\n",
      "Epoch: 1255/2000, Train Loss: 0.04346686601638794, Validation Loss: 0.08695820719003677\n",
      "Epoch: 1256/2000, Train Loss: 0.043546147644519806, Validation Loss: 0.08815886080265045\n",
      "Epoch: 1257/2000, Train Loss: 0.04355601966381073, Validation Loss: 0.08700548112392426\n",
      "Epoch: 1258/2000, Train Loss: 0.04355372115969658, Validation Loss: 0.08807049691677094\n",
      "Epoch: 1259/2000, Train Loss: 0.043429452925920486, Validation Loss: 0.0868447944521904\n",
      "Epoch: 1260/2000, Train Loss: 0.04327020421624184, Validation Loss: 0.08755304664373398\n",
      "Epoch: 1261/2000, Train Loss: 0.043031610548496246, Validation Loss: 0.0866154134273529\n",
      "Epoch: 1262/2000, Train Loss: 0.04281039535999298, Validation Loss: 0.08693082630634308\n",
      "Epoch: 1263/2000, Train Loss: 0.042631398886442184, Validation Loss: 0.0866474136710167\n",
      "Epoch: 1264/2000, Train Loss: 0.042532242834568024, Validation Loss: 0.08660653978586197\n",
      "Epoch: 1265/2000, Train Loss: 0.04250940680503845, Validation Loss: 0.08695606887340546\n",
      "Epoch: 1266/2000, Train Loss: 0.042538080364465714, Validation Loss: 0.08657284080982208\n",
      "Epoch: 1267/2000, Train Loss: 0.042587727308273315, Validation Loss: 0.08725392818450928\n",
      "Epoch: 1268/2000, Train Loss: 0.04262262582778931, Validation Loss: 0.0866021141409874\n",
      "Epoch: 1269/2000, Train Loss: 0.04263525456190109, Validation Loss: 0.08731935918331146\n",
      "Epoch: 1270/2000, Train Loss: 0.04259814694523811, Validation Loss: 0.08656647801399231\n",
      "Epoch: 1271/2000, Train Loss: 0.042533330619335175, Validation Loss: 0.08713077753782272\n",
      "Epoch: 1272/2000, Train Loss: 0.04243231192231178, Validation Loss: 0.08650343120098114\n",
      "Epoch: 1273/2000, Train Loss: 0.042326997965574265, Validation Loss: 0.08683528006076813\n",
      "Epoch: 1274/2000, Train Loss: 0.04222571849822998, Validation Loss: 0.08651895821094513\n",
      "Epoch: 1275/2000, Train Loss: 0.04214596375823021, Validation Loss: 0.08660563826560974\n",
      "Epoch: 1276/2000, Train Loss: 0.04209137335419655, Validation Loss: 0.08663921058177948\n",
      "Epoch: 1277/2000, Train Loss: 0.04205985739827156, Validation Loss: 0.0864967480301857\n",
      "Epoch: 1278/2000, Train Loss: 0.042044177651405334, Validation Loss: 0.08678761124610901\n",
      "Epoch: 1279/2000, Train Loss: 0.0420350506901741, Validation Loss: 0.08646045625209808\n",
      "Epoch: 1280/2000, Train Loss: 0.04202576354146004, Validation Loss: 0.08687832951545715\n",
      "Epoch: 1281/2000, Train Loss: 0.04200838506221771, Validation Loss: 0.08644061535596848\n",
      "Epoch: 1282/2000, Train Loss: 0.04198359325528145, Validation Loss: 0.08687933534383774\n",
      "Epoch: 1283/2000, Train Loss: 0.04194633662700653, Validation Loss: 0.08641878515481949\n",
      "Epoch: 1284/2000, Train Loss: 0.04190271347761154, Validation Loss: 0.08680817484855652\n",
      "Epoch: 1285/2000, Train Loss: 0.04185052588582039, Validation Loss: 0.08640382438898087\n",
      "Epoch: 1286/2000, Train Loss: 0.04179658368229866, Validation Loss: 0.08670254796743393\n",
      "Epoch: 1287/2000, Train Loss: 0.04174088314175606, Validation Loss: 0.08640748262405396\n",
      "Epoch: 1288/2000, Train Loss: 0.0416877456009388, Validation Loss: 0.08659645169973373\n",
      "Epoch: 1289/2000, Train Loss: 0.04163765534758568, Validation Loss: 0.08643089979887009\n",
      "Epoch: 1290/2000, Train Loss: 0.04159192368388176, Validation Loss: 0.0865086168050766\n",
      "Epoch: 1291/2000, Train Loss: 0.041550323367118835, Validation Loss: 0.08646506816148758\n",
      "Epoch: 1292/2000, Train Loss: 0.04151236638426781, Validation Loss: 0.08644244074821472\n",
      "Epoch: 1293/2000, Train Loss: 0.04147724434733391, Validation Loss: 0.08649939298629761\n",
      "Epoch: 1294/2000, Train Loss: 0.04144413769245148, Validation Loss: 0.08639378100633621\n",
      "Epoch: 1295/2000, Train Loss: 0.0414123609662056, Validation Loss: 0.08652902394533157\n",
      "Epoch: 1296/2000, Train Loss: 0.0413813441991806, Validation Loss: 0.08635812997817993\n",
      "Epoch: 1297/2000, Train Loss: 0.04135090485215187, Validation Loss: 0.08655509352684021\n",
      "Epoch: 1298/2000, Train Loss: 0.04132063314318657, Validation Loss: 0.08633162826299667\n",
      "Epoch: 1299/2000, Train Loss: 0.04129093885421753, Validation Loss: 0.08658108860254288\n",
      "Epoch: 1300/2000, Train Loss: 0.04126136004924774, Validation Loss: 0.0863104984164238\n",
      "Epoch: 1301/2000, Train Loss: 0.041232943534851074, Validation Loss: 0.08661144971847534\n",
      "Epoch: 1302/2000, Train Loss: 0.04120497778058052, Validation Loss: 0.08629196882247925\n",
      "Epoch: 1303/2000, Train Loss: 0.04117930680513382, Validation Loss: 0.08665252476930618\n",
      "Epoch: 1304/2000, Train Loss: 0.04115474224090576, Validation Loss: 0.08627563714981079\n",
      "Epoch: 1305/2000, Train Loss: 0.04113434627652168, Validation Loss: 0.08671359717845917\n",
      "Epoch: 1306/2000, Train Loss: 0.041116081178188324, Validation Loss: 0.08626490831375122\n",
      "Epoch: 1307/2000, Train Loss: 0.04110512509942055, Validation Loss: 0.08680874109268188\n",
      "Epoch: 1308/2000, Train Loss: 0.041097674518823624, Validation Loss: 0.08626940101385117\n",
      "Epoch: 1309/2000, Train Loss: 0.04110276326537132, Validation Loss: 0.08695755898952484\n",
      "Epoch: 1310/2000, Train Loss: 0.04111245274543762, Validation Loss: 0.0863054022192955\n",
      "Epoch: 1311/2000, Train Loss: 0.041142549365758896, Validation Loss: 0.08717909455299377\n",
      "Epoch: 1312/2000, Train Loss: 0.041175343096256256, Validation Loss: 0.08639021962881088\n",
      "Epoch: 1313/2000, Train Loss: 0.041237615048885345, Validation Loss: 0.08747094869613647\n",
      "Epoch: 1314/2000, Train Loss: 0.041289955377578735, Validation Loss: 0.08652115613222122\n",
      "Epoch: 1315/2000, Train Loss: 0.04137463495135307, Validation Loss: 0.08776261657476425\n",
      "Epoch: 1316/2000, Train Loss: 0.04141338914632797, Validation Loss: 0.08663228154182434\n",
      "Epoch: 1317/2000, Train Loss: 0.04146689921617508, Validation Loss: 0.08786632865667343\n",
      "Epoch: 1318/2000, Train Loss: 0.041417159140110016, Validation Loss: 0.08658646792173386\n",
      "Epoch: 1319/2000, Train Loss: 0.041346631944179535, Validation Loss: 0.08756440132856369\n",
      "Epoch: 1320/2000, Train Loss: 0.04115457087755203, Validation Loss: 0.08634834736585617\n",
      "Epoch: 1321/2000, Train Loss: 0.04094434529542923, Validation Loss: 0.08692522346973419\n",
      "Epoch: 1322/2000, Train Loss: 0.04070299491286278, Validation Loss: 0.08618279546499252\n",
      "Epoch: 1323/2000, Train Loss: 0.04051174223423004, Validation Loss: 0.08637450635433197\n",
      "Epoch: 1324/2000, Train Loss: 0.040391601622104645, Validation Loss: 0.08634362369775772\n",
      "Epoch: 1325/2000, Train Loss: 0.040350962430238724, Validation Loss: 0.08617469668388367\n",
      "Epoch: 1326/2000, Train Loss: 0.040370237082242966, Validation Loss: 0.0866929218173027\n",
      "Epoch: 1327/2000, Train Loss: 0.0404171422123909, Validation Loss: 0.086190365254879\n",
      "Epoch: 1328/2000, Train Loss: 0.04046674072742462, Validation Loss: 0.08693845570087433\n",
      "Epoch: 1329/2000, Train Loss: 0.04048576205968857, Validation Loss: 0.08620882034301758\n",
      "Epoch: 1330/2000, Train Loss: 0.0404786579310894, Validation Loss: 0.08692721277475357\n",
      "Epoch: 1331/2000, Train Loss: 0.04042147099971771, Validation Loss: 0.08615811169147491\n",
      "Epoch: 1332/2000, Train Loss: 0.04034220799803734, Validation Loss: 0.08669336140155792\n",
      "Epoch: 1333/2000, Train Loss: 0.04023626446723938, Validation Loss: 0.08610646426677704\n",
      "Epoch: 1334/2000, Train Loss: 0.0401335135102272, Validation Loss: 0.08639966696500778\n",
      "Epoch: 1335/2000, Train Loss: 0.040041957050561905, Validation Loss: 0.08614862710237503\n",
      "Epoch: 1336/2000, Train Loss: 0.03997403010725975, Validation Loss: 0.08619411289691925\n",
      "Epoch: 1337/2000, Train Loss: 0.039930641651153564, Validation Loss: 0.08628440648317337\n",
      "Epoch: 1338/2000, Train Loss: 0.03990742936730385, Validation Loss: 0.08610352873802185\n",
      "Epoch: 1339/2000, Train Loss: 0.03989659622311592, Validation Loss: 0.08642949908971786\n",
      "Epoch: 1340/2000, Train Loss: 0.039889149367809296, Validation Loss: 0.08607348054647446\n",
      "Epoch: 1341/2000, Train Loss: 0.039879728108644485, Validation Loss: 0.08650817722082138\n",
      "Epoch: 1342/2000, Train Loss: 0.039861127734184265, Validation Loss: 0.0860554650425911\n",
      "Epoch: 1343/2000, Train Loss: 0.03983519971370697, Validation Loss: 0.08650011569261551\n",
      "Epoch: 1344/2000, Train Loss: 0.039797376841306686, Validation Loss: 0.08603796362876892\n",
      "Epoch: 1345/2000, Train Loss: 0.039753980934619904, Validation Loss: 0.08642706274986267\n",
      "Epoch: 1346/2000, Train Loss: 0.03970310091972351, Validation Loss: 0.08603022247552872\n",
      "Epoch: 1347/2000, Train Loss: 0.03965115174651146, Validation Loss: 0.08632455766201019\n",
      "Epoch: 1348/2000, Train Loss: 0.03959818184375763, Validation Loss: 0.0860409289598465\n",
      "Epoch: 1349/2000, Train Loss: 0.039548035711050034, Validation Loss: 0.0862232968211174\n",
      "Epoch: 1350/2000, Train Loss: 0.03950110450387001, Validation Loss: 0.08606929332017899\n",
      "Epoch: 1351/2000, Train Loss: 0.03945840150117874, Validation Loss: 0.08614004403352737\n",
      "Epoch: 1352/2000, Train Loss: 0.03941961005330086, Validation Loss: 0.08610635250806808\n",
      "Epoch: 1353/2000, Train Loss: 0.03938419371843338, Validation Loss: 0.08607767522335052\n",
      "Epoch: 1354/2000, Train Loss: 0.03935137391090393, Validation Loss: 0.08614268898963928\n",
      "Epoch: 1355/2000, Train Loss: 0.03932035714387894, Validation Loss: 0.08603282272815704\n",
      "Epoch: 1356/2000, Train Loss: 0.039290525019168854, Validation Loss: 0.08617475628852844\n",
      "Epoch: 1357/2000, Train Loss: 0.03926132246851921, Validation Loss: 0.08600165694952011\n",
      "Epoch: 1358/2000, Train Loss: 0.03923260420560837, Validation Loss: 0.08620360493659973\n",
      "Epoch: 1359/2000, Train Loss: 0.0392039529979229, Validation Loss: 0.08597982674837112\n",
      "Epoch: 1360/2000, Train Loss: 0.03917577117681503, Validation Loss: 0.08623146265745163\n",
      "Epoch: 1361/2000, Train Loss: 0.03914758563041687, Validation Loss: 0.0859626978635788\n",
      "Epoch: 1362/2000, Train Loss: 0.03912036493420601, Validation Loss: 0.08626174926757812\n",
      "Epoch: 1363/2000, Train Loss: 0.03909340873360634, Validation Loss: 0.08594708889722824\n",
      "Epoch: 1364/2000, Train Loss: 0.03906835988163948, Validation Loss: 0.08629989624023438\n",
      "Epoch: 1365/2000, Train Loss: 0.0390441007912159, Validation Loss: 0.08593229204416275\n",
      "Epoch: 1366/2000, Train Loss: 0.03902328386902809, Validation Loss: 0.08635422587394714\n",
      "Epoch: 1367/2000, Train Loss: 0.03900407999753952, Validation Loss: 0.08592115342617035\n",
      "Epoch: 1368/2000, Train Loss: 0.03899086266756058, Validation Loss: 0.0864376649260521\n",
      "Epoch: 1369/2000, Train Loss: 0.03898043930530548, Validation Loss: 0.08592189103364944\n",
      "Epoch: 1370/2000, Train Loss: 0.03898026794195175, Validation Loss: 0.08656829595565796\n",
      "Epoch: 1371/2000, Train Loss: 0.03898414969444275, Validation Loss: 0.08594824373722076\n",
      "Epoch: 1372/2000, Train Loss: 0.039005097001791, Validation Loss: 0.08676620572805405\n",
      "Epoch: 1373/2000, Train Loss: 0.03903000056743622, Validation Loss: 0.08601753413677216\n",
      "Epoch: 1374/2000, Train Loss: 0.03908136859536171, Validation Loss: 0.08704293519258499\n",
      "Epoch: 1375/2000, Train Loss: 0.03913046419620514, Validation Loss: 0.08614107966423035\n",
      "Epoch: 1376/2000, Train Loss: 0.039214473217725754, Validation Loss: 0.08737090229988098\n",
      "Epoch: 1377/2000, Train Loss: 0.039273083209991455, Validation Loss: 0.08629176020622253\n",
      "Epoch: 1378/2000, Train Loss: 0.039362408220767975, Validation Loss: 0.08762407302856445\n",
      "Epoch: 1379/2000, Train Loss: 0.03937521204352379, Validation Loss: 0.08635804057121277\n",
      "Epoch: 1380/2000, Train Loss: 0.039388563483953476, Validation Loss: 0.08756104111671448\n",
      "Epoch: 1381/2000, Train Loss: 0.039270706474781036, Validation Loss: 0.08620350807905197\n",
      "Epoch: 1382/2000, Train Loss: 0.039123035967350006, Validation Loss: 0.08703853189945221\n",
      "Epoch: 1383/2000, Train Loss: 0.03887491673231125, Validation Loss: 0.08593204617500305\n",
      "Epoch: 1384/2000, Train Loss: 0.03863651305437088, Validation Loss: 0.08634010702371597\n",
      "Epoch: 1385/2000, Train Loss: 0.03842576965689659, Validation Loss: 0.0859055146574974\n",
      "Epoch: 1386/2000, Train Loss: 0.03829650208353996, Validation Loss: 0.08592895418405533\n",
      "Epoch: 1387/2000, Train Loss: 0.03825308382511139, Validation Loss: 0.08621681481599808\n",
      "Epoch: 1388/2000, Train Loss: 0.038275886327028275, Validation Loss: 0.0858682170510292\n",
      "Epoch: 1389/2000, Train Loss: 0.03833267465233803, Validation Loss: 0.08657625317573547\n",
      "Epoch: 1390/2000, Train Loss: 0.03838479891419411, Validation Loss: 0.08591564744710922\n",
      "Epoch: 1391/2000, Train Loss: 0.0384189747273922, Validation Loss: 0.08671160787343979\n",
      "Epoch: 1392/2000, Train Loss: 0.03840257227420807, Validation Loss: 0.0858992487192154\n",
      "Epoch: 1393/2000, Train Loss: 0.03835561126470566, Validation Loss: 0.08655931800603867\n",
      "Epoch: 1394/2000, Train Loss: 0.03826257213950157, Validation Loss: 0.08583029359579086\n",
      "Epoch: 1395/2000, Train Loss: 0.03815857693552971, Validation Loss: 0.08624923974275589\n",
      "Epoch: 1396/2000, Train Loss: 0.03804952651262283, Validation Loss: 0.085825115442276\n",
      "Epoch: 1397/2000, Train Loss: 0.037959419190883636, Validation Loss: 0.08597869426012039\n",
      "Epoch: 1398/2000, Train Loss: 0.037894949316978455, Validation Loss: 0.08594222366809845\n",
      "Epoch: 1399/2000, Train Loss: 0.03785775229334831, Validation Loss: 0.08584167063236237\n",
      "Epoch: 1400/2000, Train Loss: 0.03784165158867836, Validation Loss: 0.0861140638589859\n",
      "Epoch: 1401/2000, Train Loss: 0.037836797535419464, Validation Loss: 0.0858006700873375\n",
      "Epoch: 1402/2000, Train Loss: 0.03783438727259636, Validation Loss: 0.08623512834310532\n",
      "Epoch: 1403/2000, Train Loss: 0.03782481327652931, Validation Loss: 0.08578670769929886\n",
      "Epoch: 1404/2000, Train Loss: 0.03780704364180565, Validation Loss: 0.08625373244285583\n",
      "Epoch: 1405/2000, Train Loss: 0.0377751924097538, Validation Loss: 0.08577117323875427\n",
      "Epoch: 1406/2000, Train Loss: 0.037735357880592346, Validation Loss: 0.0861840471625328\n",
      "Epoch: 1407/2000, Train Loss: 0.037685517221689224, Validation Loss: 0.0857636108994484\n",
      "Epoch: 1408/2000, Train Loss: 0.03763330727815628, Validation Loss: 0.08607170730829239\n",
      "Epoch: 1409/2000, Train Loss: 0.03757940232753754, Validation Loss: 0.08577962219715118\n",
      "Epoch: 1410/2000, Train Loss: 0.03752869740128517, Validation Loss: 0.0859595388174057\n",
      "Epoch: 1411/2000, Train Loss: 0.03748215362429619, Validation Loss: 0.08582056313753128\n",
      "Epoch: 1412/2000, Train Loss: 0.037441037595272064, Validation Loss: 0.08587100356817245\n",
      "Epoch: 1413/2000, Train Loss: 0.03740503266453743, Validation Loss: 0.08587426692247391\n",
      "Epoch: 1414/2000, Train Loss: 0.03737330064177513, Validation Loss: 0.08580999821424484\n",
      "Epoch: 1415/2000, Train Loss: 0.03734464943408966, Validation Loss: 0.08592559397220612\n",
      "Epoch: 1416/2000, Train Loss: 0.037317853420972824, Validation Loss: 0.08577033877372742\n",
      "Epoch: 1417/2000, Train Loss: 0.037291985005140305, Validation Loss: 0.08596614003181458\n",
      "Epoch: 1418/2000, Train Loss: 0.03726610913872719, Validation Loss: 0.08574577420949936\n",
      "Epoch: 1419/2000, Train Loss: 0.03724009916186333, Validation Loss: 0.08599566668272018\n",
      "Epoch: 1420/2000, Train Loss: 0.03721319139003754, Validation Loss: 0.08573178201913834\n",
      "Epoch: 1421/2000, Train Loss: 0.037186019122600555, Validation Loss: 0.08601678162813187\n",
      "Epoch: 1422/2000, Train Loss: 0.03715776652097702, Validation Loss: 0.08572346717119217\n",
      "Epoch: 1423/2000, Train Loss: 0.03712961822748184, Validation Loss: 0.0860319659113884\n",
      "Epoch: 1424/2000, Train Loss: 0.037100568413734436, Validation Loss: 0.08571625500917435\n",
      "Epoch: 1425/2000, Train Loss: 0.03707217797636986, Validation Loss: 0.08604458719491959\n",
      "Epoch: 1426/2000, Train Loss: 0.03704320266842842, Validation Loss: 0.08570776134729385\n",
      "Epoch: 1427/2000, Train Loss: 0.03701552748680115, Validation Loss: 0.08605927973985672\n",
      "Epoch: 1428/2000, Train Loss: 0.03698763996362686, Validation Loss: 0.08569753170013428\n",
      "Epoch: 1429/2000, Train Loss: 0.036961838603019714, Validation Loss: 0.08608201146125793\n",
      "Epoch: 1430/2000, Train Loss: 0.03693624958395958, Validation Loss: 0.08568723499774933\n",
      "Epoch: 1431/2000, Train Loss: 0.03691384196281433, Validation Loss: 0.08612063527107239\n",
      "Epoch: 1432/2000, Train Loss: 0.036892157047986984, Validation Loss: 0.08568057417869568\n",
      "Epoch: 1433/2000, Train Loss: 0.03687535971403122, Validation Loss: 0.08618448674678802\n",
      "Epoch: 1434/2000, Train Loss: 0.036859914660453796, Validation Loss: 0.08568231761455536\n",
      "Epoch: 1435/2000, Train Loss: 0.03685208037495613, Validation Loss: 0.08628340065479279\n",
      "Epoch: 1436/2000, Train Loss: 0.03684622794389725, Validation Loss: 0.08569861203432083\n",
      "Epoch: 1437/2000, Train Loss: 0.036852262914180756, Validation Loss: 0.0864286720752716\n",
      "Epoch: 1438/2000, Train Loss: 0.036860305815935135, Validation Loss: 0.08573934435844421\n",
      "Epoch: 1439/2000, Train Loss: 0.03688647970557213, Validation Loss: 0.08663099259138107\n",
      "Epoch: 1440/2000, Train Loss: 0.036912087351083755, Validation Loss: 0.08581596612930298\n",
      "Epoch: 1441/2000, Train Loss: 0.03696304187178612, Validation Loss: 0.08688714355230331\n",
      "Epoch: 1442/2000, Train Loss: 0.037003204226493835, Validation Loss: 0.0859275534749031\n",
      "Epoch: 1443/2000, Train Loss: 0.03707210347056389, Validation Loss: 0.08714782446622849\n",
      "Epoch: 1444/2000, Train Loss: 0.0371042899787426, Validation Loss: 0.08603175729513168\n",
      "Epoch: 1445/2000, Train Loss: 0.03715530037879944, Validation Loss: 0.08728066086769104\n",
      "Epoch: 1446/2000, Train Loss: 0.03712758794426918, Validation Loss: 0.08603133261203766\n",
      "Epoch: 1447/2000, Train Loss: 0.03709321469068527, Validation Loss: 0.08710737526416779\n",
      "Epoch: 1448/2000, Train Loss: 0.0369553305208683, Validation Loss: 0.08586619049310684\n",
      "Epoch: 1449/2000, Train Loss: 0.03679990395903587, Validation Loss: 0.08660277724266052\n",
      "Epoch: 1450/2000, Train Loss: 0.036588650196790695, Validation Loss: 0.08567921072244644\n",
      "Epoch: 1451/2000, Train Loss: 0.036398500204086304, Validation Loss: 0.08604046702384949\n",
      "Epoch: 1452/2000, Train Loss: 0.03624241054058075, Validation Loss: 0.08571324497461319\n",
      "Epoch: 1453/2000, Train Loss: 0.03614848852157593, Validation Loss: 0.08572085946798325\n",
      "Epoch: 1454/2000, Train Loss: 0.03611483424901962, Validation Loss: 0.08597838878631592\n",
      "Epoch: 1455/2000, Train Loss: 0.0361260324716568, Validation Loss: 0.0856541246175766\n",
      "Epoch: 1456/2000, Train Loss: 0.03616100177168846, Validation Loss: 0.08626939356327057\n",
      "Epoch: 1457/2000, Train Loss: 0.03619525209069252, Validation Loss: 0.08567880839109421\n",
      "Epoch: 1458/2000, Train Loss: 0.03622045740485191, Validation Loss: 0.08641263842582703\n",
      "Epoch: 1459/2000, Train Loss: 0.03621431067585945, Validation Loss: 0.08567949384450912\n",
      "Epoch: 1460/2000, Train Loss: 0.03618905693292618, Validation Loss: 0.086356021463871\n",
      "Epoch: 1461/2000, Train Loss: 0.03612874448299408, Validation Loss: 0.08564163744449615\n",
      "Epoch: 1462/2000, Train Loss: 0.03605645149946213, Validation Loss: 0.08615315705537796\n",
      "Epoch: 1463/2000, Train Loss: 0.035968612879514694, Validation Loss: 0.08561912178993225\n",
      "Epoch: 1464/2000, Train Loss: 0.035885248333215714, Validation Loss: 0.08591772615909576\n",
      "Epoch: 1465/2000, Train Loss: 0.03581026941537857, Validation Loss: 0.08566535264253616\n",
      "Epoch: 1466/2000, Train Loss: 0.0357515811920166, Validation Loss: 0.08574359118938446\n",
      "Epoch: 1467/2000, Train Loss: 0.03570965304970741, Validation Loss: 0.08577500283718109\n",
      "Epoch: 1468/2000, Train Loss: 0.035682253539562225, Validation Loss: 0.08565156906843185\n",
      "Epoch: 1469/2000, Train Loss: 0.03566492348909378, Validation Loss: 0.08589623868465424\n",
      "Epoch: 1470/2000, Train Loss: 0.03565246984362602, Validation Loss: 0.08561278134584427\n",
      "Epoch: 1471/2000, Train Loss: 0.03564111143350601, Validation Loss: 0.08598292618989944\n",
      "Epoch: 1472/2000, Train Loss: 0.0356263592839241, Validation Loss: 0.08559694141149521\n",
      "Epoch: 1473/2000, Train Loss: 0.03560832515358925, Validation Loss: 0.08601875603199005\n",
      "Epoch: 1474/2000, Train Loss: 0.03558339178562164, Validation Loss: 0.08559007942676544\n",
      "Epoch: 1475/2000, Train Loss: 0.03555483743548393, Validation Loss: 0.08600809425115585\n",
      "Epoch: 1476/2000, Train Loss: 0.035519789904356, Validation Loss: 0.08558820188045502\n",
      "Epoch: 1477/2000, Train Loss: 0.035482678562402725, Validation Loss: 0.08596372604370117\n",
      "Epoch: 1478/2000, Train Loss: 0.035441599786281586, Validation Loss: 0.0855913907289505\n",
      "Epoch: 1479/2000, Train Loss: 0.035400453954935074, Validation Loss: 0.08590182662010193\n",
      "Epoch: 1480/2000, Train Loss: 0.0353582464158535, Validation Loss: 0.0856008306145668\n",
      "Epoch: 1481/2000, Train Loss: 0.03531741723418236, Validation Loss: 0.08583729714155197\n",
      "Epoch: 1482/2000, Train Loss: 0.03527745231986046, Validation Loss: 0.08561597764492035\n",
      "Epoch: 1483/2000, Train Loss: 0.035239383578300476, Validation Loss: 0.08577989041805267\n",
      "Epoch: 1484/2000, Train Loss: 0.03520284593105316, Validation Loss: 0.08563483506441116\n",
      "Epoch: 1485/2000, Train Loss: 0.035167984664440155, Validation Loss: 0.0857343077659607\n",
      "Epoch: 1486/2000, Train Loss: 0.03513446822762489, Validation Loss: 0.0856555625796318\n",
      "Epoch: 1487/2000, Train Loss: 0.03510211408138275, Validation Loss: 0.0857010930776596\n",
      "Epoch: 1488/2000, Train Loss: 0.03507065027952194, Validation Loss: 0.08567607402801514\n",
      "Epoch: 1489/2000, Train Loss: 0.03503987193107605, Validation Loss: 0.08567721396684647\n",
      "Epoch: 1490/2000, Train Loss: 0.03500958904623985, Validation Loss: 0.0856943354010582\n",
      "Epoch: 1491/2000, Train Loss: 0.0349796786904335, Validation Loss: 0.08565832674503326\n",
      "Epoch: 1492/2000, Train Loss: 0.034950047731399536, Validation Loss: 0.08570992946624756\n",
      "Epoch: 1493/2000, Train Loss: 0.034920647740364075, Validation Loss: 0.08564122021198273\n",
      "Epoch: 1494/2000, Train Loss: 0.03489149361848831, Validation Loss: 0.08572458475828171\n",
      "Epoch: 1495/2000, Train Loss: 0.03486260026693344, Validation Loss: 0.08562401682138443\n",
      "Epoch: 1496/2000, Train Loss: 0.034834057092666626, Validation Loss: 0.08574175089597702\n",
      "Epoch: 1497/2000, Train Loss: 0.03480594605207443, Validation Loss: 0.08560596406459808\n",
      "Epoch: 1498/2000, Train Loss: 0.03477851301431656, Validation Loss: 0.08576677739620209\n",
      "Epoch: 1499/2000, Train Loss: 0.034751906991004944, Validation Loss: 0.08558721840381622\n",
      "Epoch: 1500/2000, Train Loss: 0.034726690500974655, Validation Loss: 0.08580734580755234\n",
      "Epoch: 1501/2000, Train Loss: 0.03470313921570778, Validation Loss: 0.08556854724884033\n",
      "Epoch: 1502/2000, Train Loss: 0.03468251973390579, Validation Loss: 0.08587486296892166\n",
      "Epoch: 1503/2000, Train Loss: 0.03466536104679108, Validation Loss: 0.08555306494235992\n",
      "Epoch: 1504/2000, Train Loss: 0.03465457633137703, Validation Loss: 0.08599025011062622\n",
      "Epoch: 1505/2000, Train Loss: 0.03465103730559349, Validation Loss: 0.0855531394481659\n",
      "Epoch: 1506/2000, Train Loss: 0.034661732614040375, Validation Loss: 0.08619542419910431\n",
      "Epoch: 1507/2000, Train Loss: 0.03468743711709976, Validation Loss: 0.08560414612293243\n",
      "Epoch: 1508/2000, Train Loss: 0.034745216369628906, Validation Loss: 0.08657149970531464\n",
      "Epoch: 1509/2000, Train Loss: 0.03483191877603531, Validation Loss: 0.08578924834728241\n",
      "Epoch: 1510/2000, Train Loss: 0.03498870134353638, Validation Loss: 0.08725335448980331\n",
      "Epoch: 1511/2000, Train Loss: 0.03518860787153244, Validation Loss: 0.08625484257936478\n",
      "Epoch: 1512/2000, Train Loss: 0.035523321479558945, Validation Loss: 0.08835666626691818\n",
      "Epoch: 1513/2000, Train Loss: 0.03586333245038986, Validation Loss: 0.0870678722858429\n",
      "Epoch: 1514/2000, Train Loss: 0.036375366151332855, Validation Loss: 0.089536152780056\n",
      "Epoch: 1515/2000, Train Loss: 0.03663213178515434, Validation Loss: 0.08759339898824692\n",
      "Epoch: 1516/2000, Train Loss: 0.03686775267124176, Validation Loss: 0.08926431834697723\n",
      "Epoch: 1517/2000, Train Loss: 0.03637749329209328, Validation Loss: 0.08653007447719574\n",
      "Epoch: 1518/2000, Train Loss: 0.035642657428979874, Validation Loss: 0.0868065282702446\n",
      "Epoch: 1519/2000, Train Loss: 0.034672629088163376, Validation Loss: 0.08561652898788452\n",
      "Epoch: 1520/2000, Train Loss: 0.03415263444185257, Validation Loss: 0.08559944480657578\n",
      "Epoch: 1521/2000, Train Loss: 0.0342588908970356, Validation Loss: 0.0870598554611206\n",
      "Epoch: 1522/2000, Train Loss: 0.03471800684928894, Validation Loss: 0.08625443279743195\n",
      "Epoch: 1523/2000, Train Loss: 0.035125065594911575, Validation Loss: 0.0877440944314003\n",
      "Epoch: 1524/2000, Train Loss: 0.03507321700453758, Validation Loss: 0.08599782735109329\n",
      "Epoch: 1525/2000, Train Loss: 0.03471512719988823, Validation Loss: 0.08640297502279282\n",
      "Epoch: 1526/2000, Train Loss: 0.034204546362161636, Validation Loss: 0.0856630727648735\n",
      "Epoch: 1527/2000, Train Loss: 0.03392894193530083, Validation Loss: 0.08557607233524323\n",
      "Epoch: 1528/2000, Train Loss: 0.033990465104579926, Validation Loss: 0.08654335141181946\n",
      "Epoch: 1529/2000, Train Loss: 0.03422585874795914, Validation Loss: 0.08579898625612259\n",
      "Epoch: 1530/2000, Train Loss: 0.03439963981509209, Validation Loss: 0.086822509765625\n",
      "Epoch: 1531/2000, Train Loss: 0.034321870654821396, Validation Loss: 0.08566664904356003\n",
      "Epoch: 1532/2000, Train Loss: 0.03408760949969292, Validation Loss: 0.08600720763206482\n",
      "Epoch: 1533/2000, Train Loss: 0.033829882740974426, Validation Loss: 0.08571038395166397\n",
      "Epoch: 1534/2000, Train Loss: 0.033720605075359344, Validation Loss: 0.08557509630918503\n",
      "Epoch: 1535/2000, Train Loss: 0.0337752029299736, Validation Loss: 0.08628186583518982\n",
      "Epoch: 1536/2000, Train Loss: 0.03388215973973274, Validation Loss: 0.08564044535160065\n",
      "Epoch: 1537/2000, Train Loss: 0.0339253731071949, Validation Loss: 0.08632112294435501\n",
      "Epoch: 1538/2000, Train Loss: 0.03384062275290489, Validation Loss: 0.08558561652898788\n",
      "Epoch: 1539/2000, Train Loss: 0.03369511291384697, Validation Loss: 0.08581003546714783\n",
      "Epoch: 1540/2000, Train Loss: 0.033569108694791794, Validation Loss: 0.08572717010974884\n",
      "Epoch: 1541/2000, Train Loss: 0.03352569043636322, Validation Loss: 0.0855536088347435\n",
      "Epoch: 1542/2000, Train Loss: 0.033552564680576324, Validation Loss: 0.08606274425983429\n",
      "Epoch: 1543/2000, Train Loss: 0.033588264137506485, Validation Loss: 0.08556167781352997\n",
      "Epoch: 1544/2000, Train Loss: 0.03358276188373566, Validation Loss: 0.08604447543621063\n",
      "Epoch: 1545/2000, Train Loss: 0.03351784497499466, Validation Loss: 0.08557285368442535\n",
      "Epoch: 1546/2000, Train Loss: 0.03342990204691887, Validation Loss: 0.08573746681213379\n",
      "Epoch: 1547/2000, Train Loss: 0.03335823118686676, Validation Loss: 0.08571002632379532\n",
      "Epoch: 1548/2000, Train Loss: 0.03332706168293953, Validation Loss: 0.08556382358074188\n",
      "Epoch: 1549/2000, Train Loss: 0.03332722559571266, Validation Loss: 0.0859132632613182\n",
      "Epoch: 1550/2000, Train Loss: 0.033329978585243225, Validation Loss: 0.0855521559715271\n",
      "Epoch: 1551/2000, Train Loss: 0.03331286832690239, Validation Loss: 0.08590742200613022\n",
      "Epoch: 1552/2000, Train Loss: 0.03326834365725517, Validation Loss: 0.08557524532079697\n",
      "Epoch: 1553/2000, Train Loss: 0.0332115963101387, Validation Loss: 0.08571918308734894\n",
      "Epoch: 1554/2000, Train Loss: 0.033160433173179626, Validation Loss: 0.08566731214523315\n",
      "Epoch: 1555/2000, Train Loss: 0.033127158880233765, Validation Loss: 0.08558446168899536\n",
      "Epoch: 1556/2000, Train Loss: 0.03310998156666756, Validation Loss: 0.08580470830202103\n",
      "Epoch: 1557/2000, Train Loss: 0.033097557723522186, Validation Loss: 0.08556657284498215\n",
      "Epoch: 1558/2000, Train Loss: 0.03307873010635376, Validation Loss: 0.08584022521972656\n",
      "Epoch: 1559/2000, Train Loss: 0.033047646284103394, Validation Loss: 0.08559093624353409\n",
      "Epoch: 1560/2000, Train Loss: 0.03300802782177925, Validation Loss: 0.08574415743350983\n",
      "Epoch: 1561/2000, Train Loss: 0.03296685963869095, Validation Loss: 0.0856456533074379\n",
      "Epoch: 1562/2000, Train Loss: 0.032931435853242874, Validation Loss: 0.08563774824142456\n",
      "Epoch: 1563/2000, Train Loss: 0.03290403261780739, Validation Loss: 0.08573345094919205\n",
      "Epoch: 1564/2000, Train Loss: 0.032882124185562134, Validation Loss: 0.08559650927782059\n",
      "Epoch: 1565/2000, Train Loss: 0.03286094218492508, Validation Loss: 0.08578303456306458\n",
      "Epoch: 1566/2000, Train Loss: 0.0328361876308918, Validation Loss: 0.08559413999319077\n",
      "Epoch: 1567/2000, Train Loss: 0.032806649804115295, Validation Loss: 0.08574732393026352\n",
      "Epoch: 1568/2000, Train Loss: 0.032773490995168686, Validation Loss: 0.0856141448020935\n",
      "Epoch: 1569/2000, Train Loss: 0.03273986279964447, Validation Loss: 0.08567579835653305\n",
      "Epoch: 1570/2000, Train Loss: 0.03270827978849411, Validation Loss: 0.08566609770059586\n",
      "Epoch: 1571/2000, Train Loss: 0.03267986327409744, Validation Loss: 0.0856286808848381\n",
      "Epoch: 1572/2000, Train Loss: 0.03265403211116791, Validation Loss: 0.0857200175523758\n",
      "Epoch: 1573/2000, Train Loss: 0.03262912482023239, Validation Loss: 0.08560959994792938\n",
      "Epoch: 1574/2000, Train Loss: 0.03260346129536629, Validation Loss: 0.0857335552573204\n",
      "Epoch: 1575/2000, Train Loss: 0.03257601335644722, Validation Loss: 0.08560976386070251\n",
      "Epoch: 1576/2000, Train Loss: 0.032546911388635635, Validation Loss: 0.08570999652147293\n",
      "Epoch: 1577/2000, Train Loss: 0.032516829669475555, Validation Loss: 0.08563297986984253\n",
      "Epoch: 1578/2000, Train Loss: 0.032486818730831146, Validation Loss: 0.08567588776350021\n",
      "Epoch: 1579/2000, Train Loss: 0.032457660883665085, Validation Loss: 0.08566822856664658\n",
      "Epoch: 1580/2000, Train Loss: 0.03242965042591095, Validation Loss: 0.08564557880163193\n",
      "Epoch: 1581/2000, Train Loss: 0.03240257129073143, Validation Loss: 0.08569583296775818\n",
      "Epoch: 1582/2000, Train Loss: 0.03237592428922653, Validation Loss: 0.08562862128019333\n",
      "Epoch: 1583/2000, Train Loss: 0.03234916925430298, Validation Loss: 0.08570946007966995\n",
      "Epoch: 1584/2000, Train Loss: 0.03232192248106003, Validation Loss: 0.08563082665205002\n",
      "Epoch: 1585/2000, Train Loss: 0.03229409083724022, Validation Loss: 0.08570796996355057\n",
      "Epoch: 1586/2000, Train Loss: 0.03226577863097191, Validation Loss: 0.08564431965351105\n",
      "Epoch: 1587/2000, Train Loss: 0.032237257808446884, Validation Loss: 0.08569198101758957\n",
      "Epoch: 1588/2000, Train Loss: 0.032208800315856934, Validation Loss: 0.0856611356139183\n",
      "Epoch: 1589/2000, Train Loss: 0.03218061849474907, Validation Loss: 0.08567293733358383\n",
      "Epoch: 1590/2000, Train Loss: 0.03215278685092926, Validation Loss: 0.08568110316991806\n",
      "Epoch: 1591/2000, Train Loss: 0.032125264406204224, Validation Loss: 0.08566185086965561\n",
      "Epoch: 1592/2000, Train Loss: 0.0320979468524456, Validation Loss: 0.08569872379302979\n",
      "Epoch: 1593/2000, Train Loss: 0.032070696353912354, Validation Loss: 0.08565694838762283\n",
      "Epoch: 1594/2000, Train Loss: 0.032043393701314926, Validation Loss: 0.08570587635040283\n",
      "Epoch: 1595/2000, Train Loss: 0.03201596811413765, Validation Loss: 0.08565589785575867\n",
      "Epoch: 1596/2000, Train Loss: 0.03198839724063873, Validation Loss: 0.08570496737957001\n",
      "Epoch: 1597/2000, Train Loss: 0.03196069970726967, Validation Loss: 0.08566177636384964\n",
      "Epoch: 1598/2000, Train Loss: 0.031932931393384933, Validation Loss: 0.08570165932178497\n",
      "Epoch: 1599/2000, Train Loss: 0.03190514072775841, Validation Loss: 0.08567248284816742\n",
      "Epoch: 1600/2000, Train Loss: 0.03187738358974457, Validation Loss: 0.08569582551717758\n",
      "Epoch: 1601/2000, Train Loss: 0.0318496897816658, Validation Loss: 0.08568250387907028\n",
      "Epoch: 1602/2000, Train Loss: 0.031822070479393005, Validation Loss: 0.08568870276212692\n",
      "Epoch: 1603/2000, Train Loss: 0.03179453685879707, Validation Loss: 0.0856919065117836\n",
      "Epoch: 1604/2000, Train Loss: 0.031767070293426514, Validation Loss: 0.08568456023931503\n",
      "Epoch: 1605/2000, Train Loss: 0.031739652156829834, Validation Loss: 0.08570126444101334\n",
      "Epoch: 1606/2000, Train Loss: 0.03171226754784584, Validation Loss: 0.08568332344293594\n",
      "Epoch: 1607/2000, Train Loss: 0.03168489784002304, Validation Loss: 0.08570797741413116\n",
      "Epoch: 1608/2000, Train Loss: 0.03165752813220024, Validation Loss: 0.08568288385868073\n",
      "Epoch: 1609/2000, Train Loss: 0.03163015469908714, Validation Loss: 0.08571183681488037\n",
      "Epoch: 1610/2000, Train Loss: 0.03160277009010315, Validation Loss: 0.0856841430068016\n",
      "Epoch: 1611/2000, Train Loss: 0.031575385481119156, Validation Loss: 0.08571504801511765\n",
      "Epoch: 1612/2000, Train Loss: 0.03154798969626427, Validation Loss: 0.08568738400936127\n",
      "Epoch: 1613/2000, Train Loss: 0.03152059391140938, Validation Loss: 0.08571757376194\n",
      "Epoch: 1614/2000, Train Loss: 0.03149319812655449, Validation Loss: 0.08569084107875824\n",
      "Epoch: 1615/2000, Train Loss: 0.0314658097922802, Validation Loss: 0.08571920543909073\n",
      "Epoch: 1616/2000, Train Loss: 0.0314384289085865, Validation Loss: 0.08569447696208954\n",
      "Epoch: 1617/2000, Train Loss: 0.031411062926054, Validation Loss: 0.0857214406132698\n",
      "Epoch: 1618/2000, Train Loss: 0.031383708119392395, Validation Loss: 0.08569879829883575\n",
      "Epoch: 1619/2000, Train Loss: 0.031356360763311386, Validation Loss: 0.08572431653738022\n",
      "Epoch: 1620/2000, Train Loss: 0.03132903575897217, Validation Loss: 0.08570253103971481\n",
      "Epoch: 1621/2000, Train Loss: 0.031301725655794144, Validation Loss: 0.08572706580162048\n",
      "Epoch: 1622/2000, Train Loss: 0.031274426728487015, Validation Loss: 0.08570531755685806\n",
      "Epoch: 1623/2000, Train Loss: 0.031247150152921677, Validation Loss: 0.08573058992624283\n",
      "Epoch: 1624/2000, Train Loss: 0.03121989034116268, Validation Loss: 0.08570805191993713\n",
      "Epoch: 1625/2000, Train Loss: 0.03119264915585518, Validation Loss: 0.08573555201292038\n",
      "Epoch: 1626/2000, Train Loss: 0.031165430322289467, Validation Loss: 0.08571020513772964\n",
      "Epoch: 1627/2000, Train Loss: 0.031138239428400993, Validation Loss: 0.08574122935533524\n",
      "Epoch: 1628/2000, Train Loss: 0.03111107461154461, Validation Loss: 0.08571073412895203\n",
      "Epoch: 1629/2000, Train Loss: 0.031083950772881508, Validation Loss: 0.08574806153774261\n",
      "Epoch: 1630/2000, Train Loss: 0.031056871637701988, Validation Loss: 0.0857100710272789\n",
      "Epoch: 1631/2000, Train Loss: 0.031029853969812393, Validation Loss: 0.0857575535774231\n",
      "Epoch: 1632/2000, Train Loss: 0.031002918258309364, Validation Loss: 0.08570800721645355\n",
      "Epoch: 1633/2000, Train Loss: 0.030976099893450737, Validation Loss: 0.08577046543359756\n",
      "Epoch: 1634/2000, Train Loss: 0.030949445441365242, Validation Loss: 0.0857030600309372\n",
      "Epoch: 1635/2000, Train Loss: 0.03092302568256855, Validation Loss: 0.08578842133283615\n",
      "Epoch: 1636/2000, Train Loss: 0.030896946787834167, Validation Loss: 0.08569446206092834\n",
      "Epoch: 1637/2000, Train Loss: 0.03087138757109642, Validation Loss: 0.0858156681060791\n",
      "Epoch: 1638/2000, Train Loss: 0.030846571549773216, Validation Loss: 0.08568176627159119\n",
      "Epoch: 1639/2000, Train Loss: 0.03082297556102276, Validation Loss: 0.0858595222234726\n",
      "Epoch: 1640/2000, Train Loss: 0.030801108106970787, Validation Loss: 0.08566491305828094\n",
      "Epoch: 1641/2000, Train Loss: 0.030782168731093407, Validation Loss: 0.08593422174453735\n",
      "Epoch: 1642/2000, Train Loss: 0.030767375603318214, Validation Loss: 0.08564858883619308\n",
      "Epoch: 1643/2000, Train Loss: 0.03075995296239853, Validation Loss: 0.08607122302055359\n",
      "Epoch: 1644/2000, Train Loss: 0.030762676149606705, Validation Loss: 0.08565328270196915\n",
      "Epoch: 1645/2000, Train Loss: 0.030784498900175095, Validation Loss: 0.08634070307016373\n",
      "Epoch: 1646/2000, Train Loss: 0.03083108738064766, Validation Loss: 0.08574382215738297\n",
      "Epoch: 1647/2000, Train Loss: 0.03092784248292446, Validation Loss: 0.08689874410629272\n",
      "Epoch: 1648/2000, Train Loss: 0.03108164109289646, Validation Loss: 0.08609864115715027\n",
      "Epoch: 1649/2000, Train Loss: 0.03136371076107025, Validation Loss: 0.08805685490369797\n",
      "Epoch: 1650/2000, Train Loss: 0.03175073862075806, Validation Loss: 0.08709161728620529\n",
      "Epoch: 1651/2000, Train Loss: 0.03242355212569237, Validation Loss: 0.09016270935535431\n",
      "Epoch: 1652/2000, Train Loss: 0.03313712030649185, Validation Loss: 0.08892548084259033\n",
      "Epoch: 1653/2000, Train Loss: 0.034229375422000885, Validation Loss: 0.0923045203089714\n",
      "Epoch: 1654/2000, Train Loss: 0.034627918154001236, Validation Loss: 0.08952219784259796\n",
      "Epoch: 1655/2000, Train Loss: 0.0347541943192482, Validation Loss: 0.09028174728155136\n",
      "Epoch: 1656/2000, Train Loss: 0.03312147781252861, Validation Loss: 0.08622817695140839\n",
      "Epoch: 1657/2000, Train Loss: 0.03128759190440178, Validation Loss: 0.08584039658308029\n",
      "Epoch: 1658/2000, Train Loss: 0.03035278618335724, Validation Loss: 0.08725923299789429\n",
      "Epoch: 1659/2000, Train Loss: 0.031004415825009346, Validation Loss: 0.08709936589002609\n",
      "Epoch: 1660/2000, Train Loss: 0.03215726092457771, Validation Loss: 0.08931828290224075\n",
      "Epoch: 1661/2000, Train Loss: 0.03224338963627815, Validation Loss: 0.08659423142671585\n",
      "Epoch: 1662/2000, Train Loss: 0.031425051391124725, Validation Loss: 0.08650094270706177\n",
      "Epoch: 1663/2000, Train Loss: 0.03038492053747177, Validation Loss: 0.08630244433879852\n",
      "Epoch: 1664/2000, Train Loss: 0.030272362753748894, Validation Loss: 0.08620396256446838\n",
      "Epoch: 1665/2000, Train Loss: 0.03091573156416416, Validation Loss: 0.08809742331504822\n",
      "Epoch: 1666/2000, Train Loss: 0.0312890000641346, Validation Loss: 0.0862850546836853\n",
      "Epoch: 1667/2000, Train Loss: 0.03101460449397564, Validation Loss: 0.08658161759376526\n",
      "Epoch: 1668/2000, Train Loss: 0.030329346656799316, Validation Loss: 0.08593752235174179\n",
      "Epoch: 1669/2000, Train Loss: 0.030041340738534927, Validation Loss: 0.08586204051971436\n",
      "Epoch: 1670/2000, Train Loss: 0.030308807268738747, Validation Loss: 0.08724869042634964\n",
      "Epoch: 1671/2000, Train Loss: 0.03062460571527481, Validation Loss: 0.08606336265802383\n",
      "Epoch: 1672/2000, Train Loss: 0.03057660534977913, Validation Loss: 0.08654565364122391\n",
      "Epoch: 1673/2000, Train Loss: 0.030177082866430283, Validation Loss: 0.08586055040359497\n",
      "Epoch: 1674/2000, Train Loss: 0.029914477840065956, Validation Loss: 0.0858035683631897\n",
      "Epoch: 1675/2000, Train Loss: 0.030000101774930954, Validation Loss: 0.08675383776426315\n",
      "Epoch: 1676/2000, Train Loss: 0.03020256944000721, Validation Loss: 0.08592573553323746\n",
      "Epoch: 1677/2000, Train Loss: 0.03021889179944992, Validation Loss: 0.0864376649260521\n",
      "Epoch: 1678/2000, Train Loss: 0.029994485899806023, Validation Loss: 0.08581873029470444\n",
      "Epoch: 1679/2000, Train Loss: 0.029797442257404327, Validation Loss: 0.0857858657836914\n",
      "Epoch: 1680/2000, Train Loss: 0.02979816496372223, Validation Loss: 0.08640898019075394\n",
      "Epoch: 1681/2000, Train Loss: 0.02991003356873989, Validation Loss: 0.08582291007041931\n",
      "Epoch: 1682/2000, Train Loss: 0.02994139865040779, Validation Loss: 0.08633587509393692\n",
      "Epoch: 1683/2000, Train Loss: 0.02982022985816002, Validation Loss: 0.08581650257110596\n",
      "Epoch: 1684/2000, Train Loss: 0.02967974543571472, Validation Loss: 0.08583156764507294\n",
      "Epoch: 1685/2000, Train Loss: 0.029642054811120033, Validation Loss: 0.08620039373636246\n",
      "Epoch: 1686/2000, Train Loss: 0.029692234471440315, Validation Loss: 0.08578059077262878\n",
      "Epoch: 1687/2000, Train Loss: 0.029720868915319443, Validation Loss: 0.08624523878097534\n",
      "Epoch: 1688/2000, Train Loss: 0.02966020070016384, Validation Loss: 0.08580753207206726\n",
      "Epoch: 1689/2000, Train Loss: 0.029562756419181824, Validation Loss: 0.0858951136469841\n",
      "Epoch: 1690/2000, Train Loss: 0.029507486149668694, Validation Loss: 0.0860745906829834\n",
      "Epoch: 1691/2000, Train Loss: 0.029514437541365623, Validation Loss: 0.08579475432634354\n",
      "Epoch: 1692/2000, Train Loss: 0.02953174151480198, Validation Loss: 0.0861799344420433\n",
      "Epoch: 1693/2000, Train Loss: 0.02950531244277954, Validation Loss: 0.08579883724451065\n",
      "Epoch: 1694/2000, Train Loss: 0.029442347586154938, Validation Loss: 0.08593595027923584\n",
      "Epoch: 1695/2000, Train Loss: 0.029385684058070183, Validation Loss: 0.08597151935100555\n",
      "Epoch: 1696/2000, Train Loss: 0.029365122318267822, Validation Loss: 0.08582013100385666\n",
      "Epoch: 1697/2000, Train Loss: 0.029366206377744675, Validation Loss: 0.08613099157810211\n",
      "Epoch: 1698/2000, Train Loss: 0.02935427613556385, Validation Loss: 0.0858350619673729\n",
      "Epoch: 1699/2000, Train Loss: 0.02931646630167961, Validation Loss: 0.08601099252700806\n",
      "Epoch: 1700/2000, Train Loss: 0.02926838956773281, Validation Loss: 0.08593837171792984\n",
      "Epoch: 1701/2000, Train Loss: 0.02923446148633957, Validation Loss: 0.0858762115240097\n",
      "Epoch: 1702/2000, Train Loss: 0.029219526797533035, Validation Loss: 0.08607956767082214\n",
      "Epoch: 1703/2000, Train Loss: 0.029208319261670113, Validation Loss: 0.08585987985134125\n",
      "Epoch: 1704/2000, Train Loss: 0.02918536402285099, Validation Loss: 0.0860479325056076\n",
      "Epoch: 1705/2000, Train Loss: 0.029149556532502174, Validation Loss: 0.08591250330209732\n",
      "Epoch: 1706/2000, Train Loss: 0.029113413766026497, Validation Loss: 0.08592468500137329\n",
      "Epoch: 1707/2000, Train Loss: 0.02908710017800331, Validation Loss: 0.08601433038711548\n",
      "Epoch: 1708/2000, Train Loss: 0.02906942553818226, Validation Loss: 0.08587353676557541\n",
      "Epoch: 1709/2000, Train Loss: 0.02905123680830002, Validation Loss: 0.08604322373867035\n",
      "Epoch: 1710/2000, Train Loss: 0.029025636613368988, Validation Loss: 0.08589757233858109\n",
      "Epoch: 1711/2000, Train Loss: 0.028994446620345116, Validation Loss: 0.08597204834222794\n",
      "Epoch: 1712/2000, Train Loss: 0.028964217752218246, Validation Loss: 0.08597173541784286\n",
      "Epoch: 1713/2000, Train Loss: 0.028939511626958847, Validation Loss: 0.0859101265668869\n",
      "Epoch: 1714/2000, Train Loss: 0.0289189163595438, Validation Loss: 0.08602920919656754\n",
      "Epoch: 1715/2000, Train Loss: 0.028897587209939957, Validation Loss: 0.08590657263994217\n",
      "Epoch: 1716/2000, Train Loss: 0.028872404247522354, Validation Loss: 0.08601097762584686\n",
      "Epoch: 1717/2000, Train Loss: 0.028844285756349564, Validation Loss: 0.0859522596001625\n",
      "Epoch: 1718/2000, Train Loss: 0.0288167092949152, Validation Loss: 0.08595993369817734\n",
      "Epoch: 1719/2000, Train Loss: 0.028791986405849457, Validation Loss: 0.0860116183757782\n",
      "Epoch: 1720/2000, Train Loss: 0.028769509866833687, Validation Loss: 0.08593533933162689\n",
      "Epoch: 1721/2000, Train Loss: 0.028746899217367172, Validation Loss: 0.08603091537952423\n",
      "Epoch: 1722/2000, Train Loss: 0.028722364455461502, Validation Loss: 0.08595218509435654\n",
      "Epoch: 1723/2000, Train Loss: 0.028696175664663315, Validation Loss: 0.08600665628910065\n",
      "Epoch: 1724/2000, Train Loss: 0.0286699328571558, Validation Loss: 0.08599866181612015\n",
      "Epoch: 1725/2000, Train Loss: 0.02864501066505909, Validation Loss: 0.08597946912050247\n",
      "Epoch: 1726/2000, Train Loss: 0.028621423989534378, Validation Loss: 0.08603763580322266\n",
      "Epoch: 1727/2000, Train Loss: 0.02859812043607235, Validation Loss: 0.08597441762685776\n",
      "Epoch: 1728/2000, Train Loss: 0.028574060648679733, Validation Loss: 0.08604057133197784\n",
      "Epoch: 1729/2000, Train Loss: 0.02854902297258377, Validation Loss: 0.08599531650543213\n",
      "Epoch: 1730/2000, Train Loss: 0.028523623943328857, Validation Loss: 0.08602069318294525\n",
      "Epoch: 1731/2000, Train Loss: 0.028498657047748566, Validation Loss: 0.08603014051914215\n",
      "Epoch: 1732/2000, Train Loss: 0.028474435210227966, Validation Loss: 0.08600559830665588\n",
      "Epoch: 1733/2000, Train Loss: 0.028450656682252884, Validation Loss: 0.08605356514453888\n",
      "Epoch: 1734/2000, Train Loss: 0.028426751494407654, Validation Loss: 0.08600804954767227\n",
      "Epoch: 1735/2000, Train Loss: 0.02840237319469452, Validation Loss: 0.0860537737607956\n",
      "Epoch: 1736/2000, Train Loss: 0.028377611190080643, Validation Loss: 0.08602859824895859\n",
      "Epoch: 1737/2000, Train Loss: 0.028352828696370125, Validation Loss: 0.08604320138692856\n",
      "Epoch: 1738/2000, Train Loss: 0.028328340500593185, Validation Loss: 0.08605635911226273\n",
      "Epoch: 1739/2000, Train Loss: 0.028304193168878555, Validation Loss: 0.08603759855031967\n",
      "Epoch: 1740/2000, Train Loss: 0.02828020416200161, Validation Loss: 0.0860743299126625\n",
      "Epoch: 1741/2000, Train Loss: 0.028256112709641457, Validation Loss: 0.08604341000318527\n",
      "Epoch: 1742/2000, Train Loss: 0.028231799602508545, Validation Loss: 0.08607668429613113\n",
      "Epoch: 1743/2000, Train Loss: 0.028207330033183098, Validation Loss: 0.08606036752462387\n",
      "Epoch: 1744/2000, Train Loss: 0.02818286418914795, Validation Loss: 0.08607196062803268\n",
      "Epoch: 1745/2000, Train Loss: 0.028158538043498993, Validation Loss: 0.08608134835958481\n",
      "Epoch: 1746/2000, Train Loss: 0.028134381398558617, Validation Loss: 0.08606994897127151\n",
      "Epoch: 1747/2000, Train Loss: 0.028110302984714508, Validation Loss: 0.08609608560800552\n",
      "Epoch: 1748/2000, Train Loss: 0.02808620035648346, Validation Loss: 0.08607526868581772\n",
      "Epoch: 1749/2000, Train Loss: 0.0280620064586401, Validation Loss: 0.08610108494758606\n",
      "Epoch: 1750/2000, Train Loss: 0.02803773246705532, Validation Loss: 0.08608847856521606\n",
      "Epoch: 1751/2000, Train Loss: 0.0280134454369545, Validation Loss: 0.08610112965106964\n",
      "Epoch: 1752/2000, Train Loss: 0.027989210560917854, Validation Loss: 0.08610551804304123\n",
      "Epoch: 1753/2000, Train Loss: 0.027965055778622627, Validation Loss: 0.08610228449106216\n",
      "Epoch: 1754/2000, Train Loss: 0.027940960600972176, Validation Loss: 0.08612015098333359\n",
      "Epoch: 1755/2000, Train Loss: 0.027916880324482918, Validation Loss: 0.08610820770263672\n",
      "Epoch: 1756/2000, Train Loss: 0.02789277769625187, Validation Loss: 0.08612941205501556\n",
      "Epoch: 1757/2000, Train Loss: 0.027868637815117836, Validation Loss: 0.08611979335546494\n",
      "Epoch: 1758/2000, Train Loss: 0.027844475582242012, Validation Loss: 0.08613467216491699\n",
      "Epoch: 1759/2000, Train Loss: 0.027820320799946785, Validation Loss: 0.08613468706607819\n",
      "Epoch: 1760/2000, Train Loss: 0.027796197682619095, Validation Loss: 0.08613879978656769\n",
      "\n",
      "Early stopping at epoch 1760 due to validation loss increasing for 3 consecutive epochs.\n",
      "\n",
      "Accuracy: 0.9735\n",
      "\n",
      "Class 0 - Precision: 0.9776, Recall: 0.9695, F1 Score: 0.9735\n",
      "Class 1 - Precision: 0.9693, Recall: 0.9775, F1 Score: 0.9734\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3145   99]\n",
      " [  72 3127]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgQAAAHFCAYAAACNXuEaAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABLqklEQVR4nO3deVxU5f4H8M+wDYswCsimiGhKKGSKhnCvS6ko5ZaVGsV1Qa00jau2mDfFFlG7qaXXJTPxqqX9KsvKSL0uZYIiSW5kWaiYjLjgsG8zz+8P49QIjDPMwMicz/v1Oq+cc57znO8QL+Y73+d5zlEIIQSIiIhI1uysHQARERFZHxMCIiIiYkJARERETAiIiIgITAiIiIgITAiIiIgITAiIiIgITAiIiIgITAiIiIgITAhk5/jx45gwYQKCg4Ph7OyMFi1aoEePHliyZAmuX7/eqNc+duwY+vXrB5VKBYVCgeXLl1v8GgqFAklJSRbv93ZSUlKgUCigUCiwf//+WseFELjrrrugUCjQv3//Bl1j1apVSElJMemc/fv31xtTU6j5mdRsbm5uCA0NxYIFC1BSUqLXdvz48Wjfvr1R/fbv3x9hYWG3bffNN98gJiYGAQEBUCqVCAgIQP/+/bFo0SIAQFJSUq0Y69pq/p+NHz8eCoUC7u7uKC4urnW98+fPw87Ozmq/h0TmcLB2ANR01q1bh6lTpyIkJATPP/88unTpgqqqKhw9ehRr1qxBWloatm/f3mjXnzhxIkpKSrB161a0atXK6D/+pkhLS0Pbtm0t3q+x3N3dsX79+lof+gcOHMCvv/4Kd3f3Bve9atUqeHt7Y/z48Uaf06NHD6SlpaFLly4Nvq65Hn30UcyaNQsAUFxcjAMHDuDVV1/F8ePH8cknn0jtXnnlFTz33HMWu+6aNWvwzDPP4JFHHsHKlSvh6emJ3NxcHDp0CB9//DFeeuklTJo0CUOGDJHOycvLw6hRozB9+nTExcVJ+z08PKR/Ozo6orq6Gtu2bUNCQoLeNTds2AB3d3cUFhZa7H0QNRlBsnDo0CFhb28vhgwZIsrLy2sdr6ioEJ9//nmjxuDg4CCeeeaZRr2GtWzYsEEAEJMmTRIuLi5Co9HoHX/yySdFVFSU6Nq1q+jXr1+DrmHKuZWVlaKqqqpB17EkAGLatGm19sfHxws7OztRVlbWoH779esnunbtarBNu3btRN++fes8ptVq69yfk5MjAIg333yzzuPjxo0Tbm5uYuzYsSI6OlrvmE6nE0FBQWLy5MkCgJg/f/7t3wjRHYRDBjKxcOFCKBQKvPvuu1AqlbWOOzk5Yfjw4dJrnU6HJUuW4O6774ZSqYSPjw/+8Y9/4OLFi3rn1ZRuMzIy0KdPH7i6uqJDhw5YtGgRdDodgD/L6dXV1Vi9erVUhgX+LNnequacc+fOSfv27t2L/v37w8vLCy4uLmjXrh0eeeQRlJaWSm3qKtWePHkSI0aMQKtWreDs7Ix7770XGzdu1GtTU1r/8MMPMXfuXAQEBMDDwwMDBw7EmTNnjPshA3j88ccBAB9++KG0T6PR4JNPPsHEiRPrPGfBggWIjIyEp6cnPDw80KNHD6xfvx7iL88da9++PU6dOoUDBw5IP7+aCktN7Js2bcKsWbPQpk0bKJVKnD17ttaQwdWrVxEYGIjo6GhUVVVJ/Z8+fRpubm6Ij483+r2ao2bYyN7eXtpnypCBMa5duwZ/f/86j9nZmfenb+LEiTh06JDe78aePXtw/vx5TJgwway+iayFCYEMaLVa7N27FxEREQgMDDTqnGeeeQYvvvgiBg0ahB07duC1115DamoqoqOjcfXqVb22arUaTzzxBJ588kns2LEDsbGxmDNnDjZv3gwAeOihh5CWlgbgZvk4LS1Nem2sc+fO4aGHHoKTkxPef/99pKamYtGiRXBzc0NlZWW95505cwbR0dE4deoU3nnnHXz66afo0qULxo8fjyVLltRq//LLL+P8+fN477338O677+KXX37BsGHDoNVqjYrTw8MDjz76KN5//31p34cffgg7OzuMGTOm3vf21FNP4aOPPsKnn34qlaxfe+01qc327dvRoUMHdO/eXfr53Tq8M2fOHFy4cAFr1qzBF198AR8fn1rX8vb2xtatW5GRkYEXX3wRAFBaWorHHnsM7dq1w5o1a4x6n6YQQqC6uhrV1dW4ceMGPv/8c2zcuBFjx46Fo6Ojxa9XIyoqCp988gmSkpLw448/Gv3/0BgDBw5EUFCQ3v/n9evXo2/fvujUqZPFrkPUpKxdoqDGp1arBQAxduxYo9pnZ2cLAGLq1Kl6+w8fPiwAiJdfflna169fPwFAHD58WK9tly5dxODBg/X2oY7y8fz580Vdv4Y1JficnBwhhBAff/yxACCysrIMxo5bSrVjx44VSqVSXLhwQa9dbGyscHV1FTdu3BBCCLFv3z4BQDz44IN67T766CMBQKSlpRm8bk28GRkZUl8nT54UQgjRq1cvMX78eCHE7cv+Wq1WVFVViVdffVV4eXkJnU4nHavv3Jrr1VUerzm2b98+vf2LFy8WAMT27dvFuHHjhIuLizh+/LjB99gQAOrcYmNjRXFxsV7bcePGiaCgIKP6NWbI4OzZsyIsLEy6pouLixgwYIBYuXKlqKysrPMcY4cMhLj5u+vn5yeqqqrEtWvXhFKpFCkpKeLKlSscMqBmiRUCqmXfvn0AUGvy2n333YfQ0FD873//09vv5+eH++67T2/fPffcg/Pnz1sspnvvvRdOTk6YMmUKNm7ciN9++82o8/bu3YsBAwbUqoyMHz8epaWltSoVfx02AW6+DwAmvZd+/fqhY8eOeP/993HixAlkZGTUO1xQE+PAgQOhUqlgb28PR0dHzJs3D9euXUN+fr7R133kkUeMbvv888/joYcewuOPP46NGzdixYoVCA8Pv+15Nd/0azbxl2GN+owePRoZGRnIyMjAt99+i3feeQdHjx7FkCFDUFFRUe95Op1O71qmfsPv2LEjfvzxRxw4cAALFizAwIEDkZGRgWeffRZRUVEoLy83qb9bTZgwAZcvX8bXX3+NLVu2wMnJCY899phZfRJZExMCGfD29oarqytycnKMan/t2jUAqHP8NSAgQDpew8vLq1Y7pVKJsrKyBkRbt44dO2LPnj3w8fHBtGnT0LFjR3Ts2BFvv/22wfPqG0cOCAiQjv/Vre+lZr6FKe9FoVBgwoQJ2Lx5M9asWYPOnTujT58+dbY9cuQIYmJiANxcBfL9998jIyMDc+fONfm69Y2X1xfj+PHjUV5eDj8/P6PmDpw7dw6Ojo5624EDB257XuvWrdGzZ0/07NkTffr0wfTp0/HOO+/g4MGDBpdRTpw4Ue9aAwYMMPr91bCzs0Pfvn0xb9487NixA5cuXcKYMWOQmZmpV+5viKCgIAwYMADvv/8+3n//fYwdOxaurq5m9UlkTUwIZMDe3h4DBgxAZmZmrUmBdan5UMzLy6t17NKlS/D29rZYbM7OzgBQ65virfMUAKBPnz744osvoNFokJ6ejqioKCQmJmLr1q319u/l5VXv+wBg0ffyV+PHj8fVq1exZs0ag5PMtm7dCkdHR3z55ZcYPXo0oqOj0bNnzwZds67JmfXJy8vDtGnTcO+99+LatWuYPXv2bc8JCAiQvunXbBEREQ2Ktaby8uOPP9bbJikpSe9aa9eubdC1/srNzQ1z5swBcHOyqbkmTpyIHTt2ICsry2AViKg5YEIgE3PmzIEQApMnT65zEl5VVRW++OILAMADDzwAANKkwBoZGRnIzs5u0De1+tTMKj9+/Lje/ppY6mJvb4/IyEj85z//AQD88MMP9bYdMGAA9u7dKyUANf773//C1dUVvXv3bmDkhrVp0wbPP/88hg0bhnHjxtXbTqFQwMHBQW+2fVlZGTZt2lSrraWqLlqtFo8//jgUCgW+/vprJCcnY8WKFfj0008Nnufk5CR906/ZGnpfhaysLACoc+Jjjfbt2+tdKyQkxKRr1JUIAkB2djaAP6tE5nj44Yfx8MMPY+LEiY32u0TUVHhjIpmIiorC6tWrMXXqVEREROCZZ55B165dUVVVhWPHjuHdd99FWFgYhg0bhpCQEEyZMgUrVqyAnZ0dYmNjce7cObzyyisIDAzEP//5T4vF9eCDD8LT0xMJCQl49dVX4eDggJSUFOTm5uq1W7NmDfbu3YuHHnoI7dq1Q3l5uVTyHThwYL39z58/H19++SXuv/9+zJs3D56entiyZQu++uorLFmyBCqVymLv5VY1d8Mz5KGHHsLSpUsRFxeHKVOm4Nq1a/j3v/9d59LQ8PBwbN26Fdu2bUOHDh3g7Oxs1Lj/rebPn4/vvvsOu3btgp+fH2bNmoUDBw4gISEB3bt3R3BwsMl9GnL58mWkp6cDAMrLy5GVlYXXX38dLVu2NGuJXmFhIT7++ONa+1u3bo1+/fqha9euGDBgAGJjY9GxY0eUl5fj8OHDeOutt+Dr61vrpkIN4ezsXGcMRM2StWc1UtPKysoS48aNE+3atRNOTk7Czc1NdO/eXcybN0/k5+dL7bRarVi8eLHo3LmzcHR0FN7e3uLJJ58Uubm5ev3VN9u7rhnjqOcmNUeOHBHR0dHCzc1NtGnTRsyfP1+89957eqsM0tLSxMMPPyyCgoKEUqkUXl5eol+/fmLHjh21rnHr7O4TJ06IYcOGCZVKJZycnES3bt3Ehg0b9NrUzMb/v//7P739NbPOb21/q7+uMjCkrpUC77//vggJCRFKpVJ06NBBJCcni/Xr1+u9fyGEOHfunIiJiRHu7u4CgPTzrS/2vx6rWWWwa9cuYWdnV+tndO3aNdGuXTvRq1cvUVFRYfA9mAK3rC5wdHQUHTp0EBMmTBBnz57Va2vqKoNb+67Zan6+a9euFaNGjRIdOnQQrq6uwsnJSXTs2FE8/fTTtX6Pa5iyyqA+XGVAzZVCCCOmCRMREZFN4xwCIiIiYkJARERETAiIiIgITAiIiIgITAiIiIgITAiIiIgIzfzGRDqdDpcuXYK7u7tJt20lIqI7gxACRUVFCAgIgJ1d431HLS8vN/iodGM5OTlJt1y3Nc06Ibh06VKtp9gREVHzk5ubi7Zt2zZK3+Xl5QgOagF1vmlPzKyLn58fcnJybDIpaNYJQc191M//0B4eLTj6Qbbp4bvvtXYIRI2mWlThoPiiwc/FMEZlZSXU+Vqcz2wPD/eGf1YUFukQFHEOlZWVTAjuNDXDBB4t7Mz6n0x0J3NQOFo7BKLGJUx7WmdDtXBXoIV7w6+jg20PTTfrhICIiMhYWqGD1oyb9WuFznLB3IGYEBARkSzoIKBDwzMCc85tDlhnJyIiIlYIiIhIHnTQwZyiv3ln3/mYEBARkSxohYBWNLzsb865zQGHDIiIiIgVAiIikgdOKjSMCQEREcmCDgJaJgT14pABERERsUJARETywCEDw1ghICIiWahZZWDOZorVq1fjnnvugYeHBzw8PBAVFYWvv/5aOi6EQFJSEgICAuDi4oL+/fvj1KlTen1UVFRg+vTp8Pb2hpubG4YPH46LFy/qtSkoKEB8fDxUKhVUKhXi4+Nx48YNk38+TAiIiIgaQdu2bbFo0SIcPXoUR48exQMPPIARI0ZIH/pLlizB0qVLsXLlSmRkZMDPzw+DBg1CUVGR1EdiYiK2b9+OrVu34uDBgyguLsbQoUOh1f755Ma4uDhkZWUhNTUVqampyMrKQnx8vMnxKoRovgsrCwsLoVKpUPBzBz7ciGzW4LYR1g6BqNFUiyrs130KjUYDDw+PRrlGzWfFT9m+cDfjs6KoSIe7Qy+bFaunpyfefPNNTJw4EQEBAUhMTMSLL74I4GY1wNfXF4sXL8ZTTz0FjUaD1q1bY9OmTRgzZgwA4NKlSwgMDMTOnTsxePBgZGdno0uXLkhPT0dkZCQAID09HVFRUfjpp58QEhJidGz8FCUiIlnQ/rHKwJytwdfWarF161aUlJQgKioKOTk5UKvViImJkdoolUr069cPhw4dAgBkZmaiqqpKr01AQADCwsKkNmlpaVCpVFIyAAC9e/eGSqWS2hiLkwqJiEgWtAJmPu3w5n8LCwv19iuVSiiVyjrPOXHiBKKiolBeXo4WLVpg+/bt6NKli/Rh7evrq9fe19cX58+fBwCo1Wo4OTmhVatWtdqo1WqpjY+PT63r+vj4SG2MxQoBERGRCQIDA6UJfCqVCsnJyfW2DQkJQVZWFtLT0/HMM89g3LhxOH36tHRcoVDotRdC1Np3q1vb1NXemH5uxQoBERHJgu6PzZzzASA3N1dvDkF91QEAcHJywl133QUA6NmzJzIyMvD2229L8wbUajX8/f2l9vn5+VLVwM/PD5WVlSgoKNCrEuTn5yM6Olpqc/ny5VrXvXLlSq3qw+2wQkBERLKggwJaMzYdbn7jrllGWLMZSghuJYRARUUFgoOD4efnh927d0vHKisrceDAAenDPiIiAo6Ojnpt8vLycPLkSalNVFQUNBoNjhw5IrU5fPgwNBqN1MZYrBAQERE1gpdffhmxsbEIDAxEUVERtm7div379yM1NRUKhQKJiYlYuHAhOnXqhE6dOmHhwoVwdXVFXFwcAEClUiEhIQGzZs2Cl5cXPD09MXv2bISHh2PgwIEAgNDQUAwZMgSTJ0/G2rVrAQBTpkzB0KFDTVphADAhICIimdCJm5s555vi8uXLiI+PR15eHlQqFe655x6kpqZi0KBBAIAXXngBZWVlmDp1KgoKChAZGYldu3bB3d1d6mPZsmVwcHDA6NGjUVZWhgEDBiAlJQX29vZSmy1btmDGjBnSaoThw4dj5cqVJr8/3oeA6A7H+xCQLWvK+xAcPuWHFmZ8VhQX6RDZVd2osVoTP0WJiIiIQwZERCQPNZMDzTnfljEhICIiWdAJBXSi4R/q5pzbHHDIgIiIiFghICIieeCQgWFMCIiISBa0sIPWjMK49vZNmjUmBEREJAvCzDkEgnMIiIiIyNaxQkBERLLAOQSGMSEgIiJZ0Ao7aIUZcwia7X19jcMhAyIiImKFgIiI5EEHBXRmfA/WwbZLBEwIiIhIFjiHwDAOGRARERErBEREJA/mTyrkkAEREVGzd3MOgRkPN+KQAREREdk6VgiIiEgWdGY+y4CrDIiIiGwA5xAYxoSAiIhkQQc73ofAAM4hICIiIlYIiIhIHrRCAa0ZjzA259zmgAkBERHJgtbMSYVaDhkQERGRrWOFgIiIZEEn7KAzY5WBjqsMiIiImj8OGRjGIQMiIiJihYCIiORBB/NWCugsF8odiQkBERHJgvk3JrLtorptvzsiIiIyCisEREQkC+Y/y8C2v0MzISAiIlnQQQEdzJlDwDsVEhERNXusEBhm2++OiIiIjMIKARERyYL5Nyay7e/QTAiIiEgWdEIBnTn3IbDxpx3adrpDRERERmGFgIiIZEFn5pCBrd+YiAkBERHJgvlPO7TthMC23x0REREZhRUCIiKSBS0U0JpxcyFzzm0OmBAQEZEscMjAMNt+d0RERGQUVgiIiEgWtDCv7K+1XCh3JCYEREQkCxwyMIwJARERyQIfbmSYbb87IiIiMgorBEREJAsCCujMmEMguOyQiIio+eOQgWG2/e6IiIisJDk5Gb169YK7uzt8fHwwcuRInDlzRq/N+PHjoVAo9LbevXvrtamoqMD06dPh7e0NNzc3DB8+HBcvXtRrU1BQgPj4eKhUKqhUKsTHx+PGjRsmxcuEgIiIZKHm8cfmbKY4cOAApk2bhvT0dOzevRvV1dWIiYlBSUmJXrshQ4YgLy9P2nbu3Kl3PDExEdu3b8fWrVtx8OBBFBcXY+jQodBq/1wIGRcXh6ysLKSmpiI1NRVZWVmIj483KV4OGRARkSxozXzaoannpqam6r3esGEDfHx8kJmZib59+0r7lUol/Pz86uxDo9Fg/fr12LRpEwYOHAgA2Lx5MwIDA7Fnzx4MHjwY2dnZSE1NRXp6OiIjIwEA69atQ1RUFM6cOYOQkBCj4mWFgIiIqAloNBoAgKenp97+/fv3w8fHB507d8bkyZORn58vHcvMzERVVRViYmKkfQEBAQgLC8OhQ4cAAGlpaVCpVFIyAAC9e/eGSqWS2hiDFQIiIpKFhpT9bz0fAAoLC/X2K5VKKJVKg+cKITBz5kz8/e9/R1hYmLQ/NjYWjz32GIKCgpCTk4NXXnkFDzzwADIzM6FUKqFWq+Hk5IRWrVrp9efr6wu1Wg0AUKvV8PHxqXVNHx8fqY0xmBAQEZEs6GAHnRmF8ZpzAwMD9fbPnz8fSUlJBs999tlncfz4cRw8eFBv/5gxY6R/h4WFoWfPnggKCsJXX32FUaNG1dufEAIKxZ/JzV//XV+b22FCQEREZILc3Fx4eHhIr29XHZg+fTp27NiBb7/9Fm3btjXY1t/fH0FBQfjll18AAH5+fqisrERBQYFelSA/Px/R0dFSm8uXL9fq68qVK/D19TX6fXEOARERyYJWKMzeAMDDw0Nvqy8hEELg2Wefxaeffoq9e/ciODj4tjFeu3YNubm58Pf3BwBERETA0dERu3fvltrk5eXh5MmTUkIQFRUFjUaDI0eOSG0OHz4MjUYjtTEGKwRERCQLlppDYKxp06bhgw8+wOeffw53d3dpPF+lUsHFxQXFxcVISkrCI488An9/f5w7dw4vv/wyvL298fDDD0ttExISMGvWLHh5ecHT0xOzZ89GeHi4tOogNDQUQ4YMweTJk7F27VoAwJQpUzB06FCjVxgATAiIiEgmhJlPOxQmnrt69WoAQP/+/fX2b9iwAePHj4e9vT1OnDiB//73v7hx4wb8/f1x//33Y9u2bXB3d5faL1u2DA4ODhg9ejTKysowYMAApKSkwN7eXmqzZcsWzJgxQ1qNMHz4cKxcudKkeJkQEBERNQIhhMHjLi4u+Oabb27bj7OzM1asWIEVK1bU28bT0xObN282Oca/YkJARESyoIUCWjMeUGTOuc0BEwIiIpIFnTB9HsCt59syrjIgIiIiVgjk5ouNXvjqv964nOsEAAgKKccT/1Sj1wNFAICDO1XYuckLvxx3RWGBA1btOoOOYWV19iUE8K8nO+DoPg/MX5+D6FiNdOwf93XB5YtOeu1HT7uMhLl5jfTOiIzn4qbFuOcvIXqIBi29q/DrSVesnt8WP//oBgBo6V2FhJd/R0TfIripqnHysDv+80pbXMpxtnLkZA6dmZMKzTm3ObD6u1u1ahWCg4Ph7OyMiIgIfPfdd9YOyaa19q/CxJcvYcXXP2PF1z+j29+KkDQhGOfO3PxDV15qhy69SjDx5Uu37Wv7utYwdBOsfzyfhw+zTkpbXGLtG2cQWcM/3zyPHn2KsOS5IDw9MBSZ37pj0Ye/wMuvEoDA/PW/wb9dJZISOmDa4FBcvuiERR+ehdJFe9u+6c6lg8LszZZZNSHYtm0bEhMTMXfuXBw7dgx9+vRBbGwsLly4YM2wbFrvmELcN6AIbTtWoG3HCkx4SQ1nNx1+ynQFAAx8tABPzryM7n2LDfbz6ylnfLK2NWYurf//lUsLHTx9qqXNxU1n0fdC1BBOzjr8/cEbeO+NNjh52B2Xzjlj89IAqHOVGBp/FW2CK9AlogQrXg7Ezz+64eJvzlj5ciBc3LS4f2SBtcMnajRWTQiWLl2KhIQETJo0CaGhoVi+fDkCAwOltZvUuLRaYP9nLVFRaofQniW3P+EP5aUKLJraHtPeuAhPn+p62/3ff3zwaNcwPDMwBB+87YuqStvOrql5sLcXsHcAKiv0fx8ryu3Q9b5iOCpvzhyrrPjzz6NOp0BVpQJdexlOlOnOZqk7Fdoqq80hqKysRGZmJl566SW9/TExMSY9rpFMl5PtjMRhnVBZYQcXNx3mrc9BUOcKo89fm9QGXXqWIHpIYb1tRk66grvCS9FCpcWZY67YkByAyxec8M+3ci3xFogarKzEHqePuiEuUY0LZ51x44oj+o+8jru7l+D3HCVyzzpDneuEiS/9jrdfaofyUjuMmpIPL99qePpUWTt8MgPnEBhmtYTg6tWr0Gq1tR688NdHOt6qoqICFRV/fnDd+ghKMk7bjhVYtfsMSgrtcfCrlvj3c0F489NfjEoK0r7xQNb37li164zBdqOmXJH+3aFLOVq01OL1ycFImHsJHp4chyXrWvJce8x86zw+zDwJbTVw9qQr9n3WCneFlUFbrcBrUzpg5r/P45NTx6GtBo4d9MCRvR6375ioGbP6KoNbH81o6HGNycnJWLBgQVOEZdMcnQTaBFcCADp3K8OZLFd89l5rPLfk4m3PzfreHXnnnDDq7nC9/a9Nbo+wyBK8+cnZOs8L7VEKALh0TgkPz1Iz3wGRefLOK/H8o52hdNHCzV2H6/mOeHnVb1D/sfrm7AlXTB0cCld3LRwdddBcd8TbX/yEn390tXLkZA4dzHyWgY1PKrRaQuDt7Q17e/ta1YD8/Px6H9c4Z84czJw5U3pdWFhY67nU1DBVlcaVwsY8exmxcdf09j31wN14Kul39I6pv2Jz9qQLALDkSneUijJ7VJTZo4WqGhH9ivDewjZ6x0uL7AHYIyC4HJ3uKcXGNwOsEyhZhDBzpYBgQtA4nJycEBERgd27d0tPdQKA3bt3Y8SIEXWeo1Qqb/vcaTLs/WR/9HqgEK0DqlBWbIf9n7fE8UMt8PqWXwEAhQX2uPK7E65dvvmrkfvrzZ93K58qvRUDt/JpUwW/djerDqePuuKnH9zQLboYbh5anMlyxdqkAPSO0cCnLRMCsr6IfoVQKARyf3VGm/YVmPSv33HxNyV2bfMCAPR5qACa6w7I/90JwXeX4ekFF5H2TUv88C2HDZqzpn7aYXNj1SGDmTNnIj4+Hj179kRUVBTeffddXLhwAU8//bQ1w7JpN6444M3pQbie7wBXdy2CQ8vx+pZfEdHv5uzp9F0qvPXPdlL75GfaAwCenKlG/Oy653bcytFJ4MCOlti81A9VlQr4tKlEbNx1PDaV9yGgO4ObuxYTXvod3v5VKLphj++/boUNiwOgrb75B9/TtwpPzb+Ilt7VuJ7viD0fe+KDt/2sHDVR41KI2z2OqZGtWrUKS5YsQV5eHsLCwrBs2TL07dvXqHMLCwuhUqlQ8HMHeLjb9uxPkq/BbSOsHQJRo6kWVdiv+xQajQYeHo1Tgan5rHh49wQ4ujnd/oR6VJVUYvugDY0aqzVZfVLh1KlTMXXqVGuHQURENo5DBobxazURERFZv0JARETUFMx9HgGXHRIREdkADhkYxiEDIiIiYoWAiIjkgRUCw5gQEBGRLDAhMIxDBkRERMQKARERyQMrBIYxISAiIlkQMG/poFVv69sEmBAQEZEssEJgGOcQEBERESsEREQkD6wQGMaEgIiIZIEJgWEcMiAiIiJWCIiISB5YITCMCQEREcmCEAoIMz7UzTm3OeCQAREREbFCQERE8qCDwqwbE5lzbnPAhICIiGSBcwgM45ABERERsUJARETywEmFhjEhICIiWeCQgWFMCIiISBZYITCMcwiIiIiIFQIiIpIHYeaQga1XCJgQEBGRLAgAQph3vi3jkAERERGxQkBERPKggwIK3qmwXkwIiIhIFrjKwDAOGRARERErBEREJA86oYCCNyaqFxMCIiKSBSHMXGVg48sMOGRARERErBAQEZE8cFKhYUwIiIhIFpgQGMYhAyIikoWapx2as5kiOTkZvXr1gru7O3x8fDBy5EicOXNGr40QAklJSQgICICLiwv69++PU6dO6bWpqKjA9OnT4e3tDTc3NwwfPhwXL17Ua1NQUID4+HioVCqoVCrEx8fjxo0bJsXLhICIiKgRHDhwANOmTUN6ejp2796N6upqxMTEoKSkRGqzZMkSLF26FCtXrkRGRgb8/PwwaNAgFBUVSW0SExOxfft2bN26FQcPHkRxcTGGDh0KrVYrtYmLi0NWVhZSU1ORmpqKrKwsxMfHmxSvQojmO2+ysLAQKpUKBT93gIc7cxuyTYPbRlg7BKJGUy2qsF/3KTQaDTw8PBrlGjWfFZ23vAR7V2WD+9GWVuDnJxY1ONYrV67Ax8cHBw4cQN++fSGEQEBAABITE/Hiiy8CuFkN8PX1xeLFi/HUU09Bo9GgdevW2LRpE8aMGQMAuHTpEgIDA7Fz504MHjwY2dnZ6NKlC9LT0xEZGQkASE9PR1RUFH766SeEhIQYFR8/RYmISBZuLjtUmLHd7KewsFBvq6ioMOr6Go0GAODp6QkAyMnJgVqtRkxMjNRGqVSiX79+OHToEAAgMzMTVVVVem0CAgIQFhYmtUlLS4NKpZKSAQDo3bs3VCqV1MYYTAiIiIhMEBgYKI3Vq1QqJCcn3/YcIQRmzpyJv//97wgLCwMAqNVqAICvr69eW19fX+mYWq2Gk5MTWrVqZbCNj49PrWv6+PhIbYzBVQZERCQLllplkJubqzdkoFTefhji2WefxfHjx3Hw4MFaxxQK/ZiEELX21Y5Fv01d7Y3p569YISAiIlkQFtgAwMPDQ2+7XUIwffp07NixA/v27UPbtm2l/X5+fgBQ61t8fn6+VDXw8/NDZWUlCgoKDLa5fPlyreteuXKlVvXBECYEREREjUAIgWeffRaffvop9u7di+DgYL3jwcHB8PPzw+7du6V9lZWVOHDgAKKjowEAERERcHR01GuTl5eHkydPSm2ioqKg0Whw5MgRqc3hw4eh0WikNsbgkAEREclCU9+YaNq0afjggw/w+eefw93dXaoEqFQquLi4QKFQIDExEQsXLkSnTp3QqVMnLFy4EK6uroiLi5PaJiQkYNasWfDy8oKnpydmz56N8PBwDBw4EAAQGhqKIUOGYPLkyVi7di0AYMqUKRg6dKjRKwwAJgRERCQXf637N/R8E6xevRoA0L9/f739GzZswPjx4wEAL7zwAsrKyjB16lQUFBQgMjISu3btgru7u9R+2bJlcHBwwOjRo1FWVoYBAwYgJSUF9vb2UpstW7ZgxowZ0mqE4cOHY+XKlSbFy/sQEN3heB8CsmVNeR+CDilzYefq3OB+dKXl+G38G40aqzXxU5SIiIg4ZEBERPJw88ZE5p1vy5gQEBGRLPBph4ZxyICIiIhYISAiIpkQipubOefbMCYEREQkC5xDYBiHDIiIiIgVAiIikokmvjFRc2NUQvDOO+8Y3eGMGTMaHAwREVFj4SoDw4xKCJYtW2ZUZwqFggkBERFRM2RUQpCTk9PYcRARETU+Gy/7m6PBkworKytx5swZVFdXWzIeIiKiRlEzZGDOZstMTghKS0uRkJAAV1dXdO3aFRcuXABwc+7AokWLLB4gERGRRQgLbDbM5IRgzpw5+PHHH7F//344O//51KiBAwdi27ZtFg2OiIiImobJyw4/++wzbNu2Db1794ZC8Wf5pEuXLvj1118tGhwREZHlKP7YzDnfdpmcEFy5cgU+Pj619peUlOglCERERHcU3ofAIJOHDHr16oWvvvpKel2TBKxbtw5RUVGWi4yIiIiajMkVguTkZAwZMgSnT59GdXU13n77bZw6dQppaWk4cOBAY8RIRERkPlYIDDK5QhAdHY3vv/8epaWl6NixI3bt2gVfX1+kpaUhIiKiMWIkIiIyX83TDs3ZbFiDnmUQHh6OjRs3WjoWIiIispIGJQRarRbbt29HdnY2FAoFQkNDMWLECDg48FlJRER0Z+Ljjw0z+RP85MmTGDFiBNRqNUJCQgAAP//8M1q3bo0dO3YgPDzc4kESERGZjXMIDDJ5DsGkSZPQtWtXXLx4ET/88AN++OEH5Obm4p577sGUKVMaI0YiIiJqZCZXCH788UccPXoUrVq1kva1atUKb7zxBnr16mXR4IiIiCzG3ImBNj6p0OQKQUhICC5fvlxrf35+Pu666y6LBEVERGRpCmH+ZsuMqhAUFhZK/164cCFmzJiBpKQk9O7dGwCQnp6OV199FYsXL26cKImIiMzFOQQGGZUQtGzZUu+2xEIIjB49Wton/ph6OWzYMGi12kYIk4iIiBqTUQnBvn37GjsOIiKixsU5BAYZlRD069evseMgIiJqXBwyMKjBdxIqLS3FhQsXUFlZqbf/nnvuMTsoIiIialoNevzxhAkT8PXXX9d5nHMIiIjojsQKgUEmLztMTExEQUEB0tPT4eLigtTUVGzcuBGdOnXCjh07GiNGIiIi8wkLbDbM5ArB3r178fnnn6NXr16ws7NDUFAQBg0aBA8PDyQnJ+Ohhx5qjDiJiIioEZlcISgpKYGPjw8AwNPTE1euXAFw8wmIP/zwg2WjIyIishQ+/tigBt2p8MyZMwCAe++9F2vXrsXvv/+ONWvWwN/f3+IBEhERWQLvVGiYyUMGiYmJyMvLAwDMnz8fgwcPxpYtW+Dk5ISUlBRLx0dERERNwOSE4IknnpD+3b17d5w7dw4//fQT2rVrB29vb4sGR0REZDFcZWBQg+9DUMPV1RU9evSwRCxERERkJUYlBDNnzjS6w6VLlzY4GCIiosaigHnzAGx7SqGRCcGxY8eM6uyvD0AiIiKi5sMmHm70cOdwOCgcrR0GUaPY+XuGtUMgajSFRTp4hzTRxfhwI4PMnkNARETULHBSoUEm34eAiIiIbA8rBEREJA+sEBjEhICIiGTB3LsN2vqdCjlkQERERA1LCDZt2oS//e1vCAgIwPnz5wEAy5cvx+eff27R4IiIiCyGjz82yOSEYPXq1Zg5cyYefPBB3LhxA1qtFgDQsmVLLF++3NLxERERWQYTAoNMTghWrFiBdevWYe7cubC3t5f29+zZEydOnLBocERERNQ0TE4IcnJy0L1791r7lUolSkpKLBIUERGRpTX144+//fZbDBs2DAEBAVAoFPjss8/0jo8fPx4KhUJv6927t16biooKTJ8+Hd7e3nBzc8Pw4cNx8eJFvTYFBQWIj4+HSqWCSqVCfHw8bty4YfLPx+SEIDg4GFlZWbX2f/311+jSpYvJARARETWJmjsVmrOZoKSkBN26dcPKlSvrbTNkyBDk5eVJ286dO/WOJyYmYvv27di6dSsOHjyI4uJiDB06VBquB4C4uDhkZWUhNTUVqampyMrKQnx8vGk/GzRg2eHzzz+PadOmoby8HEIIHDlyBB9++CGSk5Px3nvvmRwAERFRk2ji+xDExsYiNjbWYBulUgk/P786j2k0Gqxfvx6bNm3CwIEDAQCbN29GYGAg9uzZg8GDByM7OxupqalIT09HZGQkAGDdunWIiorCmTNnEBJi/H2hTU4IJkyYgOrqarzwwgsoLS1FXFwc2rRpg7fffhtjx441tTsiIqJmpbCwUO+1UqmEUqlsUF/79++Hj48PWrZsiX79+uGNN96Aj48PACAzMxNVVVWIiYmR2gcEBCAsLAyHDh3C4MGDkZaWBpVKJSUDANC7d2+oVCocOnTIpISgQcsOJ0+ejPPnzyM/Px9qtRq5ublISEhoSFdERERNwlJzCAIDA6XxepVKheTk5AbFExsbiy1btmDv3r146623kJGRgQceeAAVFRUAALVaDScnJ7Rq1UrvPF9fX6jVaqlNTQLxVz4+PlIbY5l1p0Jvb29zTiciImo6FhoyyM3NhYeHh7S7odWBMWPGSP8OCwtDz549ERQUhK+++gqjRo2qPwwhoFD8OZ/hr/+ur40xTE4IgoODDV7kt99+M7VLIiKiZsPDw0MvIbAUf39/BAUF4ZdffgEA+Pn5obKyEgUFBXpVgvz8fERHR0ttLl++XKuvK1euwNfX16Trm5wQJCYm6r2uqqrCsWPHkJqaiueff97U7oiIiJqGmc8yaOwbE127dg25ubnw9/cHAERERMDR0RG7d+/G6NGjAQB5eXk4efIklixZAgCIioqCRqPBkSNHcN999wEADh8+DI1GIyUNxjI5IXjuuefq3P+f//wHR48eNbU7IiKiptHEqwyKi4tx9uxZ6XVOTg6ysrLg6ekJT09PJCUl4ZFHHoG/vz/OnTuHl19+Gd7e3nj44YcBACqVCgkJCZg1axa8vLzg6emJ2bNnIzw8XFp1EBoaiiFDhmDy5MlYu3YtAGDKlCkYOnSoSRMKAQs+3Cg2NhaffPKJpbojIiJq1o4ePYru3btLN/ObOXMmunfvjnnz5sHe3h4nTpzAiBEj0LlzZ4wbNw6dO3dGWloa3N3dpT6WLVuGkSNHYvTo0fjb3/4GV1dXfPHFF3p3Ct6yZQvCw8MRExODmJgY3HPPPdi0aZPJ8Vrs8ccff/wxPD09LdUdERGRZTVxhaB///4Qov6Tvvnmm9v24ezsjBUrVmDFihX1tvH09MTmzZtNC64OJicE3bt315tUKISAWq3GlStXsGrVKrMDIiIiagwNuf3wrefbMpMTgpEjR+q9trOzQ+vWrdG/f3/cfffdloqLiIiImpBJCUF1dTXat2+PwYMH13urRSIiImp+TJpU6ODggGeeeUa6ixIREVGzISyw2TCTVxlERkbi2LFjjRELERFRo2nqxx83NybPIZg6dSpmzZqFixcvIiIiAm5ubnrH77nnHosFR0RERE3D6IRg4sSJWL58uXTv5RkzZkjHFAqFdN/kvz6jmYiI6I5i49/yzWF0QrBx40YsWrQIOTk5jRkPERFR42ji+xA0N0YnBDU3VwgKCmq0YIiIiMg6TJpDYOqjFImIiO4UvDGRYSYlBJ07d75tUnD9+nWzAiIiImoUHDIwyKSEYMGCBVCpVI0VCxEREVmJSQnB2LFj4ePj01ixEBERNRoOGRhmdELA+QNERNSsccjAIKPvVGjoEY5ERETUvBldIdDpdI0ZBxERUeNihcAgk29dTERE1BxxDoFhTAiIiEgeWCEwyOSnHRIREZHtYYWAiIjkgRUCg5gQEBGRLHAOgWEcMiAiIiJWCIiISCY4ZGAQEwIiIpIFDhkYxiEDIiIiYoWAiIhkgkMGBjEhICIieWBCYBCHDIiIiIgVAiIikgfFH5s559syJgRERCQPHDIwiAkBERHJApcdGsY5BERERMQKARERyQSHDAxiQkBERPJh4x/q5uCQAREREbFCQERE8sBJhYYxISAiInngHAKDOGRARERErBAQEZE8cMjAMCYEREQkDxwyMIhDBkRERMQKARERyQOHDAxjQkBERPLAIQODmBAQEZE8MCEwiHMIiIiIiBUCIiKSB84hMIwJARERyQOHDAzikAERERGxQkBERPKgEAIK0fCv+eac2xywQkBERPIgLLCZ4Ntvv8WwYcMQEBAAhUKBzz77TD8cIZCUlISAgAC4uLigf//+OHXqlF6biooKTJ8+Hd7e3nBzc8Pw4cNx8eJFvTYFBQWIj4+HSqWCSqVCfHw8bty4YVqwYEJARETUKEpKStCtWzesXLmyzuNLlizB0qVLsXLlSmRkZMDPzw+DBg1CUVGR1CYxMRHbt2/H1q1bcfDgQRQXF2Po0KHQarVSm7i4OGRlZSE1NRWpqanIyspCfHy8yfFyyICIiGShqVcZxMbGIjY2ts5jQggsX74cc+fOxahRowAAGzduhK+vLz744AM89dRT0Gg0WL9+PTZt2oSBAwcCADZv3ozAwEDs2bMHgwcPRnZ2NlJTU5Geno7IyEgAwLp16xAVFYUzZ84gJCTE6HhZISAiInmw0JBBYWGh3lZRUWFyKDk5OVCr1YiJiZH2KZVK9OvXD4cOHQIAZGZmoqqqSq9NQEAAwsLCpDZpaWlQqVRSMgAAvXv3hkqlktoYiwkBERGRCQIDA6XxepVKheTkZJP7UKvVAABfX1+9/b6+vtIxtVoNJycntGrVymAbHx+fWv37+PhIbYzFIQMiIpIFSw0Z5ObmwsPDQ9qvVCob3qdCofdaCFFr361ubVNXe2P6uRUrBEREJA8WGjLw8PDQ2xqSEPj5+QFArW/x+fn5UtXAz88PlZWVKCgoMNjm8uXLtfq/cuVKrerD7TAhICIiWaipEJizWUpwcDD8/Pywe/duaV9lZSUOHDiA6OhoAEBERAQcHR312uTl5eHkyZNSm6ioKGg0Ghw5ckRqc/jwYWg0GqmNsThkQERE1AiKi4tx9uxZ6XVOTg6ysrLg6emJdu3aITExEQsXLkSnTp3QqVMnLFy4EK6uroiLiwMAqFQqJCQkYNasWfDy8oKnpydmz56N8PBwadVBaGgohgwZgsmTJ2Pt2rUAgClTpmDo0KEmrTAAmBAQEZFcNPGzDI4ePYr7779fej1z5kwAwLhx45CSkoIXXngBZWVlmDp1KgoKChAZGYldu3bB3d1dOmfZsmVwcHDA6NGjUVZWhgEDBiAlJQX29vZSmy1btmDGjBnSaoThw4fXe+8DQxRCNN97MRYWFkKlUqE/RsBB4WjtcIgaxc7ff7B2CESNprBIB++Qc9BoNHoT9Sx6jT8+KyJGvwEHR+cG91NdVY7Mj+Y2aqzWxDkERERExCEDIiKSCSFubuacb8OYEBARkSw09a2LmxsOGRARERErBEREJBNNvMqguWFCQEREsqDQ3dzMOd+WcciAiIiIWCGg2jYePg2/wKpa+3ekeGHNvDYY/2Ieej1QBP+gSpQU2uHYd+5Yv9Af1y/zXhBkfV9t9MZXm1rjcq4TACCocxke/6cavR4oBAB8v7Mlvt7sjbPHXVFY4IAV32SjY1iZdH5RgT02v+WPHw544OolJ3h4ViNqyA3EP38Jbh43vyIeP9QCLz3Wuc7rL//qJ3S+t7SR3yU1CIcMDLJqQvDtt9/izTffRGZmJvLy8rB9+3aMHDnSmiERgBmxnWFn/+dvfvu7y7Fo22/47ouWULrocFd4GT5Y7ovfTjujhUqLpxdcwoKUHEyPrfsPJFFT8vavwoQ5v8O//c1n1P/v/7zw2sQOWPHNTwgKKUd5qR269CrG34cW4J3ng2qdf+2yI65ddsSkV35Hu85luHzRCStfaodrakfMXZcDAAjtWYLNx47rnbfpzQBkfeeOTt2YDNypuMrAMKsmBCUlJejWrRsmTJiARx55xJqh0F9oruv/Wox5Nh+XcpxwPM0NgAJzxnbUO77qX22w4utf0LpNJa787tSEkRLVFhmj0Xs97qVL+GqTN376wQ1BIeUY8Oh1AJAqCLdqf3c5/vXHBz8A+LevxLgXL+HNGe2hrQbsHQBHJwFPn2qpTXUVcHiXCkPHX4GJT5ylpsT7EBhk1YQgNjYWsbGx1gyBbsPBUYcHHinAp2tbA6j7L52bhxY6HVCisa/zOJG1aLXAwS9bobzUDqERJQ3up6TIHq4ttLCv5y9m+q6WKLzugEGjrzX4GkTW1qzmEFRUVKCiokJ6XVhYaMVo5CF6SCFaeGix6yPPOo87KnWY+HIe9m1vidJiJgR0Z8jJdsas4SGorLCDi5sWr7z3G9p1Lm9QX4XX7fHhcj/EPnm13ja7tnqhR/9CtG5Te+4N3Tk4ZGBYs1plkJycDJVKJW2BgYHWDsnmDX78GjL2edQ5YdDeQeDl1eehsANWzmlrheiI6ta2YwVW7voJS784gwf/cRVvJQbhws+mP9SmtMgO8/9xF9p1LscTM/PqbHP1kiN+2O+BmLGsDtzxhAU2G9asEoI5c+ZAo9FIW25urrVDsmk+bSrRvU8xUj+oXR2wdxCYu/Yc/AIrMWdsB1YH6I7i6CQQEFyBzt1KMWHOJXToUobP32ttUh+lxXZ45Ym7pAqDQz2LaHZt84J7q2r0jrlhfuBEVtSshgyUSiWUSqW1w5CNmLHXceOqAw7v0X/MZ00y0Ca4Ei882hFFBc3q14hkSAigqtL47z+lRXb4V9xdcFQKzEv5FU7OdX81FALY85EXBjx6vd6Ege4cHDIwjH/JqU4KhUDMmOvY83+toNP+OZnQzl7glXXncFd4Geb9Ixh29gKtWt8cNy26YY/qqmZVdCIblJIcgJ4PaNA6oAqlxXb49nNPnEhzx6tbzgK4eZ+B/N+dpGGwi7/eHEpo5VMFT59qlBbbYe7jnVBRbofnV/yK0iJ7lBbd7FvlVQ37vxTDfjzoDvUFJWIe53BBs8BVBgZZNSEoLi7G2bNnpdc5OTnIysqCp6cn2rVrZ8XIqHvfYvi2rcI3W7309rf2r0LU4JuTOVfv+Vnv2POPdMTxtBZNFiNRXW5cdcC/Z7TH9XxHuLlrERxahle3nEWPvjc/1dN3qbBsZnup/eKpwQCAuJl5eHJWHs4ed8WZY24AgIS/hen1vSH9JHwDK6XX32z1QmjPYrTr1LAJi0R3EoUQ1kt59u/fj/vvv7/W/nHjxiElJeW25xcWFkKlUqE/RsBBwXod2aadv/9g7RCIGk1hkQ7eIeeg0Wjg4eFx+xMaco0/PiuiYl+Fg6Ppk0trVFeVI+3reY0aqzVZtULQv39/WDEfISIiOeGtiw3igC8RERFxUiEREckDVxkYxoSAiIjkQSdubuacb8OYEBARkTxwDoFBnENARERErBAQEZE8KGDmHAKLRXJnYkJARETywDsVGsQhAyIiImKFgIiI5IHLDg1jQkBERPLAVQYGcciAiIiIWCEgIiJ5UAgBhRkTA805tzlgQkBERPKg+2Mz53wbxiEDIiIiYoWAiIjkgUMGhjEhICIieeAqA4OYEBARkTzwToUGcQ4BERERsUJARETywDsVGsaEgIiI5IFDBgZxyICIiIhYISAiInlQ6G5u5pxvy5gQEBGRPHDIwCAOGRARERErBEREJBO8MZFBTAiIiEgWeOtiwzhkQERERKwQEBGRTHBSoUFMCIiISB4EAHOWDtp2PsCEgIiI5IFzCAzjHAIiIqJGkJSUBIVCobf5+flJx4UQSEpKQkBAAFxcXNC/f3+cOnVKr4+KigpMnz4d3t7ecHNzw/Dhw3Hx4sVGiZcJARERyYPAn/MIGrSZfsmuXbsiLy9P2k6cOCEdW7JkCZYuXYqVK1ciIyMDfn5+GDRoEIqKiqQ2iYmJ2L59O7Zu3YqDBw+iuLgYQ4cOhVartcAPRB+HDIiISB6sMKnQwcFBryrwZ1cCy5cvx9y5czFq1CgAwMaNG+Hr64sPPvgATz31FDQaDdavX49NmzZh4MCBAIDNmzcjMDAQe/bsweDBgxv+XurACgEREZEJCgsL9baKiop62/7yyy8ICAhAcHAwxo4di99++w0AkJOTA7VajZiYGKmtUqlEv379cOjQIQBAZmYmqqqq9NoEBAQgLCxMamNJTAiIiEgedBbYAAQGBkKlUklbcnJynZeLjIzEf//7X3zzzTdYt24d1Go1oqOjce3aNajVagCAr6+v3jm+vr7SMbVaDScnJ7Rq1areNpbEIQMiIpIFS60yyM3NhYeHh7RfqVTW2T42Nlb6d3h4OKKiotCxY0ds3LgRvXv3vtmnQqF3jhCi1r5bGdOmIVghICIiMoGHh4feVl9CcCs3NzeEh4fjl19+keYV3PpNPz8/X6oa+Pn5obKyEgUFBfW2sSQmBEREJA9mrTAwc0Iibi4hzM7Ohr+/P4KDg+Hn54fdu3dLxysrK3HgwAFER0cDACIiIuDo6KjXJi8vDydPnpTaWBKHDIiISB6aeJXB7NmzMWzYMLRr1w75+fl4/fXXUVhYiHHjxkGhUCAxMRELFy5Ep06d0KlTJyxcuBCurq6Ii4sDAKhUKiQkJGDWrFnw8vKCp6cnZs+ejfDwcGnVgSUxISAiImoEFy9exOOPP46rV6+idevW6N27N9LT0xEUFAQAeOGFF1BWVoapU6eioKAAkZGR2LVrF9zd3aU+li1bBgcHB4wePRplZWUYMGAAUlJSYG9vb/F4FUI033sxFhYWQqVSoT9GwEHhaO1wiBrFzt9/sHYIRI2msEgH75Bz0Gg0ehP1LHqNPz4rBoTOgoO9ceP9danWVuB/2W81aqzWxAoBERHJgw6AOZPzzXkwUjPAhICIiGSBDzcyjKsMiIiIiBUCIiKSCSs8y6A5YUJARETyoBOAwowPdZ1tJwQcMiAiIiJWCIiISCY4ZGAQEwIiIpIJc28/bNsJAYcMiIiIiBUCIiKSCQ4ZGMSEgIiI5EEnYFbZn6sMiIiIyNaxQkBERPIgdDc3c863YUwIiIhIHjiHwCAmBEREJA+cQ2AQ5xAQERERKwRERCQTHDIwiAkBERHJg4CZCYHFIrkjcciAiIiIWCEgIiKZ4JCBQUwIiIhIHnQ6AGbcS0Bn2/ch4JABERERsUJAREQywSEDg5gQEBGRPDAhMIhDBkRERMQKARERyQRvXWwQEwIiIpIFIXQQZjyx0JxzmwMmBEREJA9CmPctn3MIiIiIyNaxQkBERPIgzJxDYOMVAiYEREQkDzodoDBjHoCNzyHgkAERERGxQkBERDLBIQODmBAQEZEsCJ0OwowhA1tfdsghAyIiImKFgIiIZIJDBgYxISAiInnQCUDBhKA+HDIgIiIiVgiIiEgmhABgzn0IbLtCwISAiIhkQegEhBlDBoIJARERkQ0QOphXIeCyQyIiIrJxrBAQEZEscMjAMCYEREQkDxwyMKhZJwQ12Vo1qsy61wTRnaywyLb/CJG8FRXf/P1uim/f5n5WVKPKcsHcgZp1QlBUVAQAOIidVo6EqPF4h1g7AqLGV1RUBJVK1Sh9Ozk5wc/PDwfV5n9W+Pn5wcnJyQJR3XkUohkPiuh0Oly6dAnu7u5QKBTWDkcWCgsLERgYiNzcXHh4eFg7HCKL4u930xNCoKioCAEBAbCza7x57uXl5aisrDS7HycnJzg7O1sgojtPs64Q2NnZoW3bttYOQ5Y8PDz4B5NsFn+/m1ZjVQb+ytnZ2WY/yC2Fyw6JiIiICQERERExISATKZVKzJ8/H0ql0tqhEFkcf79Jzpr1pEIiIiKyDFYIiIiIiAkBERERMSEgIiIiMCEgIiIiMCEgE6xatQrBwcFwdnZGREQEvvvuO2uHRGQR3377LYYNG4aAgAAoFAp89tln1g6JqMkxISCjbNu2DYmJiZg7dy6OHTuGPn36IDY2FhcuXLB2aERmKykpQbdu3bBy5Uprh0JkNVx2SEaJjIxEjx49sHr1amlfaGgoRo4cieTkZCtGRmRZCoUC27dvx8iRI60dClGTYoWAbquyshKZmZmIiYnR2x8TE4NDhw5ZKSoiIrIkJgR0W1evXoVWq4Wvr6/efl9fX6jVaitFRURElsSEgIx26yOmhRB87DQRkY1gQkC35e3tDXt7+1rVgPz8/FpVAyIiap6YENBtOTk5ISIiArt379bbv3v3bkRHR1spKiIisiQHawdAzcPMmTMRHx+Pnj17IioqCu+++y4uXLiAp59+2tqhEZmtuLgYZ8+elV7n5OQgKysLnp6eaNeunRUjI2o6XHZIRlu1ahWWLFmCvLw8hIWFYdmyZejbt6+1wyIy2/79+3H//ffX2j9u3DikpKQ0fUBEVsCEgIiIiDiHgIiIiJgQEBEREZgQEBEREZgQEBEREZgQEBEREZgQEBEREZgQEBEREZgQEJktKSkJ9957r/R6/PjxGDlyZJPHce7cOSgUCmRlZdXbpn379li+fLnRfaakpKBly5Zmx6ZQKPDZZ5+Z3Q8RNR4mBGSTxo8fD4VCAYVCAUdHR3To0AGzZ89GSUlJo1/77bffNvrudsZ8iBMRNQU+y4Bs1pAhQ7BhwwZUVVXhu+++w6RJk1BSUoLVq1fXaltVVQVHR0eLXFelUlmkHyKipsQKAdkspVIJPz8/BAYGIi4uDk888YRUtq4p87///vvo0KEDlEolhBDQaDSYMmUKfHx84OHhgQceeAA//vijXr+LFi2Cr68v3N3dkZCQgPLycr3jtw4Z6HQ6LF68GHfddReUSiXatWuHN954AwAQHBwMAOjevTsUCgX69+8vnbdhwwaEhobC2dkZd999N1atWqV3nSNHjqB79+5wdnZGz549cezYMZN/RkuXLkV4eDjc3NwQGBiIqVOnori4uFa7zz77DJ07d4azszMGDRqE3NxcveNffPEFIiIi4OzsjA4dOmDBggWorq42OR4ish4mBCQbLi4uqKqqkl6fPXsWH330ET755BOpZP/QQw9BrVZj586dyMzMRI8ePTBgwABcv34dAPDRRx9h/vz5eOONN3D06FH4+/vX+qC+1Zw5c7B48WK88sorOH36ND744AP4+voCuPmhDgB79uxBXl4ePv30UwDAunXrMHfuXLzxxhvIzs7GwoUL8corr2Djxo0AgJKSEgwdOhQhISHIzMxEUlISZs+ebfLPxM7ODu+88w5OnjyJjRs3Yu/evXjhhRf02pSWluKNN97Axo0b8f3336OwsBBjx46Vjn/zzTd48sknMWPGDJw+fRpr165FSkqKlPQQUTMhiGzQuHHjxIgRI6TXhw8fFl5eXmL06NFCCCHmz58vHB0dRX5+vtTmf//7n/Dw8BDl5eV6fXXs2FGsXbtWCCFEVFSUePrpp/WOR0ZGim7dutV57cLCQqFUKsW6devqjDMnJ0cAEMeOHdPbHxgYKD744AO9fa+99pqIiooSQgixdu1a4enpKUpKSqTjq1evrrOvvwoKChLLli2r9/hHH30kvLy8pNcbNmwQAER6erq0Lzs7WwAQhw8fFkII0adPH7Fw4UK9fjZt2iT8/f2l1wDE9u3b670uEVkf5xCQzfryyy/RokULVFdXo6qqCiNGjMCKFSuk40FBQWjdurX0OjMzE8XFxfDy8tLrp6ysDL/++isAIDs7G08//bTe8aioKOzbt6/OGLKzs1FRUYEBAwYYHfeVK1eQm5uLhIQETJ48WdpfXV0tzU/Izs5Gt27d4OrqqheHqfbt24eFCxfi9OnTKCwsRHV1NcrLy1FSUgI3NzcAgIODA3r27Cmdc/fdd6Nly5bIzs7Gfffdh8zMTGRkZOhVBLRaLcrLy1FaWqoXIxHduZgQkM26//77sXr1ajg6OiIgIKDWpMGaD7waOp0O/v7+2L9/f62+Grr0zsXFxeRzdDodgJvDBpGRkXrH7O3tAQDCAk8tP3/+PB588EE8/fTTeO211+Dp6YmDBw8iISFBb2gFuLls8FY1+3Q6HRYsWIBRo0bVauPs7Gx2nETUNJgQkM1yc3PDXXfdZXT7Hj16QK1Ww8HBAe3bt6+zTWhoKNLT0/GPf/xD2peenl5vn506dYKLiwv+97//YdKkSbWOOzk5Abj5jbqGr68v2rRpg99++w1PPPFEnf126dIFmzZtQllZmZR0GIqjLkePHkV1dTXeeust2NndnE700Ucf1WpXXV2No0eP4r777gMAnDlzBjdu3MDdd98N4ObP7cyZMyb9rInozsOEgOgPAwcORFRUFEaOHInFixcjJCQEly5dws6dOzFy5Ej07NkTzz33HMaNG4eePXvi73//O7Zs2YJTp06hQ4cOdfbp7OyMF198ES+88AKcnJzwt7/9DVeuXMGpU6eQkJAAHx8fuLi4IDU1FW3btoWzszNUKhWSkpIwY8YMeHh4IDY2FhUVFTh69CgKCgowc+ZMxMXFYe7cuUhISMC//vUvnDt3Dv/+979Ner8dO3ZEdXU1VqxYgWHDhuH777/HmjVrarVzdHTE9OnT8c4778DR0RHPPvssevfuLSUI8+bNw9ChQxEYGIjHHnsMdnZ2OH78OE6cOIHXX3/d9P8RRGQVXGVA9AeFQoGdO3eib9++mDhxIjp37oyxY8fi3Llz0qqAMWPGYN68eXjxxRcRERGB8+fP45lnnjHY7yuvvIJZs2Zh3rx5CA0NxZgxY5Cfnw/g5vj8O++8g7Vr1yIgIAAjRowAAEyaNAnvvfceUlJSEB4ejn79+iElJUVaptiiRQt88cUXOH36NLp37465c+di8eLFJr3fe++9F0uXLsXixYsRFhaGLVu2IDk5uVY7V1dXvPjii4iLi0NUVBRcXFywdetW6fjgwYPx5ZdfYvfu3ejVqxd69+6NpUuXIigoyKR4iMi6FMISg5FERETUrLFCQEREREwIiIiIiAkBERERgQkBERERgQkBERERgQkBERERgQkBERERgQkBERERgQkBERERgQkBERERgQkBERERgQkBERERAfh/1JbUCjv9fDUAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "optimizer_bi_lstm = optim.Adam(bi_lstm.parameters(), lr=0.001)\n",
    "bi_lstm, bi_lstm_train_losses, bi_lstm_val_losses = train_model(bi_lstm, optimizer_bi_lstm, criterion, X_train, y_train, X_val, y_val)\n",
    "\n",
    "accuracy, report, conf_matrix = evaluate_model(bi_lstm, X_test, y_test)\n",
    "print_evaluation_metrics(accuracy, report, conf_matrix, bi_lstm_train_losses, bi_lstm_val_losses, \"Bi-LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9ab1148babcb51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
