{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d27257b93be61c17",
   "metadata": {},
   "source": [
    "## Bachelor Thesis\n",
    "## \"Exploring the Efficacy of Diverse Classification Techniques In Detecting Disinformation In News.\"\n",
    "Ilia Sokolovskiy\n",
    "HTW SS23\n",
    "\n",
    "Notebook 3/5 - LSTM & Bi-LSTM Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee2103461e11090",
   "metadata": {},
   "source": [
    "**Installing all necessary dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109df8850fb4855c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install numpy pandas matplotlib torch scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957f454062423f96",
   "metadata": {},
   "source": [
    "**Importing all necessary libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d83df6bb1d3660fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-19T03:50:46.896283700Z",
     "start_time": "2023-08-19T03:50:46.864946Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "from utils import split_data, NewsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d9562539afcba9",
   "metadata": {},
   "source": [
    "**Loading the prepared data frame from a pickle**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbbd30f24a5d7d6b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-16T14:49:46.752371600Z",
     "start_time": "2023-08-16T14:49:45.994583900Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the pickle with the df\n",
    "base_dir = \"Data\"\n",
    "pickle_folder = \"Pickles\"\n",
    "filename_pickle = \"pickle_lg_df_2\"\n",
    "\n",
    "full_path_pickle = os.path.join(base_dir, pickle_folder, filename_pickle)\n",
    "\n",
    "df = pd.read_pickle(full_path_pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "630129f2c059beaf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-16T14:49:48.080848500Z",
     "start_time": "2023-08-16T14:49:48.054204300Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>label_encoded</th>\n",
       "      <th>norp_count</th>\n",
       "      <th>gpe_count</th>\n",
       "      <th>vader_compound</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>original_text_vector</th>\n",
       "      <th>cleaned_text_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Donald Trump just couldn t wish all Americans ...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.8681</td>\n",
       "      <td>donald trump couldn t wish americans happy new...</td>\n",
       "      <td>[-1.6619356, -0.0073223817, -1.6303111, -0.190...</td>\n",
       "      <td>[-0.17023614, 1.1278214, -2.2035916, -1.195557...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>House Intelligence Committee Chairman Devin Nu...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.7141</td>\n",
       "      <td>house intelligence committee chairman devin nu...</td>\n",
       "      <td>[-2.008067, 0.6831929, -1.9811207, 0.52264357,...</td>\n",
       "      <td>[-0.3486856, 0.6266792, -1.7451725, 0.01966631...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>On Friday, it was revealed that former Milwauk...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.9953</td>\n",
       "      <td>friday reveal milwaukee sheriff david clarke c...</td>\n",
       "      <td>[-1.9425699, 0.0044210483, -1.7258451, 0.00323...</td>\n",
       "      <td>[-0.34773135, 0.7257386, -1.7822778, 0.2710289...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>On Christmas day, Donald Trump announced that ...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.9176</td>\n",
       "      <td>christmas day donald trump announce work follo...</td>\n",
       "      <td>[-1.6670086, 0.23368433, -0.6346163, 0.1001595...</td>\n",
       "      <td>[-0.18105617, 0.730818, -0.28500575, -0.608257...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pope Francis used his annual Christmas Day mes...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.3134</td>\n",
       "      <td>pope francis annual christmas day message rebu...</td>\n",
       "      <td>[-2.141846, 1.1239394, -2.4791837, 0.000615673...</td>\n",
       "      <td>[-0.003997393, 1.3095359, -2.2471106, -0.14267...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text label  label_encoded  \\\n",
       "0  Donald Trump just couldn t wish all Americans ...  FAKE              0   \n",
       "1  House Intelligence Committee Chairman Devin Nu...  FAKE              0   \n",
       "2  On Friday, it was revealed that former Milwauk...  FAKE              0   \n",
       "3  On Christmas day, Donald Trump announced that ...  FAKE              0   \n",
       "4  Pope Francis used his annual Christmas Day mes...  FAKE              0   \n",
       "\n",
       "   norp_count  gpe_count  vader_compound  \\\n",
       "0           3          3         -0.8681   \n",
       "1          10          5         -0.7141   \n",
       "2           1          4         -0.9953   \n",
       "3           0          2         -0.9176   \n",
       "4           2          5          0.3134   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  donald trump couldn t wish americans happy new...   \n",
       "1  house intelligence committee chairman devin nu...   \n",
       "2  friday reveal milwaukee sheriff david clarke c...   \n",
       "3  christmas day donald trump announce work follo...   \n",
       "4  pope francis annual christmas day message rebu...   \n",
       "\n",
       "                                original_text_vector  \\\n",
       "0  [-1.6619356, -0.0073223817, -1.6303111, -0.190...   \n",
       "1  [-2.008067, 0.6831929, -1.9811207, 0.52264357,...   \n",
       "2  [-1.9425699, 0.0044210483, -1.7258451, 0.00323...   \n",
       "3  [-1.6670086, 0.23368433, -0.6346163, 0.1001595...   \n",
       "4  [-2.141846, 1.1239394, -2.4791837, 0.000615673...   \n",
       "\n",
       "                                 cleaned_text_vector  \n",
       "0  [-0.17023614, 1.1278214, -2.2035916, -1.195557...  \n",
       "1  [-0.3486856, 0.6266792, -1.7451725, 0.01966631...  \n",
       "2  [-0.34773135, 0.7257386, -1.7822778, 0.2710289...  \n",
       "3  [-0.18105617, 0.730818, -0.28500575, -0.608257...  \n",
       "4  [-0.003997393, 1.3095359, -2.2471106, -0.14267...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83c473a351da70fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-16T14:49:49.002786200Z",
     "start_time": "2023-08-16T14:49:48.957010Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 64429 entries, 0 to 78616\n",
      "Data columns (total 9 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   text                  64429 non-null  object \n",
      " 1   label                 64429 non-null  object \n",
      " 2   label_encoded         64429 non-null  int64  \n",
      " 3   norp_count            64429 non-null  int64  \n",
      " 4   gpe_count             64429 non-null  int64  \n",
      " 5   vader_compound        64429 non-null  float64\n",
      " 6   cleaned_text          64429 non-null  object \n",
      " 7   original_text_vector  64429 non-null  object \n",
      " 8   cleaned_text_vector   64429 non-null  object \n",
      "dtypes: float64(1), int64(3), object(5)\n",
      "memory usage: 4.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac40aa71f15444c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectors of original texts: [-1.6619356  -0.00732238 -1.6303111  -0.19072783  2.7431827   0.07569402\n",
      "  0.71905965  3.1006029   0.05636455 -0.91815954] ...\n",
      "\n",
      "Vectors of cleaned texts: [-0.17023614  1.1278214  -2.2035916  -1.1955578   1.6034375   0.48677844\n",
      "  1.4271044   3.1684027  -1.089497   -1.0168833 ] ...\n",
      "\n",
      "Shape of the vectors created by spaCy: (300,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Vectors of original texts: {df['original_text_vector'].values[0][:10]} ...\")\n",
    "print(f\"\\nVectors of cleaned texts: {df['cleaned_text_vector'].values[0][:10]} ...\")\n",
    "print(f\"\\nShape of the vectors created by spaCy: {df['original_text_vector'].values[0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa9f5c8fab09840",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-16T14:49:58.915402700Z",
     "start_time": "2023-08-16T14:49:58.841379300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 51543\n",
      "Validation set: 6443\n",
      "Test set: 6443\n"
     ]
    }
   ],
   "source": [
    "# Data Split Variation Nr.1 (only Vectors of original texts)\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = split_data(df,['original_text_vector'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabcfbb31c5ab95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Split Variation Nr.2 (Number of Nationalities or Religious or Political Groups + Vader Compound + Vectors of original texts)\n",
    "# X_train, X_val, X_test, y_train, y_val, y_test = split_data(df,['norp_count', 'vader_compound', 'original_text_vector'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371d1c69df9eeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Split Variation Nr.3 (Number of Geopolitical Entities + Vader Compound + Vectors of original texts)\n",
    "# X_train, X_val, X_test, y_train, y_val, y_test = split_data(df,['gpe_count', 'vader_compound', 'original_text_vector'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11f9fbfa5430bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Split Variation Nr.4 (only Vectors of pre-processed texts)\n",
    "# X_train, X_val, X_test, y_train, y_val, y_test = split_data(df,['cleaned_text_vector'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cbb6a7b4200ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Split Variation Nr.5 (Number of Nationalities or Religious or Political Groups + Vader Compound + Vectors of pre-preocessed texts)\n",
    "# X_train, X_val, X_test, y_train, y_val, y_test = split_data(df,['norp_count', 'vader_compound', 'cleaned_text_vector'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96debebf13b7171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Split Variation Nr.6 (Number of Geopolitical Entities + Vader Compound + Vectors of pre-processed texts)\n",
    "# X_train, X_val, X_test, y_train, y_val, y_test = split_data(df,['gpe_count', 'vader_compound', 'cleaned_text_vector'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf83f9bedd3542d",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a65320917d0079e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-16T14:50:03.108808100Z",
     "start_time": "2023-08-16T14:50:03.081159200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device set to -> cuda\n"
     ]
    }
   ],
   "source": [
    "# Set device to GPU, if present\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device set to -> {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b434f39c527c23a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-16T14:50:33.312408100Z",
     "start_time": "2023-08-16T14:50:33.285137100Z"
    }
   },
   "outputs": [],
   "source": [
    "# Map tensors to the data splits\n",
    "X_train, X_val, X_test = map(torch.tensor, (X_train, X_val, X_test))\n",
    "y_train, y_val, y_test = map(torch.tensor, (y_train, y_val, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aeb33c2e133880b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-16T14:50:34.026627300Z",
     "start_time": "2023-08-16T14:50:34.006823400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([51543, 300])\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be19ae5d8024bd7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-16T14:50:34.706749600Z",
     "start_time": "2023-08-16T14:50:34.692083800Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add an empty sequence dimension so that the data fits LSTM\n",
    "X_train = X_train.unsqueeze(1)\n",
    "X_val = X_val.unsqueeze(1)\n",
    "X_test = X_test.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a6953a8df64e4621",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-16T14:50:36.254215100Z",
     "start_time": "2023-08-16T14:50:36.230571800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([51543, 1, 300])\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f1faec96ce2d042e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-16T14:50:41.432312800Z",
     "start_time": "2023-08-16T14:50:41.327796800Z"
    }
   },
   "outputs": [],
   "source": [
    "# Move the data to the active device set before (best case - GPU)\n",
    "X_train, X_val, X_test = X_train.to(device), X_val.to(device), X_test.to(device)\n",
    "y_train, y_val, y_test = y_train.to(device), y_val.to(device), y_test.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af4c718f32c29e9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-16T02:07:47.023395700Z",
     "start_time": "2023-08-16T02:07:46.595877300Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the models (see the custom class for default attribute values) and move them to the current device\n",
    "lstm = NewsClassifier(bidirectional=False).to(device)\n",
    "bi_lstm = NewsClassifier(bidirectional=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d5cffae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM Model Architecture:\n",
      "NewsClassifier(\n",
      "  (lstm): LSTM(300, 50, batch_first=True)\n",
      "  (fc): Linear(in_features=50, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "\n",
      "Bi-LSTM Model Architecture:\n",
      "NewsClassifier(\n",
      "  (lstm): LSTM(300, 50, batch_first=True, bidirectional=True)\n",
      "  (fc): Linear(in_features=100, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Print the architecture details\n",
    "print(\"LSTM Model Architecture:\")\n",
    "print(lstm)\n",
    "print(\"\\nBi-LSTM Model Architecture:\")\n",
    "print(bi_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cd6af8ea47131fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-16T02:07:47.560288600Z",
     "start_time": "2023-08-16T02:07:47.543647500Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the function for the loss calculation\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "455126e5d8b7c03d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-16T02:13:24.263547400Z",
     "start_time": "2023-08-16T02:13:24.253902300Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, criterion, X_train, y_train, X_val, y_val, num_epochs=2000):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    prev_val_loss = float('inf')\n",
    "    increasing_epochs = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train.float())\n",
    "        loss = criterion(outputs.squeeze(), y_train.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Validation Loss\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_val.float())\n",
    "            val_loss = criterion(val_outputs.squeeze(), y_val.float())\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "        val_losses.append(val_loss.item())\n",
    "\n",
    "        print(f\"Epoch: {epoch+1}/{num_epochs}, Train Loss: {loss.item()}, Validation Loss: {val_loss.item()}\")\n",
    "\n",
    "        # Check for increasing validation loss\n",
    "        if val_loss.item() > prev_val_loss:\n",
    "            increasing_epochs += 1\n",
    "        else:\n",
    "            increasing_epochs = 0\n",
    "\n",
    "        prev_val_loss = val_loss.item()\n",
    "\n",
    "        # Early stop\n",
    "        if increasing_epochs >= 3:\n",
    "            print(f\"\\nEarly stopping at epoch {epoch+1} due to validation loss increasing for 3 consecutive epochs.\")\n",
    "            break\n",
    "    return model, train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d58d87281f3e60bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-16T02:13:25.886963900Z",
     "start_time": "2023-08-16T02:13:25.867162500Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_outputs = model(X_test.float())\n",
    "        test_predictions = (test_outputs.squeeze() > 0.5).float()\n",
    "\n",
    "    accuracy = accuracy_score(y_test.cpu(), test_predictions.cpu())\n",
    "    report = classification_report(y_test.cpu(), test_predictions.cpu(), output_dict=True)\n",
    "    conf_matrix = confusion_matrix(y_test.cpu(), test_predictions.cpu())\n",
    "\n",
    "    return accuracy, report, conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9c93676d798f3ba8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-16T02:13:27.306607Z",
     "start_time": "2023-08-16T02:13:27.286727Z"
    }
   },
   "outputs": [],
   "source": [
    "def print_evaluation_metrics(accuracy, report, conf_matrix, train_losses, val_losses, model_name):\n",
    "    print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
    "    precision_0, precision_1 = report['0']['precision'], report['1']['precision']\n",
    "    recall_0, recall_1 = report['0']['recall'], report['1']['recall']\n",
    "    f1_0, f1_1 = report['0']['f1-score'], report['1']['f1-score']\n",
    "    print(f\"\\nClass 0 - Precision: {precision_0:.4f}, Recall: {recall_0:.4f}, F1 Score: {f1_0:.4f}\")\n",
    "    print(f\"Class 1 - Precision: {precision_1:.4f}, Recall: {recall_1:.4f}, F1 Score: {f1_1:.4f}\")\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(conf_matrix)\n",
    "    cm_display = ConfusionMatrixDisplay(conf_matrix).plot()\n",
    "    plt.title(\"Confusion Matrix - \" + model_name)\n",
    "    plt.show()\n",
    "    # Plotting the loss curves\n",
    "    plt.figure()\n",
    "    plt.plot(train_losses, label=\"Training Loss\")\n",
    "    plt.plot(val_losses, label=\"Validation Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(f\"{model_name} - Loss Curves\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "57de5ce7996004be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-16T02:13:47.372870Z",
     "start_time": "2023-08-16T02:13:28.563103800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2000, Train Loss: 0.6844332814216614, Validation Loss: 0.6736944317817688\n",
      "Epoch: 2/2000, Train Loss: 0.6731305122375488, Validation Loss: 0.6627963185310364\n",
      "Epoch: 3/2000, Train Loss: 0.6621474623680115, Validation Loss: 0.6526367664337158\n",
      "Epoch: 4/2000, Train Loss: 0.6519083976745605, Validation Loss: 0.6429540514945984\n",
      "Epoch: 5/2000, Train Loss: 0.6421332955360413, Validation Loss: 0.6331369280815125\n",
      "Epoch: 6/2000, Train Loss: 0.6321753263473511, Validation Loss: 0.6226779818534851\n",
      "Epoch: 7/2000, Train Loss: 0.6215511560440063, Validation Loss: 0.6113573908805847\n",
      "Epoch: 8/2000, Train Loss: 0.610100269317627, Validation Loss: 0.5995379090309143\n",
      "Epoch: 9/2000, Train Loss: 0.5982277393341064, Validation Loss: 0.5880106091499329\n",
      "Epoch: 10/2000, Train Loss: 0.5867345929145813, Validation Loss: 0.5772975087165833\n",
      "Epoch: 11/2000, Train Loss: 0.5761063694953918, Validation Loss: 0.5670292973518372\n",
      "Epoch: 12/2000, Train Loss: 0.5659083724021912, Validation Loss: 0.5567367076873779\n",
      "Epoch: 13/2000, Train Loss: 0.5556557774543762, Validation Loss: 0.5465772747993469\n",
      "Epoch: 14/2000, Train Loss: 0.5455145835876465, Validation Loss: 0.5366541743278503\n",
      "Epoch: 15/2000, Train Loss: 0.5355989336967468, Validation Loss: 0.5270475745201111\n",
      "Epoch: 16/2000, Train Loss: 0.5259954333305359, Validation Loss: 0.5179340839385986\n",
      "Epoch: 17/2000, Train Loss: 0.5168806314468384, Validation Loss: 0.5092793703079224\n",
      "Epoch: 18/2000, Train Loss: 0.5082280039787292, Validation Loss: 0.500898003578186\n",
      "Epoch: 19/2000, Train Loss: 0.499870240688324, Validation Loss: 0.49262571334838867\n",
      "Epoch: 20/2000, Train Loss: 0.49166983366012573, Validation Loss: 0.48443901538848877\n",
      "Epoch: 21/2000, Train Loss: 0.48360827565193176, Validation Loss: 0.47651657462120056\n",
      "Epoch: 22/2000, Train Loss: 0.4758298397064209, Validation Loss: 0.46902474761009216\n",
      "Epoch: 23/2000, Train Loss: 0.46846020221710205, Validation Loss: 0.4619733393192291\n",
      "Epoch: 24/2000, Train Loss: 0.4614975154399872, Validation Loss: 0.4552311897277832\n",
      "Epoch: 25/2000, Train Loss: 0.4548214077949524, Validation Loss: 0.4486132562160492\n",
      "Epoch: 26/2000, Train Loss: 0.4482560455799103, Validation Loss: 0.44205403327941895\n",
      "Epoch: 27/2000, Train Loss: 0.4417327642440796, Validation Loss: 0.4355909824371338\n",
      "Epoch: 28/2000, Train Loss: 0.43528592586517334, Validation Loss: 0.42928940057754517\n",
      "Epoch: 29/2000, Train Loss: 0.4289875626564026, Validation Loss: 0.42318010330200195\n",
      "Epoch: 30/2000, Train Loss: 0.4228762984275818, Validation Loss: 0.4172690510749817\n",
      "Epoch: 31/2000, Train Loss: 0.41696229577064514, Validation Loss: 0.411541610956192\n",
      "Epoch: 32/2000, Train Loss: 0.4112352430820465, Validation Loss: 0.40596309304237366\n",
      "Epoch: 33/2000, Train Loss: 0.40566733479499817, Validation Loss: 0.40049150586128235\n",
      "Epoch: 34/2000, Train Loss: 0.4002228081226349, Validation Loss: 0.39510390162467957\n",
      "Epoch: 35/2000, Train Loss: 0.39487791061401367, Validation Loss: 0.3898327946662903\n",
      "Epoch: 36/2000, Train Loss: 0.38965532183647156, Validation Loss: 0.38472092151641846\n",
      "Epoch: 37/2000, Train Loss: 0.3845879137516022, Validation Loss: 0.37978559732437134\n",
      "Epoch: 38/2000, Train Loss: 0.3796897530555725, Validation Loss: 0.37500739097595215\n",
      "Epoch: 39/2000, Train Loss: 0.3749392330646515, Validation Loss: 0.37036243081092834\n",
      "Epoch: 40/2000, Train Loss: 0.37030690908432007, Validation Loss: 0.3658292293548584\n",
      "Epoch: 41/2000, Train Loss: 0.3657669126987457, Validation Loss: 0.3613956570625305\n",
      "Epoch: 42/2000, Train Loss: 0.3613083064556122, Validation Loss: 0.35706546902656555\n",
      "Epoch: 43/2000, Train Loss: 0.35693955421447754, Validation Loss: 0.3528641164302826\n",
      "Epoch: 44/2000, Train Loss: 0.3526899814605713, Validation Loss: 0.34881365299224854\n",
      "Epoch: 45/2000, Train Loss: 0.34858494997024536, Validation Loss: 0.3449051082134247\n",
      "Epoch: 46/2000, Train Loss: 0.3446192443370819, Validation Loss: 0.3410993218421936\n",
      "Epoch: 47/2000, Train Loss: 0.34075841307640076, Validation Loss: 0.3373591899871826\n",
      "Epoch: 48/2000, Train Loss: 0.33697107434272766, Validation Loss: 0.33368852734565735\n",
      "Epoch: 49/2000, Train Loss: 0.3332626223564148, Validation Loss: 0.3301170766353607\n",
      "Epoch: 50/2000, Train Loss: 0.3296583294868469, Validation Loss: 0.3266619145870209\n",
      "Epoch: 51/2000, Train Loss: 0.3261701166629791, Validation Loss: 0.32331645488739014\n",
      "Epoch: 52/2000, Train Loss: 0.32278692722320557, Validation Loss: 0.3200699985027313\n",
      "Epoch: 53/2000, Train Loss: 0.3194938600063324, Validation Loss: 0.31691688299179077\n",
      "Epoch: 54/2000, Train Loss: 0.3162866234779358, Validation Loss: 0.3138542175292969\n",
      "Epoch: 55/2000, Train Loss: 0.31316646933555603, Validation Loss: 0.3108808100223541\n",
      "Epoch: 56/2000, Train Loss: 0.3101331889629364, Validation Loss: 0.30799195170402527\n",
      "Epoch: 57/2000, Train Loss: 0.3071818947792053, Validation Loss: 0.305182546377182\n",
      "Epoch: 58/2000, Train Loss: 0.3043048679828644, Validation Loss: 0.3024512529373169\n",
      "Epoch: 59/2000, Train Loss: 0.3014988601207733, Validation Loss: 0.2997923493385315\n",
      "Epoch: 60/2000, Train Loss: 0.2987665832042694, Validation Loss: 0.29719799757003784\n",
      "Epoch: 61/2000, Train Loss: 0.2961091101169586, Validation Loss: 0.2946634590625763\n",
      "Epoch: 62/2000, Train Loss: 0.29352161288261414, Validation Loss: 0.2921809256076813\n",
      "Epoch: 63/2000, Train Loss: 0.2909957766532898, Validation Loss: 0.28974947333335876\n",
      "Epoch: 64/2000, Train Loss: 0.28852730989456177, Validation Loss: 0.2873758375644684\n",
      "Epoch: 65/2000, Train Loss: 0.286119282245636, Validation Loss: 0.2850632667541504\n",
      "Epoch: 66/2000, Train Loss: 0.2837772071361542, Validation Loss: 0.2828133702278137\n",
      "Epoch: 67/2000, Train Loss: 0.28150075674057007, Validation Loss: 0.280622661113739\n",
      "Epoch: 68/2000, Train Loss: 0.2792828381061554, Validation Loss: 0.27848565578460693\n",
      "Epoch: 69/2000, Train Loss: 0.27711760997772217, Validation Loss: 0.2764061987400055\n",
      "Epoch: 70/2000, Train Loss: 0.2750045657157898, Validation Loss: 0.2743850350379944\n",
      "Epoch: 71/2000, Train Loss: 0.2729451358318329, Validation Loss: 0.2724182903766632\n",
      "Epoch: 72/2000, Train Loss: 0.2709389328956604, Validation Loss: 0.27050134539604187\n",
      "Epoch: 73/2000, Train Loss: 0.26898330450057983, Validation Loss: 0.26862651109695435\n",
      "Epoch: 74/2000, Train Loss: 0.2670750617980957, Validation Loss: 0.2667941153049469\n",
      "Epoch: 75/2000, Train Loss: 0.2652123272418976, Validation Loss: 0.2650046646595001\n",
      "Epoch: 76/2000, Train Loss: 0.26339417695999146, Validation Loss: 0.2632579207420349\n",
      "Epoch: 77/2000, Train Loss: 0.26161956787109375, Validation Loss: 0.2615545094013214\n",
      "Epoch: 78/2000, Train Loss: 0.2598867416381836, Validation Loss: 0.259891539812088\n",
      "Epoch: 79/2000, Train Loss: 0.2581935524940491, Validation Loss: 0.258271187543869\n",
      "Epoch: 80/2000, Train Loss: 0.2565388083457947, Validation Loss: 0.25669074058532715\n",
      "Epoch: 81/2000, Train Loss: 0.2549218237400055, Validation Loss: 0.2551494240760803\n",
      "Epoch: 82/2000, Train Loss: 0.25334104895591736, Validation Loss: 0.25364330410957336\n",
      "Epoch: 83/2000, Train Loss: 0.2517947554588318, Validation Loss: 0.2521713674068451\n",
      "Epoch: 84/2000, Train Loss: 0.25028151273727417, Validation Loss: 0.2507314383983612\n",
      "Epoch: 85/2000, Train Loss: 0.24880044162273407, Validation Loss: 0.24932238459587097\n",
      "Epoch: 86/2000, Train Loss: 0.24735060334205627, Validation Loss: 0.24794332683086395\n",
      "Epoch: 87/2000, Train Loss: 0.24593104422092438, Validation Loss: 0.24659359455108643\n",
      "Epoch: 88/2000, Train Loss: 0.24454078078269958, Validation Loss: 0.24527284502983093\n",
      "Epoch: 89/2000, Train Loss: 0.2431788295507431, Validation Loss: 0.24397845566272736\n",
      "Epoch: 90/2000, Train Loss: 0.24184419214725494, Validation Loss: 0.24271118640899658\n",
      "Epoch: 91/2000, Train Loss: 0.24053597450256348, Validation Loss: 0.24146762490272522\n",
      "Epoch: 92/2000, Train Loss: 0.23925358057022095, Validation Loss: 0.2402534782886505\n",
      "Epoch: 93/2000, Train Loss: 0.23799625039100647, Validation Loss: 0.23905786871910095\n",
      "Epoch: 94/2000, Train Loss: 0.23676373064517975, Validation Loss: 0.23790249228477478\n",
      "Epoch: 95/2000, Train Loss: 0.23555748164653778, Validation Loss: 0.2367558777332306\n",
      "Epoch: 96/2000, Train Loss: 0.23438675701618195, Validation Loss: 0.2357289344072342\n",
      "Epoch: 97/2000, Train Loss: 0.23328207433223724, Validation Loss: 0.23475226759910583\n",
      "Epoch: 98/2000, Train Loss: 0.23233051598072052, Validation Loss: 0.23403669893741608\n",
      "Epoch: 99/2000, Train Loss: 0.23147530853748322, Validation Loss: 0.23290182650089264\n",
      "Epoch: 100/2000, Train Loss: 0.23041601479053497, Validation Loss: 0.2315753996372223\n",
      "Epoch: 101/2000, Train Loss: 0.22897472977638245, Validation Loss: 0.23063646256923676\n",
      "Epoch: 102/2000, Train Loss: 0.22799226641654968, Validation Loss: 0.22990109026432037\n",
      "Epoch: 103/2000, Train Loss: 0.22729289531707764, Validation Loss: 0.22885961830615997\n",
      "Epoch: 104/2000, Train Loss: 0.22612662613391876, Validation Loss: 0.22778739035129547\n",
      "Epoch: 105/2000, Train Loss: 0.22504396736621857, Validation Loss: 0.22706563770771027\n",
      "Epoch: 106/2000, Train Loss: 0.22434042394161224, Validation Loss: 0.22620336711406708\n",
      "Epoch: 107/2000, Train Loss: 0.2233627438545227, Validation Loss: 0.22516965866088867\n",
      "Epoch: 108/2000, Train Loss: 0.2223273068666458, Validation Loss: 0.22443966567516327\n",
      "Epoch: 109/2000, Train Loss: 0.2215985506772995, Validation Loss: 0.2236795574426651\n",
      "Epoch: 110/2000, Train Loss: 0.22072628140449524, Validation Loss: 0.22271287441253662\n",
      "Epoch: 111/2000, Train Loss: 0.21976064145565033, Validation Loss: 0.22197996079921722\n",
      "Epoch: 112/2000, Train Loss: 0.21902017295360565, Validation Loss: 0.22128023207187653\n",
      "Epoch: 113/2000, Train Loss: 0.2182181030511856, Validation Loss: 0.22037415206432343\n",
      "Epoch: 114/2000, Train Loss: 0.21731795370578766, Validation Loss: 0.21964885294437408\n",
      "Epoch: 115/2000, Train Loss: 0.21657462418079376, Validation Loss: 0.21899807453155518\n",
      "Epoch: 116/2000, Train Loss: 0.2158258557319641, Validation Loss: 0.2181515395641327\n",
      "Epoch: 117/2000, Train Loss: 0.21498526632785797, Validation Loss: 0.21744106709957123\n",
      "Epoch: 118/2000, Train Loss: 0.2142431139945984, Validation Loss: 0.2168232947587967\n",
      "Epoch: 119/2000, Train Loss: 0.21353580057621002, Validation Loss: 0.2160273939371109\n",
      "Epoch: 120/2000, Train Loss: 0.21275220811367035, Validation Loss: 0.21533161401748657\n",
      "Epoch: 121/2000, Train Loss: 0.21201445162296295, Validation Loss: 0.2147289216518402\n",
      "Epoch: 122/2000, Train Loss: 0.21133562922477722, Validation Loss: 0.21398715674877167\n",
      "Epoch: 123/2000, Train Loss: 0.21060802042484283, Validation Loss: 0.21331968903541565\n",
      "Epoch: 124/2000, Train Loss: 0.20988190174102783, Validation Loss: 0.212717667222023\n",
      "Epoch: 125/2000, Train Loss: 0.20921719074249268, Validation Loss: 0.21203121542930603\n",
      "Epoch: 126/2000, Train Loss: 0.20854097604751587, Validation Loss: 0.21140208840370178\n",
      "Epoch: 127/2000, Train Loss: 0.2078397125005722, Validation Loss: 0.2107822746038437\n",
      "Epoch: 128/2000, Train Loss: 0.20717862248420715, Validation Loss: 0.21014264225959778\n",
      "Epoch: 129/2000, Train Loss: 0.20653995871543884, Validation Loss: 0.20956048369407654\n",
      "Epoch: 130/2000, Train Loss: 0.20587821304798126, Validation Loss: 0.20892517268657684\n",
      "Epoch: 131/2000, Train Loss: 0.20522339642047882, Validation Loss: 0.2083227038383484\n",
      "Epoch: 132/2000, Train Loss: 0.2046014368534088, Validation Loss: 0.20777884125709534\n",
      "Epoch: 133/2000, Train Loss: 0.2039816975593567, Validation Loss: 0.20714892446994781\n",
      "Epoch: 134/2000, Train Loss: 0.20335040986537933, Validation Loss: 0.2065756767988205\n",
      "Epoch: 135/2000, Train Loss: 0.20273324847221375, Validation Loss: 0.206035315990448\n",
      "Epoch: 136/2000, Train Loss: 0.2021375447511673, Validation Loss: 0.20543789863586426\n",
      "Epoch: 137/2000, Train Loss: 0.20154213905334473, Validation Loss: 0.2049017995595932\n",
      "Epoch: 138/2000, Train Loss: 0.20094159245491028, Validation Loss: 0.20433832705020905\n",
      "Epoch: 139/2000, Train Loss: 0.2003525346517563, Validation Loss: 0.20378151535987854\n",
      "Epoch: 140/2000, Train Loss: 0.19977931678295135, Validation Loss: 0.20327630639076233\n",
      "Epoch: 141/2000, Train Loss: 0.19920943677425385, Validation Loss: 0.20270735025405884\n",
      "Epoch: 142/2000, Train Loss: 0.1986377090215683, Validation Loss: 0.20218916237354279\n",
      "Epoch: 143/2000, Train Loss: 0.19807229936122894, Validation Loss: 0.2016761749982834\n",
      "Epoch: 144/2000, Train Loss: 0.19751916825771332, Validation Loss: 0.201140359044075\n",
      "Epoch: 145/2000, Train Loss: 0.19697368144989014, Validation Loss: 0.20065900683403015\n",
      "Epoch: 146/2000, Train Loss: 0.19642950594425201, Validation Loss: 0.20012681186199188\n",
      "Epoch: 147/2000, Train Loss: 0.19588759541511536, Validation Loss: 0.19963352382183075\n",
      "Epoch: 148/2000, Train Loss: 0.19535282254219055, Validation Loss: 0.19914843142032623\n",
      "Epoch: 149/2000, Train Loss: 0.1948269009590149, Validation Loss: 0.198640838265419\n",
      "Epoch: 150/2000, Train Loss: 0.19430701434612274, Validation Loss: 0.1981838047504425\n",
      "Epoch: 151/2000, Train Loss: 0.19379007816314697, Validation Loss: 0.19768059253692627\n",
      "Epoch: 152/2000, Train Loss: 0.19327610731124878, Validation Loss: 0.197220116853714\n",
      "Epoch: 153/2000, Train Loss: 0.19276711344718933, Validation Loss: 0.1967499852180481\n",
      "Epoch: 154/2000, Train Loss: 0.19226467609405518, Validation Loss: 0.19627796113491058\n",
      "Epoch: 155/2000, Train Loss: 0.19176843762397766, Validation Loss: 0.19583922624588013\n",
      "Epoch: 156/2000, Train Loss: 0.19127710163593292, Validation Loss: 0.19536232948303223\n",
      "Epoch: 157/2000, Train Loss: 0.19078955054283142, Validation Loss: 0.19493494927883148\n",
      "Epoch: 158/2000, Train Loss: 0.19030553102493286, Validation Loss: 0.19446928799152374\n",
      "Epoch: 159/2000, Train Loss: 0.18982554972171783, Validation Loss: 0.19403964281082153\n",
      "Epoch: 160/2000, Train Loss: 0.1893501579761505, Validation Loss: 0.19359658658504486\n",
      "Epoch: 161/2000, Train Loss: 0.18887962400913239, Validation Loss: 0.19316090643405914\n",
      "Epoch: 162/2000, Train Loss: 0.1884138137102127, Validation Loss: 0.19274085760116577\n",
      "Epoch: 163/2000, Train Loss: 0.18795233964920044, Validation Loss: 0.19230128824710846\n",
      "Epoch: 164/2000, Train Loss: 0.18749472498893738, Validation Loss: 0.19189947843551636\n",
      "Epoch: 165/2000, Train Loss: 0.18704059720039368, Validation Loss: 0.1914595663547516\n",
      "Epoch: 166/2000, Train Loss: 0.18659022450447083, Validation Loss: 0.1910722404718399\n",
      "Epoch: 167/2000, Train Loss: 0.18614403903484344, Validation Loss: 0.19063378870487213\n",
      "Epoch: 168/2000, Train Loss: 0.18570175766944885, Validation Loss: 0.19026166200637817\n",
      "Epoch: 169/2000, Train Loss: 0.18526290357112885, Validation Loss: 0.18982383608818054\n",
      "Epoch: 170/2000, Train Loss: 0.18482699990272522, Validation Loss: 0.18947508931159973\n",
      "Epoch: 171/2000, Train Loss: 0.18439197540283203, Validation Loss: 0.18903669714927673\n",
      "Epoch: 172/2000, Train Loss: 0.18395787477493286, Validation Loss: 0.1887369453907013\n",
      "Epoch: 173/2000, Train Loss: 0.1835329532623291, Validation Loss: 0.188301220536232\n",
      "Epoch: 174/2000, Train Loss: 0.18314401805400848, Validation Loss: 0.18808194994926453\n",
      "Epoch: 175/2000, Train Loss: 0.18276524543762207, Validation Loss: 0.18759259581565857\n",
      "Epoch: 176/2000, Train Loss: 0.18243536353111267, Validation Loss: 0.187602236866951\n",
      "Epoch: 177/2000, Train Loss: 0.18216121196746826, Validation Loss: 0.18708129227161407\n",
      "Epoch: 178/2000, Train Loss: 0.1819353699684143, Validation Loss: 0.18711547553539276\n",
      "Epoch: 179/2000, Train Loss: 0.18156443536281586, Validation Loss: 0.18624164164066315\n",
      "Epoch: 180/2000, Train Loss: 0.18099352717399597, Validation Loss: 0.18582770228385925\n",
      "Epoch: 181/2000, Train Loss: 0.18031921982765198, Validation Loss: 0.18537969887256622\n",
      "Epoch: 182/2000, Train Loss: 0.17987340688705444, Validation Loss: 0.18505340814590454\n",
      "Epoch: 183/2000, Train Loss: 0.17966201901435852, Validation Loss: 0.18508005142211914\n",
      "Epoch: 184/2000, Train Loss: 0.17940153181552887, Validation Loss: 0.1843775361776352\n",
      "Epoch: 185/2000, Train Loss: 0.17893479764461517, Validation Loss: 0.18401913344860077\n",
      "Epoch: 186/2000, Train Loss: 0.17837809026241302, Validation Loss: 0.18365418910980225\n",
      "Epoch: 187/2000, Train Loss: 0.1779915988445282, Validation Loss: 0.18330425024032593\n",
      "Epoch: 188/2000, Train Loss: 0.17774760723114014, Validation Loss: 0.18323318660259247\n",
      "Epoch: 189/2000, Train Loss: 0.17742134630680084, Validation Loss: 0.18261298537254333\n",
      "Epoch: 190/2000, Train Loss: 0.17696048319339752, Validation Loss: 0.182292640209198\n",
      "Epoch: 191/2000, Train Loss: 0.17651218175888062, Validation Loss: 0.1820579320192337\n",
      "Epoch: 192/2000, Train Loss: 0.176191046833992, Validation Loss: 0.18167120218276978\n",
      "Epoch: 193/2000, Train Loss: 0.17591024935245514, Validation Loss: 0.18152424693107605\n",
      "Epoch: 194/2000, Train Loss: 0.17553997039794922, Validation Loss: 0.1809949427843094\n",
      "Epoch: 195/2000, Train Loss: 0.17510701715946198, Validation Loss: 0.18068699538707733\n",
      "Epoch: 196/2000, Train Loss: 0.17471858859062195, Validation Loss: 0.18047794699668884\n",
      "Epoch: 197/2000, Train Loss: 0.17440630495548248, Validation Loss: 0.1800735741853714\n",
      "Epoch: 198/2000, Train Loss: 0.17409519851207733, Validation Loss: 0.17988678812980652\n",
      "Epoch: 199/2000, Train Loss: 0.1737229973077774, Validation Loss: 0.17942911386489868\n",
      "Epoch: 200/2000, Train Loss: 0.17332717776298523, Validation Loss: 0.17912283539772034\n",
      "Epoch: 201/2000, Train Loss: 0.1729685217142105, Validation Loss: 0.17889411747455597\n",
      "Epoch: 202/2000, Train Loss: 0.17265130579471588, Validation Loss: 0.1784871369600296\n",
      "Epoch: 203/2000, Train Loss: 0.17233064770698547, Validation Loss: 0.17828424274921417\n",
      "Epoch: 204/2000, Train Loss: 0.1719772070646286, Validation Loss: 0.1778489351272583\n",
      "Epoch: 205/2000, Train Loss: 0.17160747945308685, Validation Loss: 0.17756164073944092\n",
      "Epoch: 206/2000, Train Loss: 0.17125201225280762, Validation Loss: 0.17728543281555176\n",
      "Epoch: 207/2000, Train Loss: 0.17092262208461761, Validation Loss: 0.17691662907600403\n",
      "Epoch: 208/2000, Train Loss: 0.17060668766498566, Validation Loss: 0.17671914398670197\n",
      "Epoch: 209/2000, Train Loss: 0.17028212547302246, Validation Loss: 0.17629273235797882\n",
      "Epoch: 210/2000, Train Loss: 0.16994211077690125, Validation Loss: 0.17605777084827423\n",
      "Epoch: 211/2000, Train Loss: 0.16959550976753235, Validation Loss: 0.175698921084404\n",
      "Epoch: 212/2000, Train Loss: 0.1692548543214798, Validation Loss: 0.17540551722049713\n",
      "Epoch: 213/2000, Train Loss: 0.1689261496067047, Validation Loss: 0.17515647411346436\n",
      "Epoch: 214/2000, Train Loss: 0.1686081886291504, Validation Loss: 0.1747981309890747\n",
      "Epoch: 215/2000, Train Loss: 0.16829420626163483, Validation Loss: 0.1746039241552353\n",
      "Epoch: 216/2000, Train Loss: 0.16797827184200287, Validation Loss: 0.17420926690101624\n",
      "Epoch: 217/2000, Train Loss: 0.16765867173671722, Validation Loss: 0.17401725053787231\n",
      "Epoch: 218/2000, Train Loss: 0.1673341989517212, Validation Loss: 0.17363128066062927\n",
      "Epoch: 219/2000, Train Loss: 0.16700780391693115, Validation Loss: 0.17341011762619019\n",
      "Epoch: 220/2000, Train Loss: 0.16668304800987244, Validation Loss: 0.1730634868144989\n",
      "Epoch: 221/2000, Train Loss: 0.16636113822460175, Validation Loss: 0.17280353605747223\n",
      "Epoch: 222/2000, Train Loss: 0.16604265570640564, Validation Loss: 0.17250457406044006\n",
      "Epoch: 223/2000, Train Loss: 0.16572780907154083, Validation Loss: 0.17221105098724365\n",
      "Epoch: 224/2000, Train Loss: 0.16541528701782227, Validation Loss: 0.17195309698581696\n",
      "Epoch: 225/2000, Train Loss: 0.16510504484176636, Validation Loss: 0.17162764072418213\n",
      "Epoch: 226/2000, Train Loss: 0.1647973656654358, Validation Loss: 0.1714112013578415\n",
      "Epoch: 227/2000, Train Loss: 0.16449278593063354, Validation Loss: 0.17104673385620117\n",
      "Epoch: 228/2000, Train Loss: 0.16419373452663422, Validation Loss: 0.17089499533176422\n",
      "Epoch: 229/2000, Train Loss: 0.16390378773212433, Validation Loss: 0.17047686874866486\n",
      "Epoch: 230/2000, Train Loss: 0.1636301428079605, Validation Loss: 0.17046836018562317\n",
      "Epoch: 231/2000, Train Loss: 0.16338391602039337, Validation Loss: 0.16999031603336334\n",
      "Epoch: 232/2000, Train Loss: 0.16317610442638397, Validation Loss: 0.17019382119178772\n",
      "Epoch: 233/2000, Train Loss: 0.16299432516098022, Validation Loss: 0.16959527134895325\n",
      "Epoch: 234/2000, Train Loss: 0.16280321776866913, Validation Loss: 0.16974416375160217\n",
      "Epoch: 235/2000, Train Loss: 0.1624893695116043, Validation Loss: 0.1689319759607315\n",
      "Epoch: 236/2000, Train Loss: 0.16205638647079468, Validation Loss: 0.1687421351671219\n",
      "Epoch: 237/2000, Train Loss: 0.16158339381217957, Validation Loss: 0.16832351684570312\n",
      "Epoch: 238/2000, Train Loss: 0.16123348474502563, Validation Loss: 0.16803237795829773\n",
      "Epoch: 239/2000, Train Loss: 0.16103041172027588, Validation Loss: 0.16814163327217102\n",
      "Epoch: 240/2000, Train Loss: 0.16086868941783905, Validation Loss: 0.1676000952720642\n",
      "Epoch: 241/2000, Train Loss: 0.16063393652439117, Validation Loss: 0.16756823658943176\n",
      "Epoch: 242/2000, Train Loss: 0.16027648746967316, Validation Loss: 0.1669957935810089\n",
      "Epoch: 243/2000, Train Loss: 0.15989236533641815, Validation Loss: 0.16675911843776703\n",
      "Epoch: 244/2000, Train Loss: 0.15958437323570251, Validation Loss: 0.16665469110012054\n",
      "Epoch: 245/2000, Train Loss: 0.15936774015426636, Validation Loss: 0.16625620424747467\n",
      "Epoch: 246/2000, Train Loss: 0.1591687649488449, Validation Loss: 0.1662755310535431\n",
      "Epoch: 247/2000, Train Loss: 0.15890859067440033, Validation Loss: 0.16573740541934967\n",
      "Epoch: 248/2000, Train Loss: 0.15858793258666992, Validation Loss: 0.16557690501213074\n",
      "Epoch: 249/2000, Train Loss: 0.15825991332530975, Validation Loss: 0.16528651118278503\n",
      "Epoch: 250/2000, Train Loss: 0.1579817682504654, Validation Loss: 0.16498544812202454\n",
      "Epoch: 251/2000, Train Loss: 0.15775270760059357, Validation Loss: 0.1649502068758011\n",
      "Epoch: 252/2000, Train Loss: 0.15752942860126495, Validation Loss: 0.16450247168540955\n",
      "Epoch: 253/2000, Train Loss: 0.1572759449481964, Validation Loss: 0.16442400217056274\n",
      "Epoch: 254/2000, Train Loss: 0.15698620676994324, Validation Loss: 0.16401340067386627\n",
      "Epoch: 255/2000, Train Loss: 0.15669067203998566, Validation Loss: 0.16380688548088074\n",
      "Epoch: 256/2000, Train Loss: 0.15641707181930542, Validation Loss: 0.16361753642559052\n",
      "Epoch: 257/2000, Train Loss: 0.1561715006828308, Validation Loss: 0.16328907012939453\n",
      "Epoch: 258/2000, Train Loss: 0.15593862533569336, Validation Loss: 0.16322031617164612\n",
      "Epoch: 259/2000, Train Loss: 0.1556980013847351, Validation Loss: 0.16281206905841827\n",
      "Epoch: 260/2000, Train Loss: 0.15544100105762482, Validation Loss: 0.16270773112773895\n",
      "Epoch: 261/2000, Train Loss: 0.15517045557498932, Validation Loss: 0.16234992444515228\n",
      "Epoch: 262/2000, Train Loss: 0.15489961206912994, Validation Loss: 0.16215571761131287\n",
      "Epoch: 263/2000, Train Loss: 0.15463842451572418, Validation Loss: 0.16193333268165588\n",
      "Epoch: 264/2000, Train Loss: 0.1543898731470108, Validation Loss: 0.16164690256118774\n",
      "Epoch: 265/2000, Train Loss: 0.1541501134634018, Validation Loss: 0.1615263670682907\n",
      "Epoch: 266/2000, Train Loss: 0.1539125144481659, Validation Loss: 0.16117390990257263\n",
      "Epoch: 267/2000, Train Loss: 0.15367215871810913, Validation Loss: 0.16108128428459167\n",
      "Epoch: 268/2000, Train Loss: 0.153426393866539, Validation Loss: 0.16071417927742004\n",
      "Epoch: 269/2000, Train Loss: 0.15317651629447937, Validation Loss: 0.16059619188308716\n",
      "Epoch: 270/2000, Train Loss: 0.15292415022850037, Validation Loss: 0.16026292741298676\n",
      "Epoch: 271/2000, Train Loss: 0.1526724249124527, Validation Loss: 0.16009745001792908\n",
      "Epoch: 272/2000, Train Loss: 0.15242312848567963, Validation Loss: 0.15982352197170258\n",
      "Epoch: 273/2000, Train Loss: 0.15217718482017517, Validation Loss: 0.15960954129695892\n",
      "Epoch: 274/2000, Train Loss: 0.1519344002008438, Validation Loss: 0.15939435362815857\n",
      "Epoch: 275/2000, Train Loss: 0.15169432759284973, Validation Loss: 0.15913763642311096\n",
      "Epoch: 276/2000, Train Loss: 0.15145641565322876, Validation Loss: 0.1589699238538742\n",
      "Epoch: 277/2000, Train Loss: 0.15122030675411224, Validation Loss: 0.15867476165294647\n",
      "Epoch: 278/2000, Train Loss: 0.15098610520362854, Validation Loss: 0.15855209529399872\n",
      "Epoch: 279/2000, Train Loss: 0.15075425803661346, Validation Loss: 0.15821799635887146\n",
      "Epoch: 280/2000, Train Loss: 0.1505260318517685, Validation Loss: 0.1581563949584961\n",
      "Epoch: 281/2000, Train Loss: 0.1503029614686966, Validation Loss: 0.15777283906936646\n",
      "Epoch: 282/2000, Train Loss: 0.15008895099163055, Validation Loss: 0.15781269967556\n",
      "Epoch: 283/2000, Train Loss: 0.14988724887371063, Validation Loss: 0.15736129879951477\n",
      "Epoch: 284/2000, Train Loss: 0.14970679581165314, Validation Loss: 0.1575678288936615\n",
      "Epoch: 285/2000, Train Loss: 0.14954781532287598, Validation Loss: 0.15702567994594574\n",
      "Epoch: 286/2000, Train Loss: 0.14941787719726562, Validation Loss: 0.15739065408706665\n",
      "Epoch: 287/2000, Train Loss: 0.14927399158477783, Validation Loss: 0.15668991208076477\n",
      "Epoch: 288/2000, Train Loss: 0.14909273386001587, Validation Loss: 0.15691602230072021\n",
      "Epoch: 289/2000, Train Loss: 0.1487841010093689, Validation Loss: 0.15612083673477173\n",
      "Epoch: 290/2000, Train Loss: 0.14840301871299744, Validation Loss: 0.1560315489768982\n",
      "Epoch: 291/2000, Train Loss: 0.14802846312522888, Validation Loss: 0.15571263432502747\n",
      "Epoch: 292/2000, Train Loss: 0.14776629209518433, Validation Loss: 0.15544858574867249\n",
      "Epoch: 293/2000, Train Loss: 0.14761608839035034, Validation Loss: 0.15562425553798676\n",
      "Epoch: 294/2000, Train Loss: 0.14749789237976074, Validation Loss: 0.15510691702365875\n",
      "Epoch: 295/2000, Train Loss: 0.1473315954208374, Validation Loss: 0.15522463619709015\n",
      "Epoch: 296/2000, Train Loss: 0.1470678746700287, Validation Loss: 0.15464700758457184\n",
      "Epoch: 297/2000, Train Loss: 0.14675982296466827, Validation Loss: 0.15453097224235535\n",
      "Epoch: 298/2000, Train Loss: 0.14647912979125977, Validation Loss: 0.15434865653514862\n",
      "Epoch: 299/2000, Train Loss: 0.14627163112163544, Validation Loss: 0.1540507674217224\n",
      "Epoch: 300/2000, Train Loss: 0.14611442387104034, Validation Loss: 0.15415048599243164\n",
      "Epoch: 301/2000, Train Loss: 0.1459498256444931, Validation Loss: 0.15367108583450317\n",
      "Epoch: 302/2000, Train Loss: 0.14574231207370758, Validation Loss: 0.15369060635566711\n",
      "Epoch: 303/2000, Train Loss: 0.145488902926445, Validation Loss: 0.15327459573745728\n",
      "Epoch: 304/2000, Train Loss: 0.1452307552099228, Validation Loss: 0.15312698483467102\n",
      "Epoch: 305/2000, Train Loss: 0.1450016349554062, Validation Loss: 0.15300191938877106\n",
      "Epoch: 306/2000, Train Loss: 0.1448090374469757, Validation Loss: 0.152696430683136\n",
      "Epoch: 307/2000, Train Loss: 0.14463303983211517, Validation Loss: 0.15272893011569977\n",
      "Epoch: 308/2000, Train Loss: 0.1444462388753891, Validation Loss: 0.1523158997297287\n",
      "Epoch: 309/2000, Train Loss: 0.1442369520664215, Validation Loss: 0.15229855477809906\n",
      "Epoch: 310/2000, Train Loss: 0.1440076380968094, Validation Loss: 0.1519480049610138\n",
      "Epoch: 311/2000, Train Loss: 0.14377658069133759, Validation Loss: 0.1518106311559677\n",
      "Epoch: 312/2000, Train Loss: 0.14355775713920593, Validation Loss: 0.15164242684841156\n",
      "Epoch: 313/2000, Train Loss: 0.1433555781841278, Validation Loss: 0.15138398110866547\n",
      "Epoch: 314/2000, Train Loss: 0.14316391944885254, Validation Loss: 0.1513456553220749\n",
      "Epoch: 315/2000, Train Loss: 0.14297236502170563, Validation Loss: 0.1510031372308731\n",
      "Epoch: 316/2000, Train Loss: 0.1427738219499588, Validation Loss: 0.15098293125629425\n",
      "Epoch: 317/2000, Train Loss: 0.14256493747234344, Validation Loss: 0.15063828229904175\n",
      "Epoch: 318/2000, Train Loss: 0.14234957098960876, Validation Loss: 0.15056343376636505\n",
      "Epoch: 319/2000, Train Loss: 0.14213231205940247, Validation Loss: 0.1502929925918579\n",
      "Epoch: 320/2000, Train Loss: 0.14191898703575134, Validation Loss: 0.15013715624809265\n",
      "Epoch: 321/2000, Train Loss: 0.14171229302883148, Validation Loss: 0.14996463060379028\n",
      "Epoch: 322/2000, Train Loss: 0.1415124088525772, Validation Loss: 0.14973260462284088\n",
      "Epoch: 323/2000, Train Loss: 0.14131782948970795, Validation Loss: 0.14963722229003906\n",
      "Epoch: 324/2000, Train Loss: 0.14112642407417297, Validation Loss: 0.14935259521007538\n",
      "Epoch: 325/2000, Train Loss: 0.14093664288520813, Validation Loss: 0.14930008351802826\n",
      "Epoch: 326/2000, Train Loss: 0.14074687659740448, Validation Loss: 0.14898847043514252\n",
      "Epoch: 327/2000, Train Loss: 0.1405569314956665, Validation Loss: 0.1489495038986206\n",
      "Epoch: 328/2000, Train Loss: 0.14036604762077332, Validation Loss: 0.14863111078739166\n",
      "Epoch: 329/2000, Train Loss: 0.14017494022846222, Validation Loss: 0.14859044551849365\n",
      "Epoch: 330/2000, Train Loss: 0.1399831920862198, Validation Loss: 0.14827938377857208\n",
      "Epoch: 331/2000, Train Loss: 0.13979169726371765, Validation Loss: 0.1482338160276413\n",
      "Epoch: 332/2000, Train Loss: 0.1396002173423767, Validation Loss: 0.14793354272842407\n",
      "Epoch: 333/2000, Train Loss: 0.13940955698490143, Validation Loss: 0.147884801030159\n",
      "Epoch: 334/2000, Train Loss: 0.13921959698200226, Validation Loss: 0.147588849067688\n",
      "Epoch: 335/2000, Train Loss: 0.1390310525894165, Validation Loss: 0.1475449502468109\n",
      "Epoch: 336/2000, Train Loss: 0.13884387910366058, Validation Loss: 0.14724385738372803\n",
      "Epoch: 337/2000, Train Loss: 0.13865897059440613, Validation Loss: 0.14722180366516113\n",
      "Epoch: 338/2000, Train Loss: 0.1384763866662979, Validation Loss: 0.14690303802490234\n",
      "Epoch: 339/2000, Train Loss: 0.13829763233661652, Validation Loss: 0.14692702889442444\n",
      "Epoch: 340/2000, Train Loss: 0.13812315464019775, Validation Loss: 0.14657112956047058\n",
      "Epoch: 341/2000, Train Loss: 0.1379561424255371, Validation Loss: 0.14667555689811707\n",
      "Epoch: 342/2000, Train Loss: 0.1377972960472107, Validation Loss: 0.1462596356868744\n",
      "Epoch: 343/2000, Train Loss: 0.13765332102775574, Validation Loss: 0.14649170637130737\n",
      "Epoch: 344/2000, Train Loss: 0.13752181828022003, Validation Loss: 0.14599397778511047\n",
      "Epoch: 345/2000, Train Loss: 0.13741232454776764, Validation Loss: 0.14637142419815063\n",
      "Epoch: 346/2000, Train Loss: 0.13730141520500183, Validation Loss: 0.14575904607772827\n",
      "Epoch: 347/2000, Train Loss: 0.1371903121471405, Validation Loss: 0.14614452421665192\n",
      "Epoch: 348/2000, Train Loss: 0.13701246678829193, Validation Loss: 0.14541010558605194\n",
      "Epoch: 349/2000, Train Loss: 0.1367792934179306, Validation Loss: 0.1455613225698471\n",
      "Epoch: 350/2000, Train Loss: 0.13647308945655823, Validation Loss: 0.14498071372509003\n",
      "Epoch: 351/2000, Train Loss: 0.13617683947086334, Validation Loss: 0.14489543437957764\n",
      "Epoch: 352/2000, Train Loss: 0.13594353199005127, Validation Loss: 0.1448034793138504\n",
      "Epoch: 353/2000, Train Loss: 0.13579176366329193, Validation Loss: 0.14451991021633148\n",
      "Epoch: 354/2000, Train Loss: 0.13568954169750214, Validation Loss: 0.14473263919353485\n",
      "Epoch: 355/2000, Train Loss: 0.13558217883110046, Validation Loss: 0.1442461460828781\n",
      "Epoch: 356/2000, Train Loss: 0.13543573021888733, Validation Loss: 0.14440344274044037\n",
      "Epoch: 357/2000, Train Loss: 0.1352289617061615, Validation Loss: 0.1439080834388733\n",
      "Epoch: 358/2000, Train Loss: 0.13499660789966583, Validation Loss: 0.1438770294189453\n",
      "Epoch: 359/2000, Train Loss: 0.13477425277233124, Validation Loss: 0.14366555213928223\n",
      "Epoch: 360/2000, Train Loss: 0.13459230959415436, Validation Loss: 0.14346188306808472\n",
      "Epoch: 361/2000, Train Loss: 0.13444890081882477, Validation Loss: 0.14352402091026306\n",
      "Epoch: 362/2000, Train Loss: 0.13432034850120544, Validation Loss: 0.14316298067569733\n",
      "Epoch: 363/2000, Train Loss: 0.13418202102184296, Validation Loss: 0.14327163994312286\n",
      "Epoch: 364/2000, Train Loss: 0.13401612639427185, Validation Loss: 0.14286337792873383\n",
      "Epoch: 365/2000, Train Loss: 0.1338292956352234, Validation Loss: 0.14286914467811584\n",
      "Epoch: 366/2000, Train Loss: 0.13363374769687653, Validation Loss: 0.14259330928325653\n",
      "Epoch: 367/2000, Train Loss: 0.13344839215278625, Validation Loss: 0.1424659788608551\n",
      "Epoch: 368/2000, Train Loss: 0.1332814246416092, Validation Loss: 0.14238758385181427\n",
      "Epoch: 369/2000, Train Loss: 0.1331305056810379, Validation Loss: 0.1421373039484024\n",
      "Epoch: 370/2000, Train Loss: 0.13298650085926056, Validation Loss: 0.14217115938663483\n",
      "Epoch: 371/2000, Train Loss: 0.13283903896808624, Validation Loss: 0.14184542000293732\n",
      "Epoch: 372/2000, Train Loss: 0.13268323242664337, Validation Loss: 0.14187990128993988\n",
      "Epoch: 373/2000, Train Loss: 0.13251695036888123, Validation Loss: 0.14156363904476166\n",
      "Epoch: 374/2000, Train Loss: 0.13234537839889526, Validation Loss: 0.1415334939956665\n",
      "Epoch: 375/2000, Train Loss: 0.13217297196388245, Validation Loss: 0.14130249619483948\n",
      "Epoch: 376/2000, Train Loss: 0.13200485706329346, Validation Loss: 0.14118798077106476\n",
      "Epoch: 377/2000, Train Loss: 0.1318429410457611, Validation Loss: 0.14106397330760956\n",
      "Epoch: 378/2000, Train Loss: 0.13168686628341675, Validation Loss: 0.14087176322937012\n",
      "Epoch: 379/2000, Train Loss: 0.1315346658229828, Validation Loss: 0.1408279985189438\n",
      "Epoch: 380/2000, Train Loss: 0.13138394057750702, Validation Loss: 0.14057931303977966\n",
      "Epoch: 381/2000, Train Loss: 0.13123293220996857, Validation Loss: 0.14057449996471405\n",
      "Epoch: 382/2000, Train Loss: 0.13108018040657043, Validation Loss: 0.1402972787618637\n",
      "Epoch: 383/2000, Train Loss: 0.13092577457427979, Validation Loss: 0.14029893279075623\n",
      "Epoch: 384/2000, Train Loss: 0.130769282579422, Validation Loss: 0.1400201916694641\n",
      "Epoch: 385/2000, Train Loss: 0.1306118667125702, Validation Loss: 0.1400100737810135\n",
      "Epoch: 386/2000, Train Loss: 0.13045351207256317, Validation Loss: 0.13974837958812714\n",
      "Epoch: 387/2000, Train Loss: 0.1302952915430069, Validation Loss: 0.1397184133529663\n",
      "Epoch: 388/2000, Train Loss: 0.13013724982738495, Validation Loss: 0.13948029279708862\n",
      "Epoch: 389/2000, Train Loss: 0.12997999787330627, Validation Loss: 0.13942959904670715\n",
      "Epoch: 390/2000, Train Loss: 0.12982340157032013, Validation Loss: 0.1392134130001068\n",
      "Epoch: 391/2000, Train Loss: 0.1296677142381668, Validation Loss: 0.1391475647687912\n",
      "Epoch: 392/2000, Train Loss: 0.12951281666755676, Validation Loss: 0.13894784450531006\n",
      "Epoch: 393/2000, Train Loss: 0.12935873866081238, Validation Loss: 0.13887567818164825\n",
      "Epoch: 394/2000, Train Loss: 0.1292053610086441, Validation Loss: 0.13868382573127747\n",
      "Epoch: 395/2000, Train Loss: 0.12905272841453552, Validation Loss: 0.13861440122127533\n",
      "Epoch: 396/2000, Train Loss: 0.12890076637268066, Validation Loss: 0.13841888308525085\n",
      "Epoch: 397/2000, Train Loss: 0.12874962389469147, Validation Loss: 0.13836364448070526\n",
      "Epoch: 398/2000, Train Loss: 0.12859944999217987, Validation Loss: 0.1381511390209198\n",
      "Epoch: 399/2000, Train Loss: 0.12845073640346527, Validation Loss: 0.1381288319826126\n",
      "Epoch: 400/2000, Train Loss: 0.1283041387796402, Validation Loss: 0.13788212835788727\n",
      "Epoch: 401/2000, Train Loss: 0.1281612366437912, Validation Loss: 0.13792580366134644\n",
      "Epoch: 402/2000, Train Loss: 0.12802402675151825, Validation Loss: 0.13761992752552032\n",
      "Epoch: 403/2000, Train Loss: 0.12789767980575562, Validation Loss: 0.13779322803020477\n",
      "Epoch: 404/2000, Train Loss: 0.12778757512569427, Validation Loss: 0.13739918172359467\n",
      "Epoch: 405/2000, Train Loss: 0.12770946323871613, Validation Loss: 0.13782058656215668\n",
      "Epoch: 406/2000, Train Loss: 0.1276704967021942, Validation Loss: 0.13731715083122253\n",
      "Epoch: 407/2000, Train Loss: 0.12770311534404755, Validation Loss: 0.13808290660381317\n",
      "Epoch: 408/2000, Train Loss: 0.127756267786026, Validation Loss: 0.13736958801746368\n",
      "Epoch: 409/2000, Train Loss: 0.127821147441864, Validation Loss: 0.13805411756038666\n",
      "Epoch: 410/2000, Train Loss: 0.12763528525829315, Validation Loss: 0.13692067563533783\n",
      "Epoch: 411/2000, Train Loss: 0.12724046409130096, Validation Loss: 0.13694636523723602\n",
      "Epoch: 412/2000, Train Loss: 0.12670160830020905, Validation Loss: 0.1364455670118332\n",
      "Epoch: 413/2000, Train Loss: 0.12638123333454132, Validation Loss: 0.13629402220249176\n",
      "Epoch: 414/2000, Train Loss: 0.12636640667915344, Validation Loss: 0.13682813942432404\n",
      "Epoch: 415/2000, Train Loss: 0.1264512687921524, Validation Loss: 0.1362135112285614\n",
      "Epoch: 416/2000, Train Loss: 0.12639306485652924, Validation Loss: 0.13644473254680634\n",
      "Epoch: 417/2000, Train Loss: 0.1260783076286316, Validation Loss: 0.1357889473438263\n",
      "Epoch: 418/2000, Train Loss: 0.1257343590259552, Validation Loss: 0.13568557798862457\n",
      "Epoch: 419/2000, Train Loss: 0.12556229531764984, Validation Loss: 0.1359175443649292\n",
      "Epoch: 420/2000, Train Loss: 0.12555210292339325, Validation Loss: 0.1355111002922058\n",
      "Epoch: 421/2000, Train Loss: 0.125522643327713, Validation Loss: 0.13576629757881165\n",
      "Epoch: 422/2000, Train Loss: 0.12533234059810638, Validation Loss: 0.13520485162734985\n",
      "Epoch: 423/2000, Train Loss: 0.12506699562072754, Validation Loss: 0.13513417541980743\n",
      "Epoch: 424/2000, Train Loss: 0.12487160414457321, Validation Loss: 0.13518603146076202\n",
      "Epoch: 425/2000, Train Loss: 0.1247936487197876, Validation Loss: 0.13486938178539276\n",
      "Epoch: 426/2000, Train Loss: 0.12473837286233902, Validation Loss: 0.13508202135562897\n",
      "Epoch: 427/2000, Train Loss: 0.12459919601678848, Validation Loss: 0.13461725413799286\n",
      "Epoch: 428/2000, Train Loss: 0.1243939995765686, Validation Loss: 0.13457955420017242\n",
      "Epoch: 429/2000, Train Loss: 0.12420603632926941, Validation Loss: 0.13451580703258514\n",
      "Epoch: 430/2000, Train Loss: 0.12408909946680069, Validation Loss: 0.13426527380943298\n",
      "Epoch: 431/2000, Train Loss: 0.1240069568157196, Validation Loss: 0.13441859185695648\n",
      "Epoch: 432/2000, Train Loss: 0.12389127910137177, Validation Loss: 0.13403064012527466\n",
      "Epoch: 433/2000, Train Loss: 0.12372791022062302, Validation Loss: 0.1340259611606598\n",
      "Epoch: 434/2000, Train Loss: 0.12355406582355499, Validation Loss: 0.13387665152549744\n",
      "Epoch: 435/2000, Train Loss: 0.12341456860303879, Validation Loss: 0.1336909383535385\n",
      "Epoch: 436/2000, Train Loss: 0.12330831587314606, Validation Loss: 0.1337764412164688\n",
      "Epoch: 437/2000, Train Loss: 0.12320000678300858, Validation Loss: 0.13345728814601898\n",
      "Epoch: 438/2000, Train Loss: 0.12306541204452515, Validation Loss: 0.13348160684108734\n",
      "Epoch: 439/2000, Train Loss: 0.12291017174720764, Validation Loss: 0.13326621055603027\n",
      "Epoch: 440/2000, Train Loss: 0.12276152521371841, Validation Loss: 0.13314500451087952\n",
      "Epoch: 441/2000, Train Loss: 0.12263406813144684, Validation Loss: 0.133143350481987\n",
      "Epoch: 442/2000, Train Loss: 0.1225195974111557, Validation Loss: 0.13289795815944672\n",
      "Epoch: 443/2000, Train Loss: 0.12240009009838104, Validation Loss: 0.1329352706670761\n",
      "Epoch: 444/2000, Train Loss: 0.12226541340351105, Validation Loss: 0.13268908858299255\n",
      "Epoch: 445/2000, Train Loss: 0.12212228029966354, Validation Loss: 0.13263295590877533\n",
      "Epoch: 446/2000, Train Loss: 0.12198285013437271, Validation Loss: 0.13252748548984528\n",
      "Epoch: 447/2000, Train Loss: 0.12185417115688324, Validation Loss: 0.13236209750175476\n",
      "Epoch: 448/2000, Train Loss: 0.12173289805650711, Validation Loss: 0.1323646754026413\n",
      "Epoch: 449/2000, Train Loss: 0.12161052227020264, Validation Loss: 0.1321384757757187\n",
      "Epoch: 450/2000, Train Loss: 0.12148168683052063, Validation Loss: 0.13212977349758148\n",
      "Epoch: 451/2000, Train Loss: 0.12134692072868347, Validation Loss: 0.13194231688976288\n",
      "Epoch: 452/2000, Train Loss: 0.12121157348155975, Validation Loss: 0.1318589299917221\n",
      "Epoch: 453/2000, Train Loss: 0.12108013778924942, Validation Loss: 0.13176926970481873\n",
      "Epoch: 454/2000, Train Loss: 0.12095359712839127, Validation Loss: 0.13160955905914307\n",
      "Epoch: 455/2000, Train Loss: 0.1208295077085495, Validation Loss: 0.1315837800502777\n",
      "Epoch: 456/2000, Train Loss: 0.12070445716381073, Validation Loss: 0.13138653337955475\n",
      "Epoch: 457/2000, Train Loss: 0.12057659029960632, Validation Loss: 0.1313576102256775\n",
      "Epoch: 458/2000, Train Loss: 0.12044601142406464, Validation Loss: 0.13118013739585876\n",
      "Epoch: 459/2000, Train Loss: 0.12031447887420654, Validation Loss: 0.1311062127351761\n",
      "Epoch: 460/2000, Train Loss: 0.12018377333879471, Validation Loss: 0.13098442554473877\n",
      "Epoch: 461/2000, Train Loss: 0.12005474418401718, Validation Loss: 0.13085706532001495\n",
      "Epoch: 462/2000, Train Loss: 0.11992723494768143, Validation Loss: 0.13078634440898895\n",
      "Epoch: 463/2000, Train Loss: 0.11980035156011581, Validation Loss: 0.1306203305721283\n",
      "Epoch: 464/2000, Train Loss: 0.11967313289642334, Validation Loss: 0.13057313859462738\n",
      "Epoch: 465/2000, Train Loss: 0.11954496800899506, Validation Loss: 0.13039466738700867\n",
      "Epoch: 466/2000, Train Loss: 0.1194157749414444, Validation Loss: 0.13034431636333466\n",
      "Epoch: 467/2000, Train Loss: 0.1192857027053833, Validation Loss: 0.13017785549163818\n",
      "Epoch: 468/2000, Train Loss: 0.11915519833564758, Validation Loss: 0.13010865449905396\n",
      "Epoch: 469/2000, Train Loss: 0.1190246120095253, Validation Loss: 0.12996777892112732\n",
      "Epoch: 470/2000, Train Loss: 0.11889420449733734, Validation Loss: 0.12987451255321503\n",
      "Epoch: 471/2000, Train Loss: 0.11876413971185684, Validation Loss: 0.12976108491420746\n",
      "Epoch: 472/2000, Train Loss: 0.11863449215888977, Validation Loss: 0.1296447217464447\n",
      "Epoch: 473/2000, Train Loss: 0.11850525438785553, Validation Loss: 0.12955345213413239\n",
      "Epoch: 474/2000, Train Loss: 0.11837638914585114, Validation Loss: 0.1294167935848236\n",
      "Epoch: 475/2000, Train Loss: 0.11824780702590942, Validation Loss: 0.12934176623821259\n",
      "Epoch: 476/2000, Train Loss: 0.11811952292919159, Validation Loss: 0.12918853759765625\n",
      "Epoch: 477/2000, Train Loss: 0.11799163371324539, Validation Loss: 0.12912915647029877\n",
      "Epoch: 478/2000, Train Loss: 0.11786427348852158, Validation Loss: 0.128962442278862\n",
      "Epoch: 479/2000, Train Loss: 0.11773763597011566, Validation Loss: 0.12892349064350128\n",
      "Epoch: 480/2000, Train Loss: 0.1176118329167366, Validation Loss: 0.12874090671539307\n",
      "Epoch: 481/2000, Train Loss: 0.11748712509870529, Validation Loss: 0.12873129546642303\n",
      "Epoch: 482/2000, Train Loss: 0.11736380308866501, Validation Loss: 0.12852251529693604\n",
      "Epoch: 483/2000, Train Loss: 0.11724258214235306, Validation Loss: 0.12855951488018036\n",
      "Epoch: 484/2000, Train Loss: 0.11712437123060226, Validation Loss: 0.1283058375120163\n",
      "Epoch: 485/2000, Train Loss: 0.11701126396656036, Validation Loss: 0.12842491269111633\n",
      "Epoch: 486/2000, Train Loss: 0.11690574884414673, Validation Loss: 0.12809985876083374\n",
      "Epoch: 487/2000, Train Loss: 0.11681400239467621, Validation Loss: 0.1283724159002304\n",
      "Epoch: 488/2000, Train Loss: 0.11674191802740097, Validation Loss: 0.12794770300388336\n",
      "Epoch: 489/2000, Train Loss: 0.11670510470867157, Validation Loss: 0.1284869760274887\n",
      "Epoch: 490/2000, Train Loss: 0.11670590937137604, Validation Loss: 0.12793317437171936\n",
      "Epoch: 491/2000, Train Loss: 0.11676336824893951, Validation Loss: 0.12875081598758698\n",
      "Epoch: 492/2000, Train Loss: 0.11680633574724197, Validation Loss: 0.1279478818178177\n",
      "Epoch: 493/2000, Train Loss: 0.11680677533149719, Validation Loss: 0.12856025993824005\n",
      "Epoch: 494/2000, Train Loss: 0.11656932532787323, Validation Loss: 0.12749260663986206\n",
      "Epoch: 495/2000, Train Loss: 0.11618301272392273, Validation Loss: 0.127553790807724\n",
      "Epoch: 496/2000, Train Loss: 0.11576007306575775, Validation Loss: 0.1271946132183075\n",
      "Epoch: 497/2000, Train Loss: 0.11553407460451126, Validation Loss: 0.12703384459018707\n",
      "Epoch: 498/2000, Train Loss: 0.11553090810775757, Validation Loss: 0.1275431215763092\n",
      "Epoch: 499/2000, Train Loss: 0.11560055613517761, Validation Loss: 0.12698066234588623\n",
      "Epoch: 500/2000, Train Loss: 0.11558037996292114, Validation Loss: 0.12735162675380707\n",
      "Epoch: 501/2000, Train Loss: 0.1153663694858551, Validation Loss: 0.1266653835773468\n",
      "Epoch: 502/2000, Train Loss: 0.1150745376944542, Validation Loss: 0.12665870785713196\n",
      "Epoch: 503/2000, Train Loss: 0.11485198140144348, Validation Loss: 0.12667496502399445\n",
      "Epoch: 504/2000, Train Loss: 0.11477674543857574, Validation Loss: 0.12640772759914398\n",
      "Epoch: 505/2000, Train Loss: 0.11477507650852203, Validation Loss: 0.12677571177482605\n",
      "Epoch: 506/2000, Train Loss: 0.11471516638994217, Validation Loss: 0.12622785568237305\n",
      "Epoch: 507/2000, Train Loss: 0.11455526202917099, Validation Loss: 0.12634211778640747\n",
      "Epoch: 508/2000, Train Loss: 0.1143411248922348, Validation Loss: 0.1260720193386078\n",
      "Epoch: 509/2000, Train Loss: 0.11417512595653534, Validation Loss: 0.12594160437583923\n",
      "Epoch: 510/2000, Train Loss: 0.11409072577953339, Validation Loss: 0.12613576650619507\n",
      "Epoch: 511/2000, Train Loss: 0.11404033750295639, Validation Loss: 0.12576201558113098\n",
      "Epoch: 512/2000, Train Loss: 0.11395809799432755, Validation Loss: 0.1259474903345108\n",
      "Epoch: 513/2000, Train Loss: 0.11381518095731735, Validation Loss: 0.12557975947856903\n",
      "Epoch: 514/2000, Train Loss: 0.11365000903606415, Validation Loss: 0.12556129693984985\n",
      "Epoch: 515/2000, Train Loss: 0.11351018399000168, Validation Loss: 0.12554055452346802\n",
      "Epoch: 516/2000, Train Loss: 0.11341403424739838, Validation Loss: 0.125327467918396\n",
      "Epoch: 517/2000, Train Loss: 0.11333779990673065, Validation Loss: 0.12549053132534027\n",
      "Epoch: 518/2000, Train Loss: 0.11324477940797806, Validation Loss: 0.12515756487846375\n",
      "Epoch: 519/2000, Train Loss: 0.11312250047922134, Validation Loss: 0.12522229552268982\n",
      "Epoch: 520/2000, Train Loss: 0.11298364400863647, Validation Loss: 0.12502114474773407\n",
      "Epoch: 521/2000, Train Loss: 0.11285480856895447, Validation Loss: 0.12492651492357254\n",
      "Epoch: 522/2000, Train Loss: 0.1127481758594513, Validation Loss: 0.12495817244052887\n",
      "Epoch: 523/2000, Train Loss: 0.1126561090350151, Validation Loss: 0.12473075836896896\n",
      "Epoch: 524/2000, Train Loss: 0.11256171762943268, Validation Loss: 0.12483028322458267\n",
      "Epoch: 525/2000, Train Loss: 0.11245304346084595, Validation Loss: 0.12457803636789322\n",
      "Epoch: 526/2000, Train Loss: 0.11233268678188324, Validation Loss: 0.12459283322095871\n",
      "Epoch: 527/2000, Train Loss: 0.11221059411764145, Validation Loss: 0.12446185946464539\n",
      "Epoch: 528/2000, Train Loss: 0.11209684610366821, Validation Loss: 0.12435509264469147\n",
      "Epoch: 529/2000, Train Loss: 0.11199342459440231, Validation Loss: 0.12436486780643463\n",
      "Epoch: 530/2000, Train Loss: 0.11189503222703934, Validation Loss: 0.12416735291481018\n",
      "Epoch: 531/2000, Train Loss: 0.11179450154304504, Validation Loss: 0.12421849370002747\n",
      "Epoch: 532/2000, Train Loss: 0.11168751865625381, Validation Loss: 0.12401179224252701\n",
      "Epoch: 533/2000, Train Loss: 0.11157536506652832, Validation Loss: 0.12401336431503296\n",
      "Epoch: 534/2000, Train Loss: 0.11146198958158493, Validation Loss: 0.12388195097446442\n",
      "Epoch: 535/2000, Train Loss: 0.11135160177946091, Validation Loss: 0.12380179017782211\n",
      "Epoch: 536/2000, Train Loss: 0.11124584078788757, Validation Loss: 0.1237655058503151\n",
      "Epoch: 537/2000, Train Loss: 0.11114351451396942, Validation Loss: 0.12361674010753632\n",
      "Epoch: 538/2000, Train Loss: 0.11104204505681992, Validation Loss: 0.12363094836473465\n",
      "Epoch: 539/2000, Train Loss: 0.11093909293413162, Validation Loss: 0.123454749584198\n",
      "Epoch: 540/2000, Train Loss: 0.1108338013291359, Validation Loss: 0.12346130609512329\n",
      "Epoch: 541/2000, Train Loss: 0.11072655767202377, Validation Loss: 0.1233062818646431\n",
      "Epoch: 542/2000, Train Loss: 0.11061877757310867, Validation Loss: 0.12327040731906891\n",
      "Epoch: 543/2000, Train Loss: 0.11051177978515625, Validation Loss: 0.12316803634166718\n",
      "Epoch: 544/2000, Train Loss: 0.11040637642145157, Validation Loss: 0.12308307737112045\n",
      "Epoch: 545/2000, Train Loss: 0.11030258238315582, Validation Loss: 0.12303420156240463\n",
      "Epoch: 546/2000, Train Loss: 0.11019992083311081, Validation Loss: 0.12291014194488525\n",
      "Epoch: 547/2000, Train Loss: 0.11009769886732101, Validation Loss: 0.12289299070835114\n",
      "Epoch: 548/2000, Train Loss: 0.10999535024166107, Validation Loss: 0.12274772673845291\n",
      "Epoch: 549/2000, Train Loss: 0.10989253222942352, Validation Loss: 0.12273722887039185\n",
      "Epoch: 550/2000, Train Loss: 0.1097891554236412, Validation Loss: 0.12259145081043243\n",
      "Epoch: 551/2000, Train Loss: 0.10968543589115143, Validation Loss: 0.12257128953933716\n",
      "Epoch: 552/2000, Train Loss: 0.10958156734704971, Validation Loss: 0.12244097888469696\n",
      "Epoch: 553/2000, Train Loss: 0.10947781056165695, Validation Loss: 0.12240270525217056\n",
      "Epoch: 554/2000, Train Loss: 0.10937434434890747, Validation Loss: 0.12229380756616592\n",
      "Epoch: 555/2000, Train Loss: 0.10927126556634903, Validation Loss: 0.12223421782255173\n",
      "Epoch: 556/2000, Train Loss: 0.10916859656572342, Validation Loss: 0.12214591354131699\n",
      "Epoch: 557/2000, Train Loss: 0.10906632989645004, Validation Loss: 0.12206710129976273\n",
      "Epoch: 558/2000, Train Loss: 0.10896441340446472, Validation Loss: 0.1219971776008606\n",
      "Epoch: 559/2000, Train Loss: 0.10886280238628387, Validation Loss: 0.12190353125333786\n",
      "Epoch: 560/2000, Train Loss: 0.10876143723726273, Validation Loss: 0.12184897065162659\n",
      "Epoch: 561/2000, Train Loss: 0.10866031795740128, Validation Loss: 0.12174267321825027\n",
      "Epoch: 562/2000, Train Loss: 0.10855945199728012, Validation Loss: 0.1217004805803299\n",
      "Epoch: 563/2000, Train Loss: 0.10845880210399628, Validation Loss: 0.12158138304948807\n",
      "Epoch: 564/2000, Train Loss: 0.10835843533277512, Validation Loss: 0.12155213952064514\n",
      "Epoch: 565/2000, Train Loss: 0.10825840383768082, Validation Loss: 0.12141887098550797\n",
      "Epoch: 566/2000, Train Loss: 0.10815879702568054, Validation Loss: 0.12140783667564392\n",
      "Epoch: 567/2000, Train Loss: 0.10805976390838623, Validation Loss: 0.12125551700592041\n",
      "Epoch: 568/2000, Train Loss: 0.10796153545379639, Validation Loss: 0.12127236276865005\n",
      "Epoch: 569/2000, Train Loss: 0.10786445438861847, Validation Loss: 0.12109039723873138\n",
      "Epoch: 570/2000, Train Loss: 0.1077692061662674, Validation Loss: 0.12115315347909927\n",
      "Epoch: 571/2000, Train Loss: 0.10767663270235062, Validation Loss: 0.12092471867799759\n",
      "Epoch: 572/2000, Train Loss: 0.10758865624666214, Validation Loss: 0.1210678368806839\n",
      "Epoch: 573/2000, Train Loss: 0.10750742256641388, Validation Loss: 0.12076911330223083\n",
      "Epoch: 574/2000, Train Loss: 0.10743827372789383, Validation Loss: 0.12105514854192734\n",
      "Epoch: 575/2000, Train Loss: 0.10738565027713776, Validation Loss: 0.12065839022397995\n",
      "Epoch: 576/2000, Train Loss: 0.10736257582902908, Validation Loss: 0.1211824044585228\n",
      "Epoch: 577/2000, Train Loss: 0.10737031698226929, Validation Loss: 0.12065993249416351\n",
      "Epoch: 578/2000, Train Loss: 0.1074279248714447, Validation Loss: 0.12145674228668213\n",
      "Epoch: 579/2000, Train Loss: 0.10748578608036041, Validation Loss: 0.12072718143463135\n",
      "Epoch: 580/2000, Train Loss: 0.1075349822640419, Validation Loss: 0.12145823985338211\n",
      "Epoch: 581/2000, Train Loss: 0.1074070930480957, Validation Loss: 0.12044306844472885\n",
      "Epoch: 582/2000, Train Loss: 0.1071380004286766, Validation Loss: 0.12065939605236053\n",
      "Epoch: 583/2000, Train Loss: 0.10673592984676361, Validation Loss: 0.12004061043262482\n",
      "Epoch: 584/2000, Train Loss: 0.10642721503973007, Validation Loss: 0.11997091770172119\n",
      "Epoch: 585/2000, Train Loss: 0.10631778836250305, Validation Loss: 0.1202937439084053\n",
      "Epoch: 586/2000, Train Loss: 0.10636482387781143, Validation Loss: 0.1199129968881607\n",
      "Epoch: 587/2000, Train Loss: 0.10642930865287781, Validation Loss: 0.12042286992073059\n",
      "Epoch: 588/2000, Train Loss: 0.10636308789253235, Validation Loss: 0.11973261833190918\n",
      "Epoch: 589/2000, Train Loss: 0.10616761445999146, Validation Loss: 0.11987189203500748\n",
      "Epoch: 590/2000, Train Loss: 0.10591419041156769, Validation Loss: 0.11957268416881561\n",
      "Epoch: 591/2000, Train Loss: 0.10574300587177277, Validation Loss: 0.11945797502994537\n",
      "Epoch: 592/2000, Train Loss: 0.10569247603416443, Validation Loss: 0.11975755542516708\n",
      "Epoch: 593/2000, Train Loss: 0.10569406300783157, Validation Loss: 0.11935581266880035\n",
      "Epoch: 594/2000, Train Loss: 0.10565482825040817, Validation Loss: 0.11964139342308044\n",
      "Epoch: 595/2000, Train Loss: 0.10552270710468292, Validation Loss: 0.11918288469314575\n",
      "Epoch: 596/2000, Train Loss: 0.10534734278917313, Validation Loss: 0.11920688301324844\n",
      "Epoch: 597/2000, Train Loss: 0.10519774258136749, Validation Loss: 0.11916019767522812\n",
      "Epoch: 598/2000, Train Loss: 0.1051144078373909, Validation Loss: 0.11896099895238876\n",
      "Epoch: 599/2000, Train Loss: 0.10507310926914215, Validation Loss: 0.11919375509023666\n",
      "Epoch: 600/2000, Train Loss: 0.10501846671104431, Validation Loss: 0.1188221275806427\n",
      "Epoch: 601/2000, Train Loss: 0.10492099821567535, Validation Loss: 0.11896242946386337\n",
      "Epoch: 602/2000, Train Loss: 0.10478747636079788, Validation Loss: 0.11870812624692917\n",
      "Epoch: 603/2000, Train Loss: 0.1046585813164711, Validation Loss: 0.11866126954555511\n",
      "Epoch: 604/2000, Train Loss: 0.10456108301877975, Validation Loss: 0.11870268732309341\n",
      "Epoch: 605/2000, Train Loss: 0.10449214279651642, Validation Loss: 0.11847994476556778\n",
      "Epoch: 606/2000, Train Loss: 0.10442744195461273, Validation Loss: 0.11862855404615402\n",
      "Epoch: 607/2000, Train Loss: 0.10434361547231674, Validation Loss: 0.11833847314119339\n",
      "Epoch: 608/2000, Train Loss: 0.10423887521028519, Validation Loss: 0.11839745193719864\n",
      "Epoch: 609/2000, Train Loss: 0.10412595421075821, Validation Loss: 0.11824598163366318\n",
      "Epoch: 610/2000, Train Loss: 0.1040230542421341, Validation Loss: 0.11816758662462234\n",
      "Epoch: 611/2000, Train Loss: 0.10393678396940231, Validation Loss: 0.1182035505771637\n",
      "Epoch: 612/2000, Train Loss: 0.10386054962873459, Validation Loss: 0.11800651997327805\n",
      "Epoch: 613/2000, Train Loss: 0.10378239303827286, Validation Loss: 0.11809585988521576\n",
      "Epoch: 614/2000, Train Loss: 0.10369391739368439, Validation Loss: 0.11787398159503937\n",
      "Epoch: 615/2000, Train Loss: 0.10359648615121841, Validation Loss: 0.1178995743393898\n",
      "Epoch: 616/2000, Train Loss: 0.10349652916193008, Validation Loss: 0.11777415126562119\n",
      "Epoch: 617/2000, Train Loss: 0.1034015417098999, Validation Loss: 0.11770094186067581\n",
      "Epoch: 618/2000, Train Loss: 0.10331409424543381, Validation Loss: 0.11769687384366989\n",
      "Epoch: 619/2000, Train Loss: 0.10323163866996765, Validation Loss: 0.11754098534584045\n",
      "Epoch: 620/2000, Train Loss: 0.10314929485321045, Validation Loss: 0.1175866425037384\n",
      "Epoch: 621/2000, Train Loss: 0.10306313633918762, Validation Loss: 0.11740684509277344\n",
      "Epoch: 622/2000, Train Loss: 0.10297270864248276, Validation Loss: 0.11742646992206573\n",
      "Epoch: 623/2000, Train Loss: 0.10287979990243912, Validation Loss: 0.11729403585195541\n",
      "Epoch: 624/2000, Train Loss: 0.10278753191232681, Validation Loss: 0.11725141108036041\n",
      "Epoch: 625/2000, Train Loss: 0.1026979312300682, Validation Loss: 0.11719620227813721\n",
      "Epoch: 626/2000, Train Loss: 0.10261136293411255, Validation Loss: 0.11709047853946686\n",
      "Epoch: 627/2000, Train Loss: 0.10252663493156433, Validation Loss: 0.11708971112966537\n",
      "Epoch: 628/2000, Train Loss: 0.10244203358888626, Validation Loss: 0.1169472485780716\n",
      "Epoch: 629/2000, Train Loss: 0.10235626995563507, Validation Loss: 0.11695832759141922\n",
      "Epoch: 630/2000, Train Loss: 0.10226888209581375, Validation Loss: 0.11681869626045227\n",
      "Epoch: 631/2000, Train Loss: 0.1021803691983223, Validation Loss: 0.11680787801742554\n",
      "Epoch: 632/2000, Train Loss: 0.10209155082702637, Validation Loss: 0.11670199036598206\n",
      "Epoch: 633/2000, Train Loss: 0.1020032986998558, Validation Loss: 0.11665333807468414\n",
      "Epoch: 634/2000, Train Loss: 0.10191605240106583, Validation Loss: 0.11659061908721924\n",
      "Epoch: 635/2000, Train Loss: 0.1018298789858818, Validation Loss: 0.11650518327951431\n",
      "Epoch: 636/2000, Train Loss: 0.10174446552991867, Validation Loss: 0.11647646874189377\n",
      "Epoch: 637/2000, Train Loss: 0.101659394800663, Validation Loss: 0.11636660248041153\n",
      "Epoch: 638/2000, Train Loss: 0.10157427191734314, Validation Loss: 0.11635389178991318\n",
      "Epoch: 639/2000, Train Loss: 0.10148883610963821, Validation Loss: 0.1162356361746788\n",
      "Epoch: 640/2000, Train Loss: 0.10140306502580643, Validation Loss: 0.11622169613838196\n",
      "Epoch: 641/2000, Train Loss: 0.10131703317165375, Validation Loss: 0.11610952019691467\n",
      "Epoch: 642/2000, Train Loss: 0.10123087465763092, Validation Loss: 0.11608366668224335\n",
      "Epoch: 643/2000, Train Loss: 0.10114476829767227, Validation Loss: 0.11598727852106094\n",
      "Epoch: 644/2000, Train Loss: 0.10105884820222855, Validation Loss: 0.11594529449939728\n",
      "Epoch: 645/2000, Train Loss: 0.10097319632768631, Validation Loss: 0.1158677339553833\n",
      "Epoch: 646/2000, Train Loss: 0.10088783502578735, Validation Loss: 0.11580905318260193\n",
      "Epoch: 647/2000, Train Loss: 0.10080274939537048, Validation Loss: 0.11574814468622208\n",
      "Epoch: 648/2000, Train Loss: 0.10071791708469391, Validation Loss: 0.11567475646734238\n",
      "Epoch: 649/2000, Train Loss: 0.10063327103853226, Validation Loss: 0.11562707275152206\n",
      "Epoch: 650/2000, Train Loss: 0.10054877400398254, Validation Loss: 0.11554273962974548\n",
      "Epoch: 651/2000, Train Loss: 0.10046441107988358, Validation Loss: 0.11550548672676086\n",
      "Epoch: 652/2000, Train Loss: 0.10038014501333237, Validation Loss: 0.11541339755058289\n",
      "Epoch: 653/2000, Train Loss: 0.10029596090316772, Validation Loss: 0.11538428068161011\n",
      "Epoch: 654/2000, Train Loss: 0.10021188110113144, Validation Loss: 0.11528564989566803\n",
      "Epoch: 655/2000, Train Loss: 0.10012790560722351, Validation Loss: 0.11526359617710114\n",
      "Epoch: 656/2000, Train Loss: 0.10004405677318573, Validation Loss: 0.11515822261571884\n",
      "Epoch: 657/2000, Train Loss: 0.09996037930250168, Validation Loss: 0.11514453589916229\n",
      "Epoch: 658/2000, Train Loss: 0.09987691044807434, Validation Loss: 0.11503084003925323\n",
      "Epoch: 659/2000, Train Loss: 0.09979373961687088, Validation Loss: 0.11502885818481445\n",
      "Epoch: 660/2000, Train Loss: 0.09971090406179428, Validation Loss: 0.11490271240472794\n",
      "Epoch: 661/2000, Train Loss: 0.09962858259677887, Validation Loss: 0.1149183064699173\n",
      "Epoch: 662/2000, Train Loss: 0.0995469018816948, Validation Loss: 0.11477278172969818\n",
      "Epoch: 663/2000, Train Loss: 0.0994662195444107, Validation Loss: 0.1148165762424469\n",
      "Epoch: 664/2000, Train Loss: 0.09938684105873108, Validation Loss: 0.11464156955480576\n",
      "Epoch: 665/2000, Train Loss: 0.09930960834026337, Validation Loss: 0.11473219096660614\n",
      "Epoch: 666/2000, Train Loss: 0.09923526644706726, Validation Loss: 0.11451274901628494\n",
      "Epoch: 667/2000, Train Loss: 0.09916593134403229, Validation Loss: 0.11468169838190079\n",
      "Epoch: 668/2000, Train Loss: 0.09910321980714798, Validation Loss: 0.11439741402864456\n",
      "Epoch: 669/2000, Train Loss: 0.09905247390270233, Validation Loss: 0.11469736695289612\n",
      "Epoch: 670/2000, Train Loss: 0.09901617467403412, Validation Loss: 0.11432592570781708\n",
      "Epoch: 671/2000, Train Loss: 0.09900637716054916, Validation Loss: 0.11482998728752136\n",
      "Epoch: 672/2000, Train Loss: 0.09901975095272064, Validation Loss: 0.11434845626354218\n",
      "Epoch: 673/2000, Train Loss: 0.09907504916191101, Validation Loss: 0.1150774359703064\n",
      "Epoch: 674/2000, Train Loss: 0.09912609308958054, Validation Loss: 0.11442414671182632\n",
      "Epoch: 675/2000, Train Loss: 0.09917887300252914, Validation Loss: 0.11512962728738785\n",
      "Epoch: 676/2000, Train Loss: 0.09909454733133316, Validation Loss: 0.11423203349113464\n",
      "Epoch: 677/2000, Train Loss: 0.0989040657877922, Validation Loss: 0.11452372372150421\n",
      "Epoch: 678/2000, Train Loss: 0.09856593608856201, Validation Loss: 0.11382660269737244\n",
      "Epoch: 679/2000, Train Loss: 0.09825541824102402, Validation Loss: 0.11381803452968597\n",
      "Epoch: 680/2000, Train Loss: 0.0980774387717247, Validation Loss: 0.1139163076877594\n",
      "Epoch: 681/2000, Train Loss: 0.09805867820978165, Validation Loss: 0.11368067562580109\n",
      "Epoch: 682/2000, Train Loss: 0.09812048077583313, Validation Loss: 0.11417265981435776\n",
      "Epoch: 683/2000, Train Loss: 0.09814111143350601, Validation Loss: 0.11362259089946747\n",
      "Epoch: 684/2000, Train Loss: 0.09806489199399948, Validation Loss: 0.11389818042516708\n",
      "Epoch: 685/2000, Train Loss: 0.0978749692440033, Validation Loss: 0.11340782046318054\n",
      "Epoch: 686/2000, Train Loss: 0.0976710394024849, Validation Loss: 0.11341552436351776\n",
      "Epoch: 687/2000, Train Loss: 0.09753207862377167, Validation Loss: 0.11345430463552475\n",
      "Epoch: 688/2000, Train Loss: 0.09748442471027374, Validation Loss: 0.11324877291917801\n",
      "Epoch: 689/2000, Train Loss: 0.09748230874538422, Validation Loss: 0.11356502771377563\n",
      "Epoch: 690/2000, Train Loss: 0.09745336323976517, Validation Loss: 0.11315249651670456\n",
      "Epoch: 691/2000, Train Loss: 0.09736629575490952, Validation Loss: 0.11333121359348297\n",
      "Epoch: 692/2000, Train Loss: 0.09722688794136047, Validation Loss: 0.1130187138915062\n",
      "Epoch: 693/2000, Train Loss: 0.0970895066857338, Validation Loss: 0.11299595981836319\n",
      "Epoch: 694/2000, Train Loss: 0.09699215739965439, Validation Loss: 0.11303392797708511\n",
      "Epoch: 695/2000, Train Loss: 0.09693865478038788, Validation Loss: 0.11283546686172485\n",
      "Epoch: 696/2000, Train Loss: 0.09690065681934357, Validation Loss: 0.11303898692131042\n",
      "Epoch: 697/2000, Train Loss: 0.09684323519468307, Validation Loss: 0.11273003369569778\n",
      "Epoch: 698/2000, Train Loss: 0.09675563126802444, Validation Loss: 0.11284782737493515\n",
      "Epoch: 699/2000, Train Loss: 0.09664636105298996, Validation Loss: 0.1126374825835228\n",
      "Epoch: 700/2000, Train Loss: 0.09654145687818527, Validation Loss: 0.11260518431663513\n",
      "Epoch: 701/2000, Train Loss: 0.09645717591047287, Validation Loss: 0.112615667283535\n",
      "Epoch: 702/2000, Train Loss: 0.0963933914899826, Validation Loss: 0.1124488115310669\n",
      "Epoch: 703/2000, Train Loss: 0.09633656591176987, Validation Loss: 0.11257148534059525\n",
      "Epoch: 704/2000, Train Loss: 0.09627094119787216, Validation Loss: 0.11233792454004288\n",
      "Epoch: 705/2000, Train Loss: 0.09619114547967911, Validation Loss: 0.11242014914751053\n",
      "Epoch: 706/2000, Train Loss: 0.09610050171613693, Validation Loss: 0.11224786937236786\n",
      "Epoch: 707/2000, Train Loss: 0.0960099846124649, Validation Loss: 0.11222707480192184\n",
      "Epoch: 708/2000, Train Loss: 0.09592755138874054, Validation Loss: 0.11219164729118347\n",
      "Epoch: 709/2000, Train Loss: 0.09585507959127426, Validation Loss: 0.11207099258899689\n",
      "Epoch: 710/2000, Train Loss: 0.09578825533390045, Validation Loss: 0.11212921142578125\n",
      "Epoch: 711/2000, Train Loss: 0.09572039544582367, Validation Loss: 0.11195281147956848\n",
      "Epoch: 712/2000, Train Loss: 0.09564723819494247, Validation Loss: 0.11201384663581848\n",
      "Epoch: 713/2000, Train Loss: 0.0955679714679718, Validation Loss: 0.11185496300458908\n",
      "Epoch: 714/2000, Train Loss: 0.09548589587211609, Validation Loss: 0.11185956001281738\n",
      "Epoch: 715/2000, Train Loss: 0.0954047217965126, Validation Loss: 0.11177584528923035\n",
      "Epoch: 716/2000, Train Loss: 0.09532716870307922, Validation Loss: 0.11171073466539383\n",
      "Epoch: 717/2000, Train Loss: 0.09525366127490997, Validation Loss: 0.11170580238103867\n",
      "Epoch: 718/2000, Train Loss: 0.09518255293369293, Validation Loss: 0.11158803850412369\n",
      "Epoch: 719/2000, Train Loss: 0.09511160105466843, Validation Loss: 0.11162072420120239\n",
      "Epoch: 720/2000, Train Loss: 0.09503893554210663, Validation Loss: 0.11148358136415482\n",
      "Epoch: 721/2000, Train Loss: 0.09496407210826874, Validation Loss: 0.11150627583265305\n",
      "Epoch: 722/2000, Train Loss: 0.09488743543624878, Validation Loss: 0.1113869920372963\n",
      "Epoch: 723/2000, Train Loss: 0.09481026977300644, Validation Loss: 0.11137310415506363\n",
      "Epoch: 724/2000, Train Loss: 0.09473371505737305, Validation Loss: 0.1112976223230362\n",
      "Epoch: 725/2000, Train Loss: 0.09465853869915009, Validation Loss: 0.11124312877655029\n",
      "Epoch: 726/2000, Train Loss: 0.09458478540182114, Validation Loss: 0.11121360212564468\n",
      "Epoch: 727/2000, Train Loss: 0.09451204538345337, Validation Loss: 0.11112648248672485\n",
      "Epoch: 728/2000, Train Loss: 0.09443973749876022, Validation Loss: 0.1111249178647995\n",
      "Epoch: 729/2000, Train Loss: 0.09436724334955215, Validation Loss: 0.11101935058832169\n",
      "Epoch: 730/2000, Train Loss: 0.09429430216550827, Validation Loss: 0.11102385073900223\n",
      "Epoch: 731/2000, Train Loss: 0.09422075003385544, Validation Loss: 0.1109176054596901\n",
      "Epoch: 732/2000, Train Loss: 0.09414676576852798, Validation Loss: 0.11091320961713791\n",
      "Epoch: 733/2000, Train Loss: 0.09407252818346024, Validation Loss: 0.11082115024328232\n",
      "Epoch: 734/2000, Train Loss: 0.09399833530187607, Validation Loss: 0.1107998713850975\n",
      "Epoch: 735/2000, Train Loss: 0.09392434358596802, Validation Loss: 0.1107284426689148\n",
      "Epoch: 736/2000, Train Loss: 0.09385070204734802, Validation Loss: 0.11068737506866455\n",
      "Epoch: 737/2000, Train Loss: 0.09377741813659668, Validation Loss: 0.11063623428344727\n",
      "Epoch: 738/2000, Train Loss: 0.09370449185371399, Validation Loss: 0.11057746410369873\n",
      "Epoch: 739/2000, Train Loss: 0.0936318039894104, Validation Loss: 0.11054325103759766\n",
      "Epoch: 740/2000, Train Loss: 0.09355930984020233, Validation Loss: 0.1104714572429657\n",
      "Epoch: 741/2000, Train Loss: 0.09348691254854202, Validation Loss: 0.11044909805059433\n",
      "Epoch: 742/2000, Train Loss: 0.09341457486152649, Validation Loss: 0.11036836355924606\n",
      "Epoch: 743/2000, Train Loss: 0.09334225207567215, Validation Loss: 0.11035283654928207\n",
      "Epoch: 744/2000, Train Loss: 0.09326993674039841, Validation Loss: 0.11026658862829208\n",
      "Epoch: 745/2000, Train Loss: 0.09319761395454407, Validation Loss: 0.11025520414113998\n",
      "Epoch: 746/2000, Train Loss: 0.09312529861927032, Validation Loss: 0.11016648262739182\n",
      "Epoch: 747/2000, Train Loss: 0.09305299818515778, Validation Loss: 0.11015857756137848\n",
      "Epoch: 748/2000, Train Loss: 0.09298071265220642, Validation Loss: 0.11006826162338257\n",
      "Epoch: 749/2000, Train Loss: 0.09290849417448044, Validation Loss: 0.11006364226341248\n",
      "Epoch: 750/2000, Train Loss: 0.09283628314733505, Validation Loss: 0.10997019708156586\n",
      "Epoch: 751/2000, Train Loss: 0.09276419132947922, Validation Loss: 0.10996976494789124\n",
      "Epoch: 752/2000, Train Loss: 0.09269219636917114, Validation Loss: 0.10987074673175812\n",
      "Epoch: 753/2000, Train Loss: 0.0926203802227974, Validation Loss: 0.10987772047519684\n",
      "Epoch: 754/2000, Train Loss: 0.09254879504442215, Validation Loss: 0.109769806265831\n",
      "Epoch: 755/2000, Train Loss: 0.09247756004333496, Validation Loss: 0.10978946834802628\n",
      "Epoch: 756/2000, Train Loss: 0.09240678697824478, Validation Loss: 0.1096673458814621\n",
      "Epoch: 757/2000, Train Loss: 0.0923367589712143, Validation Loss: 0.10970775783061981\n",
      "Epoch: 758/2000, Train Loss: 0.09226766228675842, Validation Loss: 0.10956378281116486\n",
      "Epoch: 759/2000, Train Loss: 0.09220009297132492, Validation Loss: 0.1096382886171341\n",
      "Epoch: 760/2000, Train Loss: 0.092134490609169, Validation Loss: 0.10946181416511536\n",
      "Epoch: 761/2000, Train Loss: 0.0920722633600235, Validation Loss: 0.10959215462207794\n",
      "Epoch: 762/2000, Train Loss: 0.0920143648982048, Validation Loss: 0.10936836898326874\n",
      "Epoch: 763/2000, Train Loss: 0.09196417778730392, Validation Loss: 0.109589584171772\n",
      "Epoch: 764/2000, Train Loss: 0.09192321449518204, Validation Loss: 0.10930118709802628\n",
      "Epoch: 765/2000, Train Loss: 0.0918993204832077, Validation Loss: 0.10966610908508301\n",
      "Epoch: 766/2000, Train Loss: 0.0918920487165451, Validation Loss: 0.10929703712463379\n",
      "Epoch: 767/2000, Train Loss: 0.09191656112670898, Validation Loss: 0.10985462367534637\n",
      "Epoch: 768/2000, Train Loss: 0.09195464849472046, Validation Loss: 0.10937785357236862\n",
      "Epoch: 769/2000, Train Loss: 0.09202385693788528, Validation Loss: 0.11005975306034088\n",
      "Epoch: 770/2000, Train Loss: 0.09204355627298355, Validation Loss: 0.10939650982618332\n",
      "Epoch: 771/2000, Train Loss: 0.09202510118484497, Validation Loss: 0.10989369451999664\n",
      "Epoch: 772/2000, Train Loss: 0.09184536337852478, Validation Loss: 0.10909011214971542\n",
      "Epoch: 773/2000, Train Loss: 0.0915878564119339, Validation Loss: 0.10922635346651077\n",
      "Epoch: 774/2000, Train Loss: 0.0912870392203331, Validation Loss: 0.10881836712360382\n",
      "Epoch: 775/2000, Train Loss: 0.09107893705368042, Validation Loss: 0.10877005010843277\n",
      "Epoch: 776/2000, Train Loss: 0.09100786596536636, Validation Loss: 0.10900864005088806\n",
      "Epoch: 777/2000, Train Loss: 0.09103845059871674, Validation Loss: 0.10874699056148529\n",
      "Epoch: 778/2000, Train Loss: 0.09109017997980118, Validation Loss: 0.1091664656996727\n",
      "Epoch: 779/2000, Train Loss: 0.09107258915901184, Validation Loss: 0.10867030173540115\n",
      "Epoch: 780/2000, Train Loss: 0.09097183495759964, Validation Loss: 0.1088729128241539\n",
      "Epoch: 781/2000, Train Loss: 0.09079606086015701, Validation Loss: 0.10849588364362717\n",
      "Epoch: 782/2000, Train Loss: 0.09062840789556503, Validation Loss: 0.10849540680646896\n",
      "Epoch: 783/2000, Train Loss: 0.09052154421806335, Validation Loss: 0.108550064265728\n",
      "Epoch: 784/2000, Train Loss: 0.09048453718423843, Validation Loss: 0.10837715119123459\n",
      "Epoch: 785/2000, Train Loss: 0.09047971665859222, Validation Loss: 0.10864449292421341\n",
      "Epoch: 786/2000, Train Loss: 0.09045419096946716, Validation Loss: 0.10830499976873398\n",
      "Epoch: 787/2000, Train Loss: 0.09038503468036652, Validation Loss: 0.10847379267215729\n",
      "Epoch: 788/2000, Train Loss: 0.09027210623025894, Validation Loss: 0.10818782448768616\n",
      "Epoch: 789/2000, Train Loss: 0.0901532843708992, Validation Loss: 0.10819772630929947\n",
      "Epoch: 790/2000, Train Loss: 0.09005845338106155, Validation Loss: 0.10817849636077881\n",
      "Epoch: 791/2000, Train Loss: 0.0899989902973175, Validation Loss: 0.10805042088031769\n",
      "Epoch: 792/2000, Train Loss: 0.08996129781007767, Validation Loss: 0.10820822417736053\n",
      "Epoch: 793/2000, Train Loss: 0.08992068469524384, Validation Loss: 0.10796895623207092\n",
      "Epoch: 794/2000, Train Loss: 0.08986140787601471, Validation Loss: 0.10810288041830063\n",
      "Epoch: 795/2000, Train Loss: 0.08977926522493362, Validation Loss: 0.10787858814001083\n",
      "Epoch: 796/2000, Train Loss: 0.0896882489323616, Validation Loss: 0.10790460556745529\n",
      "Epoch: 797/2000, Train Loss: 0.0896027684211731, Validation Loss: 0.10783203691244125\n",
      "Epoch: 798/2000, Train Loss: 0.08953262865543365, Validation Loss: 0.10775113105773926\n",
      "Epoch: 799/2000, Train Loss: 0.08947636187076569, Validation Loss: 0.10782041400671005\n",
      "Epoch: 800/2000, Train Loss: 0.08942496031522751, Validation Loss: 0.10765652358531952\n",
      "Epoch: 801/2000, Train Loss: 0.08936921507120132, Validation Loss: 0.10775551199913025\n",
      "Epoch: 802/2000, Train Loss: 0.08930346369743347, Validation Loss: 0.1075742244720459\n",
      "Epoch: 803/2000, Train Loss: 0.08922990411520004, Validation Loss: 0.10761889070272446\n",
      "Epoch: 804/2000, Train Loss: 0.08915335685014725, Validation Loss: 0.10750516504049301\n",
      "Epoch: 805/2000, Train Loss: 0.08908016979694366, Validation Loss: 0.10747310519218445\n",
      "Epoch: 806/2000, Train Loss: 0.08901336044073105, Validation Loss: 0.10745926201343536\n",
      "Epoch: 807/2000, Train Loss: 0.08895225822925568, Validation Loss: 0.10735999792814255\n",
      "Epoch: 808/2000, Train Loss: 0.08889365941286087, Validation Loss: 0.10740696638822556\n",
      "Epoch: 809/2000, Train Loss: 0.08883388340473175, Validation Loss: 0.1072697713971138\n",
      "Epoch: 810/2000, Train Loss: 0.08877088874578476, Validation Loss: 0.10731775313615799\n",
      "Epoch: 811/2000, Train Loss: 0.0887041762471199, Validation Loss: 0.1071881577372551\n",
      "Epoch: 812/2000, Train Loss: 0.0886354073882103, Validation Loss: 0.10720023512840271\n",
      "Epoch: 813/2000, Train Loss: 0.08856654912233353, Validation Loss: 0.10711802542209625\n",
      "Epoch: 814/2000, Train Loss: 0.08849938213825226, Validation Loss: 0.10708349943161011\n",
      "Epoch: 815/2000, Train Loss: 0.08843456208705902, Validation Loss: 0.10705753415822983\n",
      "Epoch: 816/2000, Train Loss: 0.08837177604436874, Validation Loss: 0.10698077082633972\n",
      "Epoch: 817/2000, Train Loss: 0.08831007033586502, Validation Loss: 0.10699095577001572\n",
      "Epoch: 818/2000, Train Loss: 0.0882483497262001, Validation Loss: 0.10688786953687668\n",
      "Epoch: 819/2000, Train Loss: 0.08818583190441132, Validation Loss: 0.10690705478191376\n",
      "Epoch: 820/2000, Train Loss: 0.0881221815943718, Validation Loss: 0.10680150985717773\n",
      "Epoch: 821/2000, Train Loss: 0.08805760741233826, Validation Loss: 0.10681010782718658\n",
      "Epoch: 822/2000, Train Loss: 0.08799250423908234, Validation Loss: 0.1067231073975563\n",
      "Epoch: 823/2000, Train Loss: 0.08792740106582642, Validation Loss: 0.10671015828847885\n",
      "Epoch: 824/2000, Train Loss: 0.08786269277334213, Validation Loss: 0.10665083676576614\n",
      "Epoch: 825/2000, Train Loss: 0.0877986028790474, Validation Loss: 0.10661270469427109\n",
      "Epoch: 826/2000, Train Loss: 0.08773516863584518, Validation Loss: 0.1065795049071312\n",
      "Epoch: 827/2000, Train Loss: 0.08767227083444595, Validation Loss: 0.10651952773332596\n",
      "Epoch: 828/2000, Train Loss: 0.0876096859574318, Validation Loss: 0.10650550574064255\n",
      "Epoch: 829/2000, Train Loss: 0.08754724264144897, Validation Loss: 0.10643121600151062\n",
      "Epoch: 830/2000, Train Loss: 0.08748476207256317, Validation Loss: 0.10642747581005096\n",
      "Epoch: 831/2000, Train Loss: 0.08742215484380722, Validation Loss: 0.10634671896696091\n",
      "Epoch: 832/2000, Train Loss: 0.08735939860343933, Validation Loss: 0.10634514689445496\n",
      "Epoch: 833/2000, Train Loss: 0.08729647099971771, Validation Loss: 0.10626450926065445\n",
      "Epoch: 834/2000, Train Loss: 0.08723345398902893, Validation Loss: 0.10625975579023361\n",
      "Epoch: 835/2000, Train Loss: 0.08717038482427597, Validation Loss: 0.10618395358324051\n",
      "Epoch: 836/2000, Train Loss: 0.08710732311010361, Validation Loss: 0.1061733067035675\n",
      "Epoch: 837/2000, Train Loss: 0.08704432100057602, Validation Loss: 0.10610441863536835\n",
      "Epoch: 838/2000, Train Loss: 0.0869813859462738, Validation Loss: 0.10608670115470886\n",
      "Epoch: 839/2000, Train Loss: 0.08691854774951935, Validation Loss: 0.10602478682994843\n",
      "Epoch: 840/2000, Train Loss: 0.08685580641031265, Validation Loss: 0.10600025206804276\n",
      "Epoch: 841/2000, Train Loss: 0.08679316937923431, Validation Loss: 0.10594464093446732\n",
      "Epoch: 842/2000, Train Loss: 0.08673061430454254, Validation Loss: 0.10591461509466171\n",
      "Epoch: 843/2000, Train Loss: 0.08666814118623734, Validation Loss: 0.10586433857679367\n",
      "Epoch: 844/2000, Train Loss: 0.0866057351231575, Validation Loss: 0.10583006590604782\n",
      "Epoch: 845/2000, Train Loss: 0.08654339611530304, Validation Loss: 0.10578366369009018\n",
      "Epoch: 846/2000, Train Loss: 0.08648110181093216, Validation Loss: 0.10574605315923691\n",
      "Epoch: 847/2000, Train Loss: 0.08641885966062546, Validation Loss: 0.10570237785577774\n",
      "Epoch: 848/2000, Train Loss: 0.08635665476322174, Validation Loss: 0.10566241294145584\n",
      "Epoch: 849/2000, Train Loss: 0.0862944945693016, Validation Loss: 0.10562095791101456\n",
      "Epoch: 850/2000, Train Loss: 0.08623237907886505, Validation Loss: 0.1055794283747673\n",
      "Epoch: 851/2000, Train Loss: 0.08617029339075089, Validation Loss: 0.10553982108831406\n",
      "Epoch: 852/2000, Train Loss: 0.08610823005437851, Validation Loss: 0.10549689084291458\n",
      "Epoch: 853/2000, Train Loss: 0.08604621887207031, Validation Loss: 0.10545886307954788\n",
      "Epoch: 854/2000, Train Loss: 0.08598422259092331, Validation Loss: 0.10541439801454544\n",
      "Epoch: 855/2000, Train Loss: 0.08592228591442108, Validation Loss: 0.10537828505039215\n",
      "Epoch: 856/2000, Train Loss: 0.08586037158966064, Validation Loss: 0.10533205419778824\n",
      "Epoch: 857/2000, Train Loss: 0.0857984721660614, Validation Loss: 0.105298712849617\n",
      "Epoch: 858/2000, Train Loss: 0.08573663979768753, Validation Loss: 0.10524990409612656\n",
      "Epoch: 859/2000, Train Loss: 0.08567484468221664, Validation Loss: 0.10522051900625229\n",
      "Epoch: 860/2000, Train Loss: 0.08561310172080994, Validation Loss: 0.10516739636659622\n",
      "Epoch: 861/2000, Train Loss: 0.085551418364048, Validation Loss: 0.1051441878080368\n",
      "Epoch: 862/2000, Train Loss: 0.08548983186483383, Validation Loss: 0.1050838977098465\n",
      "Epoch: 863/2000, Train Loss: 0.08542840927839279, Validation Loss: 0.10507111996412277\n",
      "Epoch: 864/2000, Train Loss: 0.08536723256111145, Validation Loss: 0.10499898344278336\n",
      "Epoch: 865/2000, Train Loss: 0.08530648052692413, Validation Loss: 0.10500436276197433\n",
      "Epoch: 866/2000, Train Loss: 0.08524642884731293, Validation Loss: 0.10491224378347397\n",
      "Epoch: 867/2000, Train Loss: 0.0851876363158226, Validation Loss: 0.10495033115148544\n",
      "Epoch: 868/2000, Train Loss: 0.0851309597492218, Validation Loss: 0.10482524335384369\n",
      "Epoch: 869/2000, Train Loss: 0.08507820218801498, Validation Loss: 0.10492587089538574\n",
      "Epoch: 870/2000, Train Loss: 0.08503204584121704, Validation Loss: 0.1047503873705864\n",
      "Epoch: 871/2000, Train Loss: 0.08499851077795029, Validation Loss: 0.10497751832008362\n",
      "Epoch: 872/2000, Train Loss: 0.08498528599739075, Validation Loss: 0.10473903268575668\n",
      "Epoch: 873/2000, Train Loss: 0.08501231670379639, Validation Loss: 0.10522730648517609\n",
      "Epoch: 874/2000, Train Loss: 0.08509513735771179, Validation Loss: 0.10494665056467056\n",
      "Epoch: 875/2000, Train Loss: 0.08528777211904526, Validation Loss: 0.10588784515857697\n",
      "Epoch: 876/2000, Train Loss: 0.0855627954006195, Validation Loss: 0.10554700344800949\n",
      "Epoch: 877/2000, Train Loss: 0.08597675710916519, Validation Loss: 0.10664670169353485\n",
      "Epoch: 878/2000, Train Loss: 0.08614841848611832, Validation Loss: 0.10564756393432617\n",
      "Epoch: 879/2000, Train Loss: 0.08604636788368225, Validation Loss: 0.10568657517433167\n",
      "Epoch: 880/2000, Train Loss: 0.0852755680680275, Validation Loss: 0.10445333272218704\n",
      "Epoch: 881/2000, Train Loss: 0.08452840894460678, Validation Loss: 0.1043451651930809\n",
      "Epoch: 882/2000, Train Loss: 0.08430242538452148, Validation Loss: 0.10496212542057037\n",
      "Epoch: 883/2000, Train Loss: 0.08461768925189972, Validation Loss: 0.10477916896343231\n",
      "Epoch: 884/2000, Train Loss: 0.0849691703915596, Validation Loss: 0.10527495294809341\n",
      "Epoch: 885/2000, Train Loss: 0.0848146378993988, Validation Loss: 0.10431668162345886\n",
      "Epoch: 886/2000, Train Loss: 0.08434540778398514, Validation Loss: 0.10421253740787506\n",
      "Epoch: 887/2000, Train Loss: 0.08399885147809982, Validation Loss: 0.10440506041049957\n",
      "Epoch: 888/2000, Train Loss: 0.08406835049390793, Validation Loss: 0.10429119318723679\n",
      "Epoch: 889/2000, Train Loss: 0.08429303765296936, Validation Loss: 0.10473693907260895\n",
      "Epoch: 890/2000, Train Loss: 0.08424869924783707, Validation Loss: 0.10407703369855881\n",
      "Epoch: 891/2000, Train Loss: 0.0839582160115242, Validation Loss: 0.10404335707426071\n",
      "Epoch: 892/2000, Train Loss: 0.0837145671248436, Validation Loss: 0.10415959358215332\n",
      "Epoch: 893/2000, Train Loss: 0.08373074233531952, Validation Loss: 0.10401997715234756\n",
      "Epoch: 894/2000, Train Loss: 0.08384474366903305, Validation Loss: 0.10434146225452423\n",
      "Epoch: 895/2000, Train Loss: 0.08378959447145462, Validation Loss: 0.10385870933532715\n",
      "Epoch: 896/2000, Train Loss: 0.0835859477519989, Validation Loss: 0.10384166985750198\n",
      "Epoch: 897/2000, Train Loss: 0.08342935889959335, Validation Loss: 0.10393664985895157\n",
      "Epoch: 898/2000, Train Loss: 0.08343092352151871, Validation Loss: 0.10377131402492523\n",
      "Epoch: 899/2000, Train Loss: 0.08347378671169281, Validation Loss: 0.10400047153234482\n",
      "Epoch: 900/2000, Train Loss: 0.08340476453304291, Validation Loss: 0.10365550965070724\n",
      "Epoch: 901/2000, Train Loss: 0.08325554430484772, Validation Loss: 0.1036480963230133\n",
      "Epoch: 902/2000, Train Loss: 0.08315020054578781, Validation Loss: 0.10373040288686752\n",
      "Epoch: 903/2000, Train Loss: 0.08313778787851334, Validation Loss: 0.10356359928846359\n",
      "Epoch: 904/2000, Train Loss: 0.08313777297735214, Validation Loss: 0.1037248969078064\n",
      "Epoch: 905/2000, Train Loss: 0.08306678384542465, Validation Loss: 0.10347156226634979\n",
      "Epoch: 906/2000, Train Loss: 0.08295400440692902, Validation Loss: 0.10346244275569916\n",
      "Epoch: 907/2000, Train Loss: 0.0828738659620285, Validation Loss: 0.10352517664432526\n",
      "Epoch: 908/2000, Train Loss: 0.08284731954336166, Validation Loss: 0.10337680578231812\n",
      "Epoch: 909/2000, Train Loss: 0.08282352238893509, Validation Loss: 0.10349816828966141\n",
      "Epoch: 910/2000, Train Loss: 0.08275695145130157, Validation Loss: 0.10330958664417267\n",
      "Epoch: 911/2000, Train Loss: 0.08266746252775192, Validation Loss: 0.10329816490411758\n",
      "Epoch: 912/2000, Train Loss: 0.08259864896535873, Validation Loss: 0.10333438217639923\n",
      "Epoch: 913/2000, Train Loss: 0.08256109058856964, Validation Loss: 0.10320384800434113\n",
      "Epoch: 914/2000, Train Loss: 0.08252489566802979, Validation Loss: 0.1032911166548729\n",
      "Epoch: 915/2000, Train Loss: 0.08246393501758575, Validation Loss: 0.10313846915960312\n",
      "Epoch: 916/2000, Train Loss: 0.082388736307621, Validation Loss: 0.10312581062316895\n",
      "Epoch: 917/2000, Train Loss: 0.08232450485229492, Validation Loss: 0.10313477367162704\n",
      "Epoch: 918/2000, Train Loss: 0.08227898180484772, Validation Loss: 0.10302688926458359\n",
      "Epoch: 919/2000, Train Loss: 0.08223621547222137, Validation Loss: 0.10309109836816788\n",
      "Epoch: 920/2000, Train Loss: 0.08218000829219818, Validation Loss: 0.1029660776257515\n",
      "Epoch: 921/2000, Train Loss: 0.0821138322353363, Validation Loss: 0.10296010971069336\n",
      "Epoch: 922/2000, Train Loss: 0.08205188810825348, Validation Loss: 0.10294807702302933\n",
      "Epoch: 923/2000, Train Loss: 0.08200109004974365, Validation Loss: 0.10286304354667664\n",
      "Epoch: 924/2000, Train Loss: 0.08195425570011139, Validation Loss: 0.10290654748678207\n",
      "Epoch: 925/2000, Train Loss: 0.08190116286277771, Validation Loss: 0.10279776901006699\n",
      "Epoch: 926/2000, Train Loss: 0.08184096962213516, Validation Loss: 0.10279493033885956\n",
      "Epoch: 927/2000, Train Loss: 0.08178089559078217, Validation Loss: 0.10276103764772415\n",
      "Epoch: 928/2000, Train Loss: 0.08172682672739029, Validation Loss: 0.10269510000944138\n",
      "Epoch: 929/2000, Train Loss: 0.08167695254087448, Validation Loss: 0.10271928459405899\n",
      "Epoch: 930/2000, Train Loss: 0.0816253125667572, Validation Loss: 0.10262635350227356\n",
      "Epoch: 931/2000, Train Loss: 0.08156918734312057, Validation Loss: 0.10262902826070786\n",
      "Epoch: 932/2000, Train Loss: 0.08151119202375412, Validation Loss: 0.10258051753044128\n",
      "Epoch: 933/2000, Train Loss: 0.0814555436372757, Validation Loss: 0.10253395885229111\n",
      "Epoch: 934/2000, Train Loss: 0.08140319585800171, Validation Loss: 0.10253935307264328\n",
      "Epoch: 935/2000, Train Loss: 0.08135157078504562, Validation Loss: 0.10246217995882034\n",
      "Epoch: 936/2000, Train Loss: 0.08129794895648956, Validation Loss: 0.10246676951646805\n",
      "Epoch: 937/2000, Train Loss: 0.08124218881130219, Validation Loss: 0.10240843147039413\n",
      "Epoch: 938/2000, Train Loss: 0.08118642866611481, Validation Loss: 0.1023775115609169\n",
      "Epoch: 939/2000, Train Loss: 0.0811324194073677, Validation Loss: 0.1023625060915947\n",
      "Epoch: 940/2000, Train Loss: 0.0810798704624176, Validation Loss: 0.10230013728141785\n",
      "Epoch: 941/2000, Train Loss: 0.08102720230817795, Validation Loss: 0.10229996591806412\n",
      "Epoch: 942/2000, Train Loss: 0.08097328990697861, Validation Loss: 0.10223778337240219\n",
      "Epoch: 943/2000, Train Loss: 0.08091846108436584, Validation Loss: 0.10221853107213974\n",
      "Epoch: 944/2000, Train Loss: 0.08086390048265457, Validation Loss: 0.10218564420938492\n",
      "Epoch: 945/2000, Train Loss: 0.0808103010058403, Validation Loss: 0.10213922709226608\n",
      "Epoch: 946/2000, Train Loss: 0.08075740933418274, Validation Loss: 0.10212956368923187\n",
      "Epoch: 947/2000, Train Loss: 0.08070439100265503, Validation Loss: 0.1020716056227684\n",
      "Epoch: 948/2000, Train Loss: 0.0806507095694542, Validation Loss: 0.10205889493227005\n",
      "Epoch: 949/2000, Train Loss: 0.08059658110141754, Validation Loss: 0.10201378911733627\n",
      "Epoch: 950/2000, Train Loss: 0.08054258674383163, Validation Loss: 0.10198180377483368\n",
      "Epoch: 951/2000, Train Loss: 0.08048907667398453, Validation Loss: 0.10195830464363098\n",
      "Epoch: 952/2000, Train Loss: 0.08043598383665085, Validation Loss: 0.10190954059362411\n",
      "Epoch: 953/2000, Train Loss: 0.08038288354873657, Validation Loss: 0.10189495235681534\n",
      "Epoch: 954/2000, Train Loss: 0.08032950758934021, Validation Loss: 0.10184508562088013\n",
      "Epoch: 955/2000, Train Loss: 0.08027587085962296, Validation Loss: 0.1018228605389595\n",
      "Epoch: 956/2000, Train Loss: 0.0802222192287445, Validation Loss: 0.10178631544113159\n",
      "Epoch: 957/2000, Train Loss: 0.08016878366470337, Validation Loss: 0.10174985975027084\n",
      "Epoch: 958/2000, Train Loss: 0.08011562377214432, Validation Loss: 0.10172710567712784\n",
      "Epoch: 959/2000, Train Loss: 0.08006257563829422, Validation Loss: 0.10168146342039108\n",
      "Epoch: 960/2000, Train Loss: 0.08000944554805756, Validation Loss: 0.10166191309690475\n",
      "Epoch: 961/2000, Train Loss: 0.07995618879795074, Validation Loss: 0.10161826759576797\n",
      "Epoch: 962/2000, Train Loss: 0.07990287244319916, Validation Loss: 0.1015918105840683\n",
      "Epoch: 963/2000, Train Loss: 0.07984961569309235, Validation Loss: 0.101558156311512\n",
      "Epoch: 964/2000, Train Loss: 0.07979650050401688, Validation Loss: 0.10152191668748856\n",
      "Epoch: 965/2000, Train Loss: 0.07974350452423096, Validation Loss: 0.10149721801280975\n",
      "Epoch: 966/2000, Train Loss: 0.07969056069850922, Validation Loss: 0.10145547240972519\n",
      "Epoch: 967/2000, Train Loss: 0.07963757961988449, Validation Loss: 0.1014326810836792\n",
      "Epoch: 968/2000, Train Loss: 0.07958455383777618, Validation Loss: 0.10139264911413193\n",
      "Epoch: 969/2000, Train Loss: 0.07953149825334549, Validation Loss: 0.10136527568101883\n",
      "Epoch: 970/2000, Train Loss: 0.07947847992181778, Validation Loss: 0.10133185237646103\n",
      "Epoch: 971/2000, Train Loss: 0.07942552119493484, Validation Loss: 0.10129785537719727\n",
      "Epoch: 972/2000, Train Loss: 0.07937263697385788, Validation Loss: 0.10127073526382446\n",
      "Epoch: 973/2000, Train Loss: 0.0793197751045227, Validation Loss: 0.10123256593942642\n",
      "Epoch: 974/2000, Train Loss: 0.07926691323518753, Validation Loss: 0.10120773315429688\n",
      "Epoch: 975/2000, Train Loss: 0.07921403646469116, Validation Loss: 0.10116999596357346\n",
      "Epoch: 976/2000, Train Loss: 0.0791611447930336, Validation Loss: 0.1011432409286499\n",
      "Epoch: 977/2000, Train Loss: 0.07910826057195663, Validation Loss: 0.10110945999622345\n",
      "Epoch: 978/2000, Train Loss: 0.07905540615320206, Validation Loss: 0.10107866674661636\n",
      "Epoch: 979/2000, Train Loss: 0.07900258153676987, Validation Loss: 0.10104946047067642\n",
      "Epoch: 980/2000, Train Loss: 0.07894978672266006, Validation Loss: 0.10101509839296341\n",
      "Epoch: 981/2000, Train Loss: 0.07889701426029205, Validation Loss: 0.10098858177661896\n",
      "Epoch: 982/2000, Train Loss: 0.07884424179792404, Validation Loss: 0.10095296800136566\n",
      "Epoch: 983/2000, Train Loss: 0.07879148423671722, Validation Loss: 0.1009264662861824\n",
      "Epoch: 984/2000, Train Loss: 0.07873871922492981, Validation Loss: 0.10089205205440521\n",
      "Epoch: 985/2000, Train Loss: 0.07868597656488419, Validation Loss: 0.10086352378129959\n",
      "Epoch: 986/2000, Train Loss: 0.07863324880599976, Validation Loss: 0.10083167254924774\n",
      "Epoch: 987/2000, Train Loss: 0.07858054339885712, Validation Loss: 0.10080043971538544\n",
      "Epoch: 988/2000, Train Loss: 0.07852787524461746, Validation Loss: 0.10077112168073654\n",
      "Epoch: 989/2000, Train Loss: 0.0784752294421196, Validation Loss: 0.10073789954185486\n",
      "Epoch: 990/2000, Train Loss: 0.07842259854078293, Validation Loss: 0.10071013867855072\n",
      "Epoch: 991/2000, Train Loss: 0.07836999744176865, Validation Loss: 0.10067638009786606\n",
      "Epoch: 992/2000, Train Loss: 0.07831742614507675, Validation Loss: 0.1006489023566246\n",
      "Epoch: 993/2000, Train Loss: 0.07826486974954605, Validation Loss: 0.10061593353748322\n",
      "Epoch: 994/2000, Train Loss: 0.07821234315633774, Validation Loss: 0.10058768093585968\n",
      "Epoch: 995/2000, Train Loss: 0.07815983891487122, Validation Loss: 0.10055630654096603\n",
      "Epoch: 996/2000, Train Loss: 0.07810737192630768, Validation Loss: 0.10052680969238281\n",
      "Epoch: 997/2000, Train Loss: 0.07805492728948593, Validation Loss: 0.10049712657928467\n",
      "Epoch: 998/2000, Train Loss: 0.07800250500440598, Validation Loss: 0.10046643018722534\n",
      "Epoch: 999/2000, Train Loss: 0.077950119972229, Validation Loss: 0.10043800622224808\n",
      "Epoch: 1000/2000, Train Loss: 0.07789774984121323, Validation Loss: 0.10040648281574249\n",
      "Epoch: 1001/2000, Train Loss: 0.07784540951251984, Validation Loss: 0.10037866234779358\n",
      "Epoch: 1002/2000, Train Loss: 0.07779308408498764, Validation Loss: 0.10034685581922531\n",
      "Epoch: 1003/2000, Train Loss: 0.07774077355861664, Validation Loss: 0.10031906515359879\n",
      "Epoch: 1004/2000, Train Loss: 0.07768848538398743, Validation Loss: 0.1002875342965126\n",
      "Epoch: 1005/2000, Train Loss: 0.07763620465993881, Validation Loss: 0.10025942325592041\n",
      "Epoch: 1006/2000, Train Loss: 0.07758394628763199, Validation Loss: 0.10022855550050735\n",
      "Epoch: 1007/2000, Train Loss: 0.07753168791532516, Validation Loss: 0.10019996017217636\n",
      "Epoch: 1008/2000, Train Loss: 0.07747945934534073, Validation Loss: 0.10016994178295135\n",
      "Epoch: 1009/2000, Train Loss: 0.07742723822593689, Validation Loss: 0.10014091432094574\n",
      "Epoch: 1010/2000, Train Loss: 0.07737505435943604, Validation Loss: 0.10011176764965057\n",
      "Epoch: 1011/2000, Train Loss: 0.07732287049293518, Validation Loss: 0.1000823825597763\n",
      "Epoch: 1012/2000, Train Loss: 0.07727070897817612, Validation Loss: 0.10005390644073486\n",
      "Epoch: 1013/2000, Train Loss: 0.07721856236457825, Validation Loss: 0.10002423077821732\n",
      "Epoch: 1014/2000, Train Loss: 0.07716643065214157, Validation Loss: 0.09999622404575348\n",
      "Epoch: 1015/2000, Train Loss: 0.0771142989397049, Validation Loss: 0.09996632486581802\n",
      "Epoch: 1016/2000, Train Loss: 0.07706218212842941, Validation Loss: 0.09993863105773926\n",
      "Epoch: 1017/2000, Train Loss: 0.07701006531715393, Validation Loss: 0.09990864992141724\n",
      "Epoch: 1018/2000, Train Loss: 0.07695796340703964, Validation Loss: 0.09988121688365936\n",
      "Epoch: 1019/2000, Train Loss: 0.07690587639808655, Validation Loss: 0.09985129535198212\n",
      "Epoch: 1020/2000, Train Loss: 0.07685380429029465, Validation Loss: 0.09982416033744812\n",
      "Epoch: 1021/2000, Train Loss: 0.07680173963308334, Validation Loss: 0.09979438781738281\n",
      "Epoch: 1022/2000, Train Loss: 0.07674971222877502, Validation Loss: 0.09976761788129807\n",
      "Epoch: 1023/2000, Train Loss: 0.0766976922750473, Validation Loss: 0.09973803162574768\n",
      "Epoch: 1024/2000, Train Loss: 0.07664570957422256, Validation Loss: 0.09971165657043457\n",
      "Epoch: 1025/2000, Train Loss: 0.07659374922513962, Validation Loss: 0.09968212991952896\n",
      "Epoch: 1026/2000, Train Loss: 0.07654182612895966, Validation Loss: 0.09965615719556808\n",
      "Epoch: 1027/2000, Train Loss: 0.07648993283510208, Validation Loss: 0.09962650388479233\n",
      "Epoch: 1028/2000, Train Loss: 0.0764380693435669, Validation Loss: 0.09960095584392548\n",
      "Epoch: 1029/2000, Train Loss: 0.07638625055551529, Validation Loss: 0.09957097470760345\n",
      "Epoch: 1030/2000, Train Loss: 0.07633446902036667, Validation Loss: 0.09954600036144257\n",
      "Epoch: 1031/2000, Train Loss: 0.07628271728754044, Validation Loss: 0.0995153859257698\n",
      "Epoch: 1032/2000, Train Loss: 0.07623100280761719, Validation Loss: 0.09949126094579697\n",
      "Epoch: 1033/2000, Train Loss: 0.07617933303117752, Validation Loss: 0.09945966303348541\n",
      "Epoch: 1034/2000, Train Loss: 0.07612770050764084, Validation Loss: 0.09943679720163345\n",
      "Epoch: 1035/2000, Train Loss: 0.07607611268758774, Validation Loss: 0.09940364211797714\n",
      "Epoch: 1036/2000, Train Loss: 0.07602456957101822, Validation Loss: 0.09938272833824158\n",
      "Epoch: 1037/2000, Train Loss: 0.07597307115793228, Validation Loss: 0.09934704750776291\n",
      "Epoch: 1038/2000, Train Loss: 0.07592163234949112, Validation Loss: 0.09932923316955566\n",
      "Epoch: 1039/2000, Train Loss: 0.07587024569511414, Validation Loss: 0.09928955882787704\n",
      "Epoch: 1040/2000, Train Loss: 0.07581895589828491, Validation Loss: 0.09927678108215332\n",
      "Epoch: 1041/2000, Train Loss: 0.07576776295900345, Validation Loss: 0.09923069924116135\n",
      "Epoch: 1042/2000, Train Loss: 0.07571673393249512, Validation Loss: 0.0992264598608017\n",
      "Epoch: 1043/2000, Train Loss: 0.07566593587398529, Validation Loss: 0.09917010366916656\n",
      "Epoch: 1044/2000, Train Loss: 0.07561551034450531, Validation Loss: 0.09918072074651718\n",
      "Epoch: 1045/2000, Train Loss: 0.07556569576263428, Validation Loss: 0.09910768270492554\n",
      "Epoch: 1046/2000, Train Loss: 0.07551688700914383, Validation Loss: 0.09914496541023254\n",
      "Epoch: 1047/2000, Train Loss: 0.07546976208686829, Validation Loss: 0.09904512017965317\n",
      "Epoch: 1048/2000, Train Loss: 0.0754256471991539, Validation Loss: 0.09913225471973419\n",
      "Epoch: 1049/2000, Train Loss: 0.07538646459579468, Validation Loss: 0.09899165481328964\n",
      "Epoch: 1050/2000, Train Loss: 0.07535658776760101, Validation Loss: 0.09917684644460678\n",
      "Epoch: 1051/2000, Train Loss: 0.07534142583608627, Validation Loss: 0.09898355603218079\n",
      "Epoch: 1052/2000, Train Loss: 0.07535523176193237, Validation Loss: 0.09936902672052383\n",
      "Epoch: 1053/2000, Train Loss: 0.07541022449731827, Validation Loss: 0.099136583507061\n",
      "Epoch: 1054/2000, Train Loss: 0.0755486711859703, Validation Loss: 0.09990264475345612\n",
      "Epoch: 1055/2000, Train Loss: 0.07576949149370193, Validation Loss: 0.09967073798179626\n",
      "Epoch: 1056/2000, Train Loss: 0.07615375518798828, Validation Loss: 0.10082357376813889\n",
      "Epoch: 1057/2000, Train Loss: 0.07648776471614838, Validation Loss: 0.10027045011520386\n",
      "Epoch: 1058/2000, Train Loss: 0.07678240537643433, Validation Loss: 0.10078044980764389\n",
      "Epoch: 1059/2000, Train Loss: 0.07639188319444656, Validation Loss: 0.09933220595121384\n",
      "Epoch: 1060/2000, Train Loss: 0.07566417753696442, Validation Loss: 0.09901702404022217\n",
      "Epoch: 1061/2000, Train Loss: 0.07490471005439758, Validation Loss: 0.09883933514356613\n",
      "Epoch: 1062/2000, Train Loss: 0.07476130872964859, Validation Loss: 0.09894455969333649\n",
      "Epoch: 1063/2000, Train Loss: 0.0751444548368454, Validation Loss: 0.0998215302824974\n",
      "Epoch: 1064/2000, Train Loss: 0.07544752955436707, Validation Loss: 0.09912125021219254\n",
      "Epoch: 1065/2000, Train Loss: 0.0753309428691864, Validation Loss: 0.09908617287874222\n",
      "Epoch: 1066/2000, Train Loss: 0.07480866461992264, Validation Loss: 0.09855917096138\n",
      "Epoch: 1067/2000, Train Loss: 0.07447521388530731, Validation Loss: 0.09856519848108292\n",
      "Epoch: 1068/2000, Train Loss: 0.07456475496292114, Validation Loss: 0.09916488826274872\n",
      "Epoch: 1069/2000, Train Loss: 0.07479386031627655, Validation Loss: 0.09874767065048218\n",
      "Epoch: 1070/2000, Train Loss: 0.07479744404554367, Validation Loss: 0.09885386377573013\n",
      "Epoch: 1071/2000, Train Loss: 0.07449381798505783, Validation Loss: 0.09843916445970535\n",
      "Epoch: 1072/2000, Train Loss: 0.07424259930849075, Validation Loss: 0.09842539578676224\n",
      "Epoch: 1073/2000, Train Loss: 0.074251189827919, Validation Loss: 0.0988280400633812\n",
      "Epoch: 1074/2000, Train Loss: 0.07437577098608017, Validation Loss: 0.09850116074085236\n",
      "Epoch: 1075/2000, Train Loss: 0.07436853647232056, Validation Loss: 0.0986180528998375\n",
      "Epoch: 1076/2000, Train Loss: 0.07417242228984833, Validation Loss: 0.09831531345844269\n",
      "Epoch: 1077/2000, Train Loss: 0.07400205731391907, Validation Loss: 0.09828177839517593\n",
      "Epoch: 1078/2000, Train Loss: 0.07398898899555206, Validation Loss: 0.09855683892965317\n",
      "Epoch: 1079/2000, Train Loss: 0.07404538244009018, Validation Loss: 0.09828798472881317\n",
      "Epoch: 1080/2000, Train Loss: 0.0740189328789711, Validation Loss: 0.09839987009763718\n",
      "Epoch: 1081/2000, Train Loss: 0.07388227432966232, Validation Loss: 0.09819141775369644\n",
      "Epoch: 1082/2000, Train Loss: 0.07376357167959213, Validation Loss: 0.09815575182437897\n",
      "Epoch: 1083/2000, Train Loss: 0.0737391784787178, Validation Loss: 0.09834866225719452\n",
      "Epoch: 1084/2000, Train Loss: 0.0737541988492012, Validation Loss: 0.0981311947107315\n",
      "Epoch: 1085/2000, Train Loss: 0.07371831685304642, Validation Loss: 0.09822928160429001\n",
      "Epoch: 1086/2000, Train Loss: 0.07361814379692078, Validation Loss: 0.0980733186006546\n",
      "Epoch: 1087/2000, Train Loss: 0.07352810353040695, Validation Loss: 0.09803647547960281\n",
      "Epoch: 1088/2000, Train Loss: 0.07349365949630737, Validation Loss: 0.09816647320985794\n",
      "Epoch: 1089/2000, Train Loss: 0.0734839215874672, Validation Loss: 0.09799302369356155\n",
      "Epoch: 1090/2000, Train Loss: 0.0734451487660408, Validation Loss: 0.09807679057121277\n",
      "Epoch: 1091/2000, Train Loss: 0.07336794584989548, Validation Loss: 0.09795276820659637\n",
      "Epoch: 1092/2000, Train Loss: 0.07329423725605011, Validation Loss: 0.0979200005531311\n",
      "Epoch: 1093/2000, Train Loss: 0.07325255125761032, Validation Loss: 0.09800256788730621\n",
      "Epoch: 1094/2000, Train Loss: 0.07322796434164047, Validation Loss: 0.0978640466928482\n",
      "Epoch: 1095/2000, Train Loss: 0.07318876683712006, Validation Loss: 0.09793475270271301\n",
      "Epoch: 1096/2000, Train Loss: 0.07312610745429993, Validation Loss: 0.09782694280147552\n",
      "Epoch: 1097/2000, Train Loss: 0.0730617418885231, Validation Loss: 0.09780396521091461\n",
      "Epoch: 1098/2000, Train Loss: 0.07301469147205353, Validation Loss: 0.09784935414791107\n",
      "Epoch: 1099/2000, Train Loss: 0.07298058271408081, Validation Loss: 0.0977432131767273\n",
      "Epoch: 1100/2000, Train Loss: 0.07294127345085144, Validation Loss: 0.09780395776033401\n",
      "Epoch: 1101/2000, Train Loss: 0.07288790494203568, Validation Loss: 0.09770848602056503\n",
      "Epoch: 1102/2000, Train Loss: 0.07282997667789459, Validation Loss: 0.09769769757986069\n",
      "Epoch: 1103/2000, Train Loss: 0.07278004288673401, Validation Loss: 0.09771176427602768\n",
      "Epoch: 1104/2000, Train Loss: 0.07273952662944794, Validation Loss: 0.09763059765100479\n",
      "Epoch: 1105/2000, Train Loss: 0.07269924879074097, Validation Loss: 0.09767612814903259\n",
      "Epoch: 1106/2000, Train Loss: 0.07265154272317886, Validation Loss: 0.09758665412664413\n",
      "Epoch: 1107/2000, Train Loss: 0.07259867340326309, Validation Loss: 0.09758399426937103\n",
      "Epoch: 1108/2000, Train Loss: 0.07254793494939804, Validation Loss: 0.09757106751203537\n",
      "Epoch: 1109/2000, Train Loss: 0.07250309735536575, Validation Loss: 0.09751347452402115\n",
      "Epoch: 1110/2000, Train Loss: 0.07246096432209015, Validation Loss: 0.09754642099142075\n",
      "Epoch: 1111/2000, Train Loss: 0.07241616398096085, Validation Loss: 0.09746991842985153\n",
      "Epoch: 1112/2000, Train Loss: 0.07236732542514801, Validation Loss: 0.09747733175754547\n",
      "Epoch: 1113/2000, Train Loss: 0.07231748104095459, Validation Loss: 0.09744485467672348\n",
      "Epoch: 1114/2000, Train Loss: 0.07227024435997009, Validation Loss: 0.09740648418664932\n",
      "Epoch: 1115/2000, Train Loss: 0.07222586870193481, Validation Loss: 0.09742061048746109\n",
      "Epoch: 1116/2000, Train Loss: 0.07218175381422043, Validation Loss: 0.09735605865716934\n",
      "Epoch: 1117/2000, Train Loss: 0.07213561981916428, Validation Loss: 0.09736667573451996\n",
      "Epoch: 1118/2000, Train Loss: 0.07208764553070068, Validation Loss: 0.09732118248939514\n",
      "Epoch: 1119/2000, Train Loss: 0.07203983515501022, Validation Loss: 0.09729927033185959\n",
      "Epoch: 1120/2000, Train Loss: 0.07199365645647049, Validation Loss: 0.0972939059138298\n",
      "Epoch: 1121/2000, Train Loss: 0.07194878160953522, Validation Loss: 0.09724435955286026\n",
      "Epoch: 1122/2000, Train Loss: 0.07190374284982681, Validation Loss: 0.09725262224674225\n",
      "Epoch: 1123/2000, Train Loss: 0.07185754925012589, Validation Loss: 0.09720338135957718\n",
      "Epoch: 1124/2000, Train Loss: 0.0718105211853981, Validation Loss: 0.09719403833150864\n",
      "Epoch: 1125/2000, Train Loss: 0.07176368683576584, Validation Loss: 0.09717081487178802\n",
      "Epoch: 1126/2000, Train Loss: 0.07171773910522461, Validation Loss: 0.09713638573884964\n",
      "Epoch: 1127/2000, Train Loss: 0.0716724619269371, Validation Loss: 0.09713472425937653\n",
      "Epoch: 1128/2000, Train Loss: 0.07162710279226303, Validation Loss: 0.09708918631076813\n",
      "Epoch: 1129/2000, Train Loss: 0.071581169962883, Validation Loss: 0.09708584100008011\n",
      "Epoch: 1130/2000, Train Loss: 0.07153478264808655, Validation Loss: 0.09705107659101486\n",
      "Epoch: 1131/2000, Train Loss: 0.07148844003677368, Validation Loss: 0.09703074395656586\n",
      "Epoch: 1132/2000, Train Loss: 0.07144252955913544, Validation Loss: 0.09701572358608246\n",
      "Epoch: 1133/2000, Train Loss: 0.07139702141284943, Validation Loss: 0.09697972238063812\n",
      "Epoch: 1134/2000, Train Loss: 0.0713515654206276, Validation Loss: 0.09697404503822327\n",
      "Epoch: 1135/2000, Train Loss: 0.07130587100982666, Validation Loss: 0.09693563729524612\n",
      "Epoch: 1136/2000, Train Loss: 0.07125990837812424, Validation Loss: 0.09692398458719254\n",
      "Epoch: 1137/2000, Train Loss: 0.07121388614177704, Validation Loss: 0.09689656645059586\n",
      "Epoch: 1138/2000, Train Loss: 0.07116801291704178, Validation Loss: 0.09687214344739914\n",
      "Epoch: 1139/2000, Train Loss: 0.07112239301204681, Validation Loss: 0.09685776382684708\n",
      "Epoch: 1140/2000, Train Loss: 0.0710768923163414, Validation Loss: 0.09682432562112808\n",
      "Epoch: 1141/2000, Train Loss: 0.0710313469171524, Validation Loss: 0.09681416302919388\n",
      "Epoch: 1142/2000, Train Loss: 0.07098568230867386, Validation Loss: 0.09678123146295547\n",
      "Epoch: 1143/2000, Train Loss: 0.07093992084264755, Validation Loss: 0.09676565229892731\n",
      "Epoch: 1144/2000, Train Loss: 0.07089417427778244, Validation Loss: 0.09674099087715149\n",
      "Epoch: 1145/2000, Train Loss: 0.07084853202104568, Validation Loss: 0.09671662002801895\n",
      "Epoch: 1146/2000, Train Loss: 0.07080299407243729, Validation Loss: 0.096700519323349\n",
      "Epoch: 1147/2000, Train Loss: 0.07075753062963486, Validation Loss: 0.09667041897773743\n",
      "Epoch: 1148/2000, Train Loss: 0.07071204483509064, Validation Loss: 0.09665701538324356\n",
      "Epoch: 1149/2000, Train Loss: 0.07066651433706284, Validation Loss: 0.0966273620724678\n",
      "Epoch: 1150/2000, Train Loss: 0.07062094658613205, Validation Loss: 0.09661050885915756\n",
      "Epoch: 1151/2000, Train Loss: 0.07057540118694305, Validation Loss: 0.09658628702163696\n",
      "Epoch: 1152/2000, Train Loss: 0.07052990794181824, Validation Loss: 0.09656357020139694\n",
      "Epoch: 1153/2000, Train Loss: 0.07048448175191879, Validation Loss: 0.09654521197080612\n",
      "Epoch: 1154/2000, Train Loss: 0.07043911516666412, Validation Loss: 0.0965181514620781\n",
      "Epoch: 1155/2000, Train Loss: 0.07039374858140945, Validation Loss: 0.09650234133005142\n",
      "Epoch: 1156/2000, Train Loss: 0.07034838944673538, Validation Loss: 0.09647475183010101\n",
      "Epoch: 1157/2000, Train Loss: 0.07030302286148071, Validation Loss: 0.09645766019821167\n",
      "Epoch: 1158/2000, Train Loss: 0.07025767117738724, Validation Loss: 0.09643305838108063\n",
      "Epoch: 1159/2000, Train Loss: 0.07021234184503555, Validation Loss: 0.09641242772340775\n",
      "Epoch: 1160/2000, Train Loss: 0.07016705721616745, Validation Loss: 0.09639181941747665\n",
      "Epoch: 1161/2000, Train Loss: 0.07012181729078293, Validation Loss: 0.09636770188808441\n",
      "Epoch: 1162/2000, Train Loss: 0.0700766071677208, Validation Loss: 0.09634972363710403\n",
      "Epoch: 1163/2000, Train Loss: 0.07003141194581985, Validation Loss: 0.09632403403520584\n",
      "Epoch: 1164/2000, Train Loss: 0.0699862390756607, Validation Loss: 0.09630642831325531\n",
      "Epoch: 1165/2000, Train Loss: 0.06994107365608215, Validation Loss: 0.09628152847290039\n",
      "Epoch: 1166/2000, Train Loss: 0.0698959231376648, Validation Loss: 0.09626240283250809\n",
      "Epoch: 1167/2000, Train Loss: 0.06985079497098923, Validation Loss: 0.09623973816633224\n",
      "Epoch: 1168/2000, Train Loss: 0.06980568915605545, Validation Loss: 0.09621834754943848\n",
      "Epoch: 1169/2000, Train Loss: 0.06976061314344406, Validation Loss: 0.09619807451963425\n",
      "Epoch: 1170/2000, Train Loss: 0.06971555948257446, Validation Loss: 0.09617485105991364\n",
      "Epoch: 1171/2000, Train Loss: 0.06967050582170486, Validation Loss: 0.09615606814622879\n",
      "Epoch: 1172/2000, Train Loss: 0.06962547451257706, Validation Loss: 0.09613209217786789\n",
      "Epoch: 1173/2000, Train Loss: 0.06958043575286865, Validation Loss: 0.09611347317695618\n",
      "Epoch: 1174/2000, Train Loss: 0.06953538209199905, Validation Loss: 0.0960899069905281\n",
      "Epoch: 1175/2000, Train Loss: 0.06949029862880707, Validation Loss: 0.09607043862342834\n",
      "Epoch: 1176/2000, Train Loss: 0.06944519281387329, Validation Loss: 0.09604804217815399\n",
      "Epoch: 1177/2000, Train Loss: 0.06940004229545593, Validation Loss: 0.09602727741003036\n",
      "Epoch: 1178/2000, Train Loss: 0.06935489177703857, Validation Loss: 0.09600625932216644\n",
      "Epoch: 1179/2000, Train Loss: 0.0693097934126854, Validation Loss: 0.0959843248128891\n",
      "Epoch: 1180/2000, Train Loss: 0.06926478445529938, Validation Loss: 0.09596437960863113\n",
      "Epoch: 1181/2000, Train Loss: 0.06921985745429993, Validation Loss: 0.09594173729419708\n",
      "Epoch: 1182/2000, Train Loss: 0.06917499750852585, Validation Loss: 0.09592234343290329\n",
      "Epoch: 1183/2000, Train Loss: 0.06913018971681595, Validation Loss: 0.09589958190917969\n",
      "Epoch: 1184/2000, Train Loss: 0.06908541917800903, Validation Loss: 0.0958801805973053\n",
      "Epoch: 1185/2000, Train Loss: 0.0690406858921051, Validation Loss: 0.09585774689912796\n",
      "Epoch: 1186/2000, Train Loss: 0.06899599730968475, Validation Loss: 0.09583791345357895\n",
      "Epoch: 1187/2000, Train Loss: 0.06895133852958679, Validation Loss: 0.09581603109836578\n",
      "Epoch: 1188/2000, Train Loss: 0.06890667974948883, Validation Loss: 0.09579546004533768\n",
      "Epoch: 1189/2000, Train Loss: 0.06886199861764908, Validation Loss: 0.09577406197786331\n",
      "Epoch: 1190/2000, Train Loss: 0.06881728768348694, Validation Loss: 0.09575265645980835\n",
      "Epoch: 1191/2000, Train Loss: 0.06877249479293823, Validation Loss: 0.09573156386613846\n",
      "Epoch: 1192/2000, Train Loss: 0.06872760504484177, Validation Loss: 0.09570939093828201\n",
      "Epoch: 1193/2000, Train Loss: 0.06868255138397217, Validation Loss: 0.0956883504986763\n",
      "Epoch: 1194/2000, Train Loss: 0.06863725930452347, Validation Loss: 0.0956653505563736\n",
      "Epoch: 1195/2000, Train Loss: 0.0685916319489479, Validation Loss: 0.09564365446567535\n",
      "Epoch: 1196/2000, Train Loss: 0.06854581087827682, Validation Loss: 0.09561900794506073\n",
      "Epoch: 1197/2000, Train Loss: 0.0685001015663147, Validation Loss: 0.09559538215398788\n",
      "Epoch: 1198/2000, Train Loss: 0.06845454126596451, Validation Loss: 0.09557008743286133\n",
      "Epoch: 1199/2000, Train Loss: 0.06840860843658447, Validation Loss: 0.09554913640022278\n",
      "Epoch: 1200/2000, Train Loss: 0.06836257129907608, Validation Loss: 0.09552807360887527\n",
      "Epoch: 1201/2000, Train Loss: 0.06831692159175873, Validation Loss: 0.09550902247428894\n",
      "Epoch: 1202/2000, Train Loss: 0.06827156245708466, Validation Loss: 0.09548746049404144\n",
      "Epoch: 1203/2000, Train Loss: 0.06822623312473297, Validation Loss: 0.09546611458063126\n",
      "Epoch: 1204/2000, Train Loss: 0.06818090379238129, Validation Loss: 0.09544200450181961\n",
      "Epoch: 1205/2000, Train Loss: 0.06813562661409378, Validation Loss: 0.09541834890842438\n",
      "Epoch: 1206/2000, Train Loss: 0.06809046864509583, Validation Loss: 0.09539327025413513\n",
      "Epoch: 1207/2000, Train Loss: 0.06804537028074265, Validation Loss: 0.0953696146607399\n",
      "Epoch: 1208/2000, Train Loss: 0.06800026446580887, Validation Loss: 0.09534528851509094\n",
      "Epoch: 1209/2000, Train Loss: 0.06795510649681091, Validation Loss: 0.09532201290130615\n",
      "Epoch: 1210/2000, Train Loss: 0.06790993362665176, Validation Loss: 0.0952971801161766\n",
      "Epoch: 1211/2000, Train Loss: 0.06786476820707321, Validation Loss: 0.09527193009853363\n",
      "Epoch: 1212/2000, Train Loss: 0.06781960278749466, Validation Loss: 0.09524375945329666\n",
      "Epoch: 1213/2000, Train Loss: 0.06777441501617432, Validation Loss: 0.09521424770355225\n",
      "Epoch: 1214/2000, Train Loss: 0.06772920489311218, Validation Loss: 0.09518174827098846\n",
      "Epoch: 1215/2000, Train Loss: 0.06768397241830826, Validation Loss: 0.09514953196048737\n",
      "Epoch: 1216/2000, Train Loss: 0.06763873249292374, Validation Loss: 0.09511669725179672\n",
      "Epoch: 1217/2000, Train Loss: 0.06759344041347504, Validation Loss: 0.0950864851474762\n",
      "Epoch: 1218/2000, Train Loss: 0.06754807382822037, Validation Loss: 0.09505575895309448\n",
      "Epoch: 1219/2000, Train Loss: 0.06750266253948212, Validation Loss: 0.09502670913934708\n",
      "Epoch: 1220/2000, Train Loss: 0.06745722889900208, Validation Loss: 0.09499502927064896\n",
      "Epoch: 1221/2000, Train Loss: 0.06741173565387726, Validation Loss: 0.0949653834104538\n",
      "Epoch: 1222/2000, Train Loss: 0.06736621260643005, Validation Loss: 0.09493377804756165\n",
      "Epoch: 1223/2000, Train Loss: 0.06732065230607986, Validation Loss: 0.09490739554166794\n",
      "Epoch: 1224/2000, Train Loss: 0.0672750398516655, Validation Loss: 0.0948791578412056\n",
      "Epoch: 1225/2000, Train Loss: 0.06722936779260635, Validation Loss: 0.09485732018947601\n",
      "Epoch: 1226/2000, Train Loss: 0.06718368083238602, Validation Loss: 0.09483018517494202\n",
      "Epoch: 1227/2000, Train Loss: 0.06713799387216568, Validation Loss: 0.09481079131364822\n",
      "Epoch: 1228/2000, Train Loss: 0.06709235906600952, Validation Loss: 0.09478243440389633\n",
      "Epoch: 1229/2000, Train Loss: 0.06704676896333694, Validation Loss: 0.09476625919342041\n",
      "Epoch: 1230/2000, Train Loss: 0.06700120121240616, Validation Loss: 0.0947355180978775\n",
      "Epoch: 1231/2000, Train Loss: 0.06695562601089478, Validation Loss: 0.09472351521253586\n",
      "Epoch: 1232/2000, Train Loss: 0.0669100284576416, Validation Loss: 0.09468688070774078\n",
      "Epoch: 1233/2000, Train Loss: 0.06686440855264664, Validation Loss: 0.09468145668506622\n",
      "Epoch: 1234/2000, Train Loss: 0.06681879609823227, Validation Loss: 0.09463537484407425\n",
      "Epoch: 1235/2000, Train Loss: 0.06677331775426865, Validation Loss: 0.09464407712221146\n",
      "Epoch: 1236/2000, Train Loss: 0.06672818958759308, Validation Loss: 0.09458257257938385\n",
      "Epoch: 1237/2000, Train Loss: 0.06668393313884735, Validation Loss: 0.09461832791566849\n",
      "Epoch: 1238/2000, Train Loss: 0.06664139032363892, Validation Loss: 0.09452994167804718\n",
      "Epoch: 1239/2000, Train Loss: 0.06660250574350357, Validation Loss: 0.09462354332208633\n",
      "Epoch: 1240/2000, Train Loss: 0.06657053530216217, Validation Loss: 0.09449462592601776\n",
      "Epoch: 1241/2000, Train Loss: 0.06655276566743851, Validation Loss: 0.09471727907657623\n",
      "Epoch: 1242/2000, Train Loss: 0.06656105071306229, Validation Loss: 0.0945490375161171\n",
      "Epoch: 1243/2000, Train Loss: 0.06662146002054214, Validation Loss: 0.09507182240486145\n",
      "Epoch: 1244/2000, Train Loss: 0.06676526367664337, Validation Loss: 0.09494556486606598\n",
      "Epoch: 1245/2000, Train Loss: 0.06707458198070526, Validation Loss: 0.09606854617595673\n",
      "Epoch: 1246/2000, Train Loss: 0.06753870844841003, Validation Loss: 0.09607352316379547\n",
      "Epoch: 1247/2000, Train Loss: 0.06827782839536667, Validation Loss: 0.09742213785648346\n",
      "\n",
      "Early stopping at epoch 1247 due to validation loss increasing for 3 consecutive epochs.\n",
      "\n",
      "Accuracy: 0.9655\n",
      "\n",
      "Class 0 - Precision: 0.9615, Recall: 0.9704, F1 Score: 0.9659\n",
      "Class 1 - Precision: 0.9697, Recall: 0.9606, F1 Score: 0.9651\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3148   96]\n",
      " [ 126 3073]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "optimizer_lstm = optim.Adam(lstm.parameters(), lr=0.001)\n",
    "lstm, lstm_train_losses, lstm_val_losses = train_model(lstm, optimizer_lstm, criterion, X_train, y_train, X_val, y_val)\n",
    "\n",
    "accuracy, report, conf_matrix = evaluate_model(lstm, X_test, y_test)\n",
    "print_evaluation_metrics(accuracy, report, conf_matrix, lstm_train_losses, lstm_val_losses, \"LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "70aabc7e1d3be7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-16T02:15:03.956577300Z",
     "start_time": "2023-08-16T02:14:16.606533500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2000, Train Loss: 0.6929253339767456, Validation Loss: 0.6764959096908569\n",
      "Epoch: 2/2000, Train Loss: 0.6755416989326477, Validation Loss: 0.6630359888076782\n",
      "Epoch: 3/2000, Train Loss: 0.6615344285964966, Validation Loss: 0.65004962682724\n",
      "Epoch: 4/2000, Train Loss: 0.6483249664306641, Validation Loss: 0.6370728611946106\n",
      "Epoch: 5/2000, Train Loss: 0.6352864503860474, Validation Loss: 0.6237024068832397\n",
      "Epoch: 6/2000, Train Loss: 0.6218580603599548, Validation Loss: 0.6096261143684387\n",
      "Epoch: 7/2000, Train Loss: 0.6076511740684509, Validation Loss: 0.5954881310462952\n",
      "Epoch: 8/2000, Train Loss: 0.5933177471160889, Validation Loss: 0.5819429159164429\n",
      "Epoch: 9/2000, Train Loss: 0.5795716047286987, Validation Loss: 0.5686237812042236\n",
      "Epoch: 10/2000, Train Loss: 0.5661202073097229, Validation Loss: 0.5551929473876953\n",
      "Epoch: 11/2000, Train Loss: 0.5526320934295654, Validation Loss: 0.5419567227363586\n",
      "Epoch: 12/2000, Train Loss: 0.539366602897644, Validation Loss: 0.5288493633270264\n",
      "Epoch: 13/2000, Train Loss: 0.5262429714202881, Validation Loss: 0.5161346793174744\n",
      "Epoch: 14/2000, Train Loss: 0.513540506362915, Validation Loss: 0.5039030909538269\n",
      "Epoch: 15/2000, Train Loss: 0.5013689398765564, Validation Loss: 0.49204134941101074\n",
      "Epoch: 16/2000, Train Loss: 0.48959672451019287, Validation Loss: 0.48071545362472534\n",
      "Epoch: 17/2000, Train Loss: 0.4783407747745514, Validation Loss: 0.46972915530204773\n",
      "Epoch: 18/2000, Train Loss: 0.46739110350608826, Validation Loss: 0.459238201379776\n",
      "Epoch: 19/2000, Train Loss: 0.4569160044193268, Validation Loss: 0.44909852743148804\n",
      "Epoch: 20/2000, Train Loss: 0.44679802656173706, Validation Loss: 0.4393605589866638\n",
      "Epoch: 21/2000, Train Loss: 0.43709322810173035, Validation Loss: 0.42996758222579956\n",
      "Epoch: 22/2000, Train Loss: 0.42773866653442383, Validation Loss: 0.4209403097629547\n",
      "Epoch: 23/2000, Train Loss: 0.41873693466186523, Validation Loss: 0.412324458360672\n",
      "Epoch: 24/2000, Train Loss: 0.4101017713546753, Validation Loss: 0.40417617559432983\n",
      "Epoch: 25/2000, Train Loss: 0.4018978476524353, Validation Loss: 0.39629536867141724\n",
      "Epoch: 26/2000, Train Loss: 0.39400726556777954, Validation Loss: 0.38873258233070374\n",
      "Epoch: 27/2000, Train Loss: 0.3864920735359192, Validation Loss: 0.3814414441585541\n",
      "Epoch: 28/2000, Train Loss: 0.37928059697151184, Validation Loss: 0.3744543492794037\n",
      "Epoch: 29/2000, Train Loss: 0.3723953068256378, Validation Loss: 0.36772972345352173\n",
      "Epoch: 30/2000, Train Loss: 0.3657831847667694, Validation Loss: 0.36133110523223877\n",
      "Epoch: 31/2000, Train Loss: 0.35945990681648254, Validation Loss: 0.35523515939712524\n",
      "Epoch: 32/2000, Train Loss: 0.35338935256004333, Validation Loss: 0.34936878085136414\n",
      "Epoch: 33/2000, Train Loss: 0.3475412130355835, Validation Loss: 0.34373176097869873\n",
      "Epoch: 34/2000, Train Loss: 0.34191426634788513, Validation Loss: 0.3382827341556549\n",
      "Epoch: 35/2000, Train Loss: 0.3364522457122803, Validation Loss: 0.3329905569553375\n",
      "Epoch: 36/2000, Train Loss: 0.3311654031276703, Validation Loss: 0.32789087295532227\n",
      "Epoch: 37/2000, Train Loss: 0.3260881006717682, Validation Loss: 0.3230574429035187\n",
      "Epoch: 38/2000, Train Loss: 0.32125088572502136, Validation Loss: 0.3184695839881897\n",
      "Epoch: 39/2000, Train Loss: 0.3166652023792267, Validation Loss: 0.3140755295753479\n",
      "Epoch: 40/2000, Train Loss: 0.31228896975517273, Validation Loss: 0.3098823130130768\n",
      "Epoch: 41/2000, Train Loss: 0.3080812692642212, Validation Loss: 0.3058435916900635\n",
      "Epoch: 42/2000, Train Loss: 0.3040359318256378, Validation Loss: 0.3019489645957947\n",
      "Epoch: 43/2000, Train Loss: 0.30015313625335693, Validation Loss: 0.29824697971343994\n",
      "Epoch: 44/2000, Train Loss: 0.29643481969833374, Validation Loss: 0.29469600319862366\n",
      "Epoch: 45/2000, Train Loss: 0.292880117893219, Validation Loss: 0.29129084944725037\n",
      "Epoch: 46/2000, Train Loss: 0.28946706652641296, Validation Loss: 0.28803518414497375\n",
      "Epoch: 47/2000, Train Loss: 0.2861657738685608, Validation Loss: 0.28486767411231995\n",
      "Epoch: 48/2000, Train Loss: 0.28296613693237305, Validation Loss: 0.28182560205459595\n",
      "Epoch: 49/2000, Train Loss: 0.2798728346824646, Validation Loss: 0.27890273928642273\n",
      "Epoch: 50/2000, Train Loss: 0.27688929438591003, Validation Loss: 0.27606678009033203\n",
      "Epoch: 51/2000, Train Loss: 0.2740122377872467, Validation Loss: 0.2733592689037323\n",
      "Epoch: 52/2000, Train Loss: 0.27123695611953735, Validation Loss: 0.2707277536392212\n",
      "Epoch: 53/2000, Train Loss: 0.2685573399066925, Validation Loss: 0.26819393038749695\n",
      "Epoch: 54/2000, Train Loss: 0.26596516370773315, Validation Loss: 0.2657424509525299\n",
      "Epoch: 55/2000, Train Loss: 0.26345279812812805, Validation Loss: 0.2633494734764099\n",
      "Epoch: 56/2000, Train Loss: 0.2610168755054474, Validation Loss: 0.2610473334789276\n",
      "Epoch: 57/2000, Train Loss: 0.25865602493286133, Validation Loss: 0.258781373500824\n",
      "Epoch: 58/2000, Train Loss: 0.25636792182922363, Validation Loss: 0.25661057233810425\n",
      "Epoch: 59/2000, Train Loss: 0.2541499435901642, Validation Loss: 0.2544705271720886\n",
      "Epoch: 60/2000, Train Loss: 0.2520005404949188, Validation Loss: 0.2524440586566925\n",
      "Epoch: 61/2000, Train Loss: 0.24991825222969055, Validation Loss: 0.2504417598247528\n",
      "Epoch: 62/2000, Train Loss: 0.24790434539318085, Validation Loss: 0.2485932558774948\n",
      "Epoch: 63/2000, Train Loss: 0.24596291780471802, Validation Loss: 0.24673114717006683\n",
      "Epoch: 64/2000, Train Loss: 0.2441059947013855, Validation Loss: 0.245133638381958\n",
      "Epoch: 65/2000, Train Loss: 0.24235375225543976, Validation Loss: 0.2434554249048233\n",
      "Epoch: 66/2000, Train Loss: 0.24075421690940857, Validation Loss: 0.2421821653842926\n",
      "Epoch: 67/2000, Train Loss: 0.23922783136367798, Validation Loss: 0.24043869972229004\n",
      "Epoch: 68/2000, Train Loss: 0.2376372516155243, Validation Loss: 0.23883309960365295\n",
      "Epoch: 69/2000, Train Loss: 0.23579952120780945, Validation Loss: 0.23718054592609406\n",
      "Epoch: 70/2000, Train Loss: 0.23414967954158783, Validation Loss: 0.23587064445018768\n",
      "Epoch: 71/2000, Train Loss: 0.23285908997058868, Validation Loss: 0.23479345440864563\n",
      "Epoch: 72/2000, Train Loss: 0.2315739095211029, Validation Loss: 0.23317576944828033\n",
      "Epoch: 73/2000, Train Loss: 0.23006896674633026, Validation Loss: 0.2318001091480255\n",
      "Epoch: 74/2000, Train Loss: 0.22856895625591278, Validation Loss: 0.23066850006580353\n",
      "Epoch: 75/2000, Train Loss: 0.227344810962677, Validation Loss: 0.2294364720582962\n",
      "Epoch: 76/2000, Train Loss: 0.226191908121109, Validation Loss: 0.22830577194690704\n",
      "Epoch: 77/2000, Train Loss: 0.22485730051994324, Validation Loss: 0.22693529725074768\n",
      "Epoch: 78/2000, Train Loss: 0.22350646555423737, Validation Loss: 0.22580888867378235\n",
      "Epoch: 79/2000, Train Loss: 0.2223404198884964, Validation Loss: 0.22489646077156067\n",
      "Epoch: 80/2000, Train Loss: 0.22125431895256042, Validation Loss: 0.22364424169063568\n",
      "Epoch: 81/2000, Train Loss: 0.22007480263710022, Validation Loss: 0.22256653010845184\n",
      "Epoch: 82/2000, Train Loss: 0.2188381403684616, Validation Loss: 0.22148843109607697\n",
      "Epoch: 83/2000, Train Loss: 0.21770653128623962, Validation Loss: 0.22044311463832855\n",
      "Epoch: 84/2000, Train Loss: 0.21667566895484924, Validation Loss: 0.21957427263259888\n",
      "Epoch: 85/2000, Train Loss: 0.21562090516090393, Validation Loss: 0.2184005230665207\n",
      "Epoch: 86/2000, Train Loss: 0.2145065814256668, Validation Loss: 0.2174202799797058\n",
      "Epoch: 87/2000, Train Loss: 0.2134048491716385, Validation Loss: 0.21646353602409363\n",
      "Epoch: 88/2000, Train Loss: 0.21238155663013458, Validation Loss: 0.2154579907655716\n",
      "Epoch: 89/2000, Train Loss: 0.21140888333320618, Validation Loss: 0.21463601291179657\n",
      "Epoch: 90/2000, Train Loss: 0.2104230672121048, Validation Loss: 0.2135472297668457\n",
      "Epoch: 91/2000, Train Loss: 0.20940878987312317, Validation Loss: 0.21266014873981476\n",
      "Epoch: 92/2000, Train Loss: 0.20839613676071167, Validation Loss: 0.21171213686466217\n",
      "Epoch: 93/2000, Train Loss: 0.2074263095855713, Validation Loss: 0.2107919305562973\n",
      "Epoch: 94/2000, Train Loss: 0.20649956166744232, Validation Loss: 0.2100183069705963\n",
      "Epoch: 95/2000, Train Loss: 0.20559294521808624, Validation Loss: 0.209043949842453\n",
      "Epoch: 96/2000, Train Loss: 0.20468662679195404, Validation Loss: 0.2083013653755188\n",
      "Epoch: 97/2000, Train Loss: 0.2037724256515503, Validation Loss: 0.20732325315475464\n",
      "Epoch: 98/2000, Train Loss: 0.20285819470882416, Validation Loss: 0.20655085146427155\n",
      "Epoch: 99/2000, Train Loss: 0.2019532322883606, Validation Loss: 0.20565935969352722\n",
      "Epoch: 100/2000, Train Loss: 0.20106682181358337, Validation Loss: 0.20485247671604156\n",
      "Epoch: 101/2000, Train Loss: 0.20020054280757904, Validation Loss: 0.20407256484031677\n",
      "Epoch: 102/2000, Train Loss: 0.19935183227062225, Validation Loss: 0.20323024690151215\n",
      "Epoch: 103/2000, Train Loss: 0.19851811230182648, Validation Loss: 0.2025459110736847\n",
      "Epoch: 104/2000, Train Loss: 0.19769960641860962, Validation Loss: 0.2016681581735611\n",
      "Epoch: 105/2000, Train Loss: 0.1969003826379776, Validation Loss: 0.20110464096069336\n",
      "Epoch: 106/2000, Train Loss: 0.19612768292427063, Validation Loss: 0.20020683109760284\n",
      "Epoch: 107/2000, Train Loss: 0.19539864361286163, Validation Loss: 0.19985587894916534\n",
      "Epoch: 108/2000, Train Loss: 0.19471792876720428, Validation Loss: 0.1989375203847885\n",
      "Epoch: 109/2000, Train Loss: 0.19409285485744476, Validation Loss: 0.19871065020561218\n",
      "Epoch: 110/2000, Train Loss: 0.1934141367673874, Validation Loss: 0.1975633203983307\n",
      "Epoch: 111/2000, Train Loss: 0.1926218420267105, Validation Loss: 0.19698092341423035\n",
      "Epoch: 112/2000, Train Loss: 0.19165591895580292, Validation Loss: 0.1959376186132431\n",
      "Epoch: 113/2000, Train Loss: 0.19074782729148865, Validation Loss: 0.19528000056743622\n",
      "Epoch: 114/2000, Train Loss: 0.19004659354686737, Validation Loss: 0.19495917856693268\n",
      "Epoch: 115/2000, Train Loss: 0.18949666619300842, Validation Loss: 0.19414693117141724\n",
      "Epoch: 116/2000, Train Loss: 0.18893221020698547, Validation Loss: 0.19379684329032898\n",
      "Epoch: 117/2000, Train Loss: 0.1882067322731018, Validation Loss: 0.19276879727840424\n",
      "Epoch: 118/2000, Train Loss: 0.18738846480846405, Validation Loss: 0.19217856228351593\n",
      "Epoch: 119/2000, Train Loss: 0.18662229180335999, Validation Loss: 0.19164910912513733\n",
      "Epoch: 120/2000, Train Loss: 0.1860019415616989, Validation Loss: 0.19097958505153656\n",
      "Epoch: 121/2000, Train Loss: 0.1854633241891861, Validation Loss: 0.1907147914171219\n",
      "Epoch: 122/2000, Train Loss: 0.18488359451293945, Validation Loss: 0.18981309235095978\n",
      "Epoch: 123/2000, Train Loss: 0.18421728909015656, Validation Loss: 0.1893579512834549\n",
      "Epoch: 124/2000, Train Loss: 0.18348927795886993, Validation Loss: 0.18859970569610596\n",
      "Epoch: 125/2000, Train Loss: 0.18279853463172913, Validation Loss: 0.18802587687969208\n",
      "Epoch: 126/2000, Train Loss: 0.18218739330768585, Validation Loss: 0.1876543015241623\n",
      "Epoch: 127/2000, Train Loss: 0.18162865936756134, Validation Loss: 0.1869398057460785\n",
      "Epoch: 128/2000, Train Loss: 0.1810731589794159, Validation Loss: 0.18665295839309692\n",
      "Epoch: 129/2000, Train Loss: 0.18047113716602325, Validation Loss: 0.18582534790039062\n",
      "Epoch: 130/2000, Train Loss: 0.17983782291412354, Validation Loss: 0.18542127311229706\n",
      "Epoch: 131/2000, Train Loss: 0.1791824996471405, Validation Loss: 0.1846821904182434\n",
      "Epoch: 132/2000, Train Loss: 0.17854899168014526, Validation Loss: 0.18417872488498688\n",
      "Epoch: 133/2000, Train Loss: 0.1779337376356125, Validation Loss: 0.18362493813037872\n",
      "Epoch: 134/2000, Train Loss: 0.17734071612358093, Validation Loss: 0.18303349614143372\n",
      "Epoch: 135/2000, Train Loss: 0.1767701953649521, Validation Loss: 0.1826404184103012\n",
      "Epoch: 136/2000, Train Loss: 0.1762172132730484, Validation Loss: 0.1819641888141632\n",
      "Epoch: 137/2000, Train Loss: 0.1756802350282669, Validation Loss: 0.18172819912433624\n",
      "Epoch: 138/2000, Train Loss: 0.1751651167869568, Validation Loss: 0.18098321557044983\n",
      "Epoch: 139/2000, Train Loss: 0.17469076812267303, Validation Loss: 0.18098804354667664\n",
      "Epoch: 140/2000, Train Loss: 0.17426025867462158, Validation Loss: 0.18016403913497925\n",
      "Epoch: 141/2000, Train Loss: 0.17387919127941132, Validation Loss: 0.18033412098884583\n",
      "Epoch: 142/2000, Train Loss: 0.17344971001148224, Validation Loss: 0.17925356328487396\n",
      "Epoch: 143/2000, Train Loss: 0.17291100323200226, Validation Loss: 0.17904655635356903\n",
      "Epoch: 144/2000, Train Loss: 0.17215541005134583, Validation Loss: 0.1779801845550537\n",
      "Epoch: 145/2000, Train Loss: 0.1713884174823761, Validation Loss: 0.1775190532207489\n",
      "Epoch: 146/2000, Train Loss: 0.170796737074852, Validation Loss: 0.17731505632400513\n",
      "Epoch: 147/2000, Train Loss: 0.17041033506393433, Validation Loss: 0.17669256031513214\n",
      "Epoch: 148/2000, Train Loss: 0.170094296336174, Validation Loss: 0.17675480246543884\n",
      "Epoch: 149/2000, Train Loss: 0.1696736067533493, Validation Loss: 0.17578484117984772\n",
      "Epoch: 150/2000, Train Loss: 0.1691095530986786, Validation Loss: 0.17550873756408691\n",
      "Epoch: 151/2000, Train Loss: 0.16844826936721802, Validation Loss: 0.17479756474494934\n",
      "Epoch: 152/2000, Train Loss: 0.16785715520381927, Validation Loss: 0.17435519397258759\n",
      "Epoch: 153/2000, Train Loss: 0.16740281879901886, Validation Loss: 0.17424717545509338\n",
      "Epoch: 154/2000, Train Loss: 0.16703318059444427, Validation Loss: 0.17358285188674927\n",
      "Epoch: 155/2000, Train Loss: 0.16665445268154144, Validation Loss: 0.17353719472885132\n",
      "Epoch: 156/2000, Train Loss: 0.16618530452251434, Validation Loss: 0.1726950705051422\n",
      "Epoch: 157/2000, Train Loss: 0.16564439237117767, Validation Loss: 0.17241699993610382\n",
      "Epoch: 158/2000, Train Loss: 0.1650823950767517, Validation Loss: 0.17184701561927795\n",
      "Epoch: 159/2000, Train Loss: 0.16457496583461761, Validation Loss: 0.17140695452690125\n",
      "Epoch: 160/2000, Train Loss: 0.16413895785808563, Validation Loss: 0.17122779786586761\n",
      "Epoch: 161/2000, Train Loss: 0.1637464016675949, Validation Loss: 0.17060701549053192\n",
      "Epoch: 162/2000, Train Loss: 0.16336044669151306, Validation Loss: 0.1705528199672699\n",
      "Epoch: 163/2000, Train Loss: 0.16294610500335693, Validation Loss: 0.16979233920574188\n",
      "Epoch: 164/2000, Train Loss: 0.16250135004520416, Validation Loss: 0.16966313123703003\n",
      "Epoch: 165/2000, Train Loss: 0.16201724112033844, Validation Loss: 0.1689419001340866\n",
      "Epoch: 166/2000, Train Loss: 0.16152425110340118, Validation Loss: 0.16867923736572266\n",
      "Epoch: 167/2000, Train Loss: 0.1610412895679474, Validation Loss: 0.1681738793849945\n",
      "Epoch: 168/2000, Train Loss: 0.1605863869190216, Validation Loss: 0.16777993738651276\n",
      "Epoch: 169/2000, Train Loss: 0.16016075015068054, Validation Loss: 0.1675005555152893\n",
      "Epoch: 170/2000, Train Loss: 0.15975765883922577, Validation Loss: 0.16697728633880615\n",
      "Epoch: 171/2000, Train Loss: 0.15937110781669617, Validation Loss: 0.1668701022863388\n",
      "Epoch: 172/2000, Train Loss: 0.1589965969324112, Validation Loss: 0.16623957455158234\n",
      "Epoch: 173/2000, Train Loss: 0.1586385816335678, Validation Loss: 0.16629303991794586\n",
      "Epoch: 174/2000, Train Loss: 0.15828794240951538, Validation Loss: 0.1655658632516861\n",
      "Epoch: 175/2000, Train Loss: 0.15795522928237915, Validation Loss: 0.16573812067508698\n",
      "Epoch: 176/2000, Train Loss: 0.15760108828544617, Validation Loss: 0.16489045321941376\n",
      "Epoch: 177/2000, Train Loss: 0.15723305940628052, Validation Loss: 0.16500075161457062\n",
      "Epoch: 178/2000, Train Loss: 0.15678420662879944, Validation Loss: 0.1640893965959549\n",
      "Epoch: 179/2000, Train Loss: 0.15629805624485016, Validation Loss: 0.16398340463638306\n",
      "Epoch: 180/2000, Train Loss: 0.1557808220386505, Validation Loss: 0.16332295536994934\n",
      "Epoch: 181/2000, Train Loss: 0.15530633926391602, Validation Loss: 0.16303013265132904\n",
      "Epoch: 182/2000, Train Loss: 0.15489396452903748, Validation Loss: 0.16278386116027832\n",
      "Epoch: 183/2000, Train Loss: 0.1545388549566269, Validation Loss: 0.16229717433452606\n",
      "Epoch: 184/2000, Train Loss: 0.1542198210954666, Validation Loss: 0.16233216226100922\n",
      "Epoch: 185/2000, Train Loss: 0.15391433238983154, Validation Loss: 0.161666139960289\n",
      "Epoch: 186/2000, Train Loss: 0.1536141186952591, Validation Loss: 0.16181418299674988\n",
      "Epoch: 187/2000, Train Loss: 0.1532832235097885, Validation Loss: 0.16101504862308502\n",
      "Epoch: 188/2000, Train Loss: 0.15292444825172424, Validation Loss: 0.1610826551914215\n",
      "Epoch: 189/2000, Train Loss: 0.15250541269779205, Validation Loss: 0.1602824330329895\n",
      "Epoch: 190/2000, Train Loss: 0.1520635336637497, Validation Loss: 0.1601683497428894\n",
      "Epoch: 191/2000, Train Loss: 0.15161554515361786, Validation Loss: 0.15960583090782166\n",
      "Epoch: 192/2000, Train Loss: 0.1511971801519394, Validation Loss: 0.15932336449623108\n",
      "Epoch: 193/2000, Train Loss: 0.15081700682640076, Validation Loss: 0.15908005833625793\n",
      "Epoch: 194/2000, Train Loss: 0.15047107636928558, Validation Loss: 0.15864123404026031\n",
      "Epoch: 195/2000, Train Loss: 0.15015047788619995, Validation Loss: 0.1586378663778305\n",
      "Epoch: 196/2000, Train Loss: 0.14984354376792908, Validation Loss: 0.15804994106292725\n",
      "Epoch: 197/2000, Train Loss: 0.1495518535375595, Validation Loss: 0.15822403132915497\n",
      "Epoch: 198/2000, Train Loss: 0.14926204085350037, Validation Loss: 0.15752199292182922\n",
      "Epoch: 199/2000, Train Loss: 0.14900465309619904, Validation Loss: 0.1578451544046402\n",
      "Epoch: 200/2000, Train Loss: 0.14873497188091278, Validation Loss: 0.15697604417800903\n",
      "Epoch: 201/2000, Train Loss: 0.14845512807369232, Validation Loss: 0.15725719928741455\n",
      "Epoch: 202/2000, Train Loss: 0.1480812430381775, Validation Loss: 0.1562623828649521\n",
      "Epoch: 203/2000, Train Loss: 0.14764507114887238, Validation Loss: 0.15626904368400574\n",
      "Epoch: 204/2000, Train Loss: 0.14713986217975616, Validation Loss: 0.15552935004234314\n",
      "Epoch: 205/2000, Train Loss: 0.1466677486896515, Validation Loss: 0.1552952080965042\n",
      "Epoch: 206/2000, Train Loss: 0.14627717435359955, Validation Loss: 0.15511997044086456\n",
      "Epoch: 207/2000, Train Loss: 0.14597325026988983, Validation Loss: 0.15466482937335968\n",
      "Epoch: 208/2000, Train Loss: 0.14572840929031372, Validation Loss: 0.15486730635166168\n",
      "Epoch: 209/2000, Train Loss: 0.1454988420009613, Validation Loss: 0.15417422354221344\n",
      "Epoch: 210/2000, Train Loss: 0.14526484906673431, Validation Loss: 0.15447533130645752\n",
      "Epoch: 211/2000, Train Loss: 0.14497730135917664, Validation Loss: 0.15360267460346222\n",
      "Epoch: 212/2000, Train Loss: 0.14464031159877777, Validation Loss: 0.1537470668554306\n",
      "Epoch: 213/2000, Train Loss: 0.14422014355659485, Validation Loss: 0.15293067693710327\n",
      "Epoch: 214/2000, Train Loss: 0.14378002285957336, Validation Loss: 0.1528228521347046\n",
      "Epoch: 215/2000, Train Loss: 0.14336074888706207, Validation Loss: 0.15240950882434845\n",
      "Epoch: 216/2000, Train Loss: 0.14300285279750824, Validation Loss: 0.15207824110984802\n",
      "Epoch: 217/2000, Train Loss: 0.14270560443401337, Validation Loss: 0.15207920968532562\n",
      "Epoch: 218/2000, Train Loss: 0.14244697988033295, Validation Loss: 0.1515318602323532\n",
      "Epoch: 219/2000, Train Loss: 0.14221036434173584, Validation Loss: 0.15176527202129364\n",
      "Epoch: 220/2000, Train Loss: 0.14196550846099854, Validation Loss: 0.1510394960641861\n",
      "Epoch: 221/2000, Train Loss: 0.1417289525270462, Validation Loss: 0.15139776468276978\n",
      "Epoch: 222/2000, Train Loss: 0.14146782457828522, Validation Loss: 0.1505393236875534\n",
      "Epoch: 223/2000, Train Loss: 0.14118392765522003, Validation Loss: 0.1508190929889679\n",
      "Epoch: 224/2000, Train Loss: 0.14082437753677368, Validation Loss: 0.14990228414535522\n",
      "Epoch: 225/2000, Train Loss: 0.140421524643898, Validation Loss: 0.14993245899677277\n",
      "Epoch: 226/2000, Train Loss: 0.13998757302761078, Validation Loss: 0.14927956461906433\n",
      "Epoch: 227/2000, Train Loss: 0.1395871937274933, Validation Loss: 0.14907535910606384\n",
      "Epoch: 228/2000, Train Loss: 0.13924361765384674, Validation Loss: 0.14887376129627228\n",
      "Epoch: 229/2000, Train Loss: 0.13895587623119354, Validation Loss: 0.1484500616788864\n",
      "Epoch: 230/2000, Train Loss: 0.13870522379875183, Validation Loss: 0.14858968555927277\n",
      "Epoch: 231/2000, Train Loss: 0.13847246766090393, Validation Loss: 0.1479775756597519\n",
      "Epoch: 232/2000, Train Loss: 0.1382543295621872, Validation Loss: 0.1483246088027954\n",
      "Epoch: 233/2000, Train Loss: 0.13802789151668549, Validation Loss: 0.14755657315254211\n",
      "Epoch: 234/2000, Train Loss: 0.1377963274717331, Validation Loss: 0.1479407101869583\n",
      "Epoch: 235/2000, Train Loss: 0.1375105232000351, Validation Loss: 0.14705093204975128\n",
      "Epoch: 236/2000, Train Loss: 0.13718724250793457, Validation Loss: 0.1472683548927307\n",
      "Epoch: 237/2000, Train Loss: 0.13679981231689453, Validation Loss: 0.14644722640514374\n",
      "Epoch: 238/2000, Train Loss: 0.13640327751636505, Validation Loss: 0.14642545580863953\n",
      "Epoch: 239/2000, Train Loss: 0.13601695001125336, Validation Loss: 0.14592528343200684\n",
      "Epoch: 240/2000, Train Loss: 0.13567160069942474, Validation Loss: 0.14568930864334106\n",
      "Epoch: 241/2000, Train Loss: 0.13536898791790009, Validation Loss: 0.14554926753044128\n",
      "Epoch: 242/2000, Train Loss: 0.1351015716791153, Validation Loss: 0.14512120187282562\n",
      "Epoch: 243/2000, Train Loss: 0.13486112654209137, Validation Loss: 0.14525891840457916\n",
      "Epoch: 244/2000, Train Loss: 0.13463962078094482, Validation Loss: 0.14466075599193573\n",
      "Epoch: 245/2000, Train Loss: 0.13444092869758606, Validation Loss: 0.14503712952136993\n",
      "Epoch: 246/2000, Train Loss: 0.13425347208976746, Validation Loss: 0.1442776471376419\n",
      "Epoch: 247/2000, Train Loss: 0.13408684730529785, Validation Loss: 0.14482218027114868\n",
      "Epoch: 248/2000, Train Loss: 0.13388749957084656, Validation Loss: 0.14386999607086182\n",
      "Epoch: 249/2000, Train Loss: 0.13365235924720764, Validation Loss: 0.14429537951946259\n",
      "Epoch: 250/2000, Train Loss: 0.1332920789718628, Validation Loss: 0.14325390756130219\n",
      "Epoch: 251/2000, Train Loss: 0.13286232948303223, Validation Loss: 0.1433270126581192\n",
      "Epoch: 252/2000, Train Loss: 0.13239581882953644, Validation Loss: 0.14269836246967316\n",
      "Epoch: 253/2000, Train Loss: 0.13199813663959503, Validation Loss: 0.14249871671199799\n",
      "Epoch: 254/2000, Train Loss: 0.131703719496727, Validation Loss: 0.14249730110168457\n",
      "Epoch: 255/2000, Train Loss: 0.13149578869342804, Validation Loss: 0.14202716946601868\n",
      "Epoch: 256/2000, Train Loss: 0.1313323974609375, Validation Loss: 0.14236126840114594\n",
      "Epoch: 257/2000, Train Loss: 0.13116411864757538, Validation Loss: 0.14162246882915497\n",
      "Epoch: 258/2000, Train Loss: 0.13096708059310913, Validation Loss: 0.14197632670402527\n",
      "Epoch: 259/2000, Train Loss: 0.13069124519824982, Validation Loss: 0.14111341536045074\n",
      "Epoch: 260/2000, Train Loss: 0.13035766780376434, Validation Loss: 0.1412569284439087\n",
      "Epoch: 261/2000, Train Loss: 0.12997165322303772, Validation Loss: 0.14060242474079132\n",
      "Epoch: 262/2000, Train Loss: 0.12959840893745422, Validation Loss: 0.1405046135187149\n",
      "Epoch: 263/2000, Train Loss: 0.12926867604255676, Validation Loss: 0.14027713239192963\n",
      "Epoch: 264/2000, Train Loss: 0.1289953738451004, Validation Loss: 0.1399514079093933\n",
      "Epoch: 265/2000, Train Loss: 0.1287679672241211, Validation Loss: 0.14006654918193817\n",
      "Epoch: 266/2000, Train Loss: 0.12856756150722504, Validation Loss: 0.13953319191932678\n",
      "Epoch: 267/2000, Train Loss: 0.12838825583457947, Validation Loss: 0.13987880945205688\n",
      "Epoch: 268/2000, Train Loss: 0.1282106190919876, Validation Loss: 0.13915550708770752\n",
      "Epoch: 269/2000, Train Loss: 0.12803451716899872, Validation Loss: 0.1396292895078659\n",
      "Epoch: 270/2000, Train Loss: 0.12781263887882233, Validation Loss: 0.1387515813112259\n",
      "Epoch: 271/2000, Train Loss: 0.12756577134132385, Validation Loss: 0.13912375271320343\n",
      "Epoch: 272/2000, Train Loss: 0.1272394359111786, Validation Loss: 0.13821591436862946\n",
      "Epoch: 273/2000, Train Loss: 0.12687990069389343, Validation Loss: 0.13832204043865204\n",
      "Epoch: 274/2000, Train Loss: 0.1265006810426712, Validation Loss: 0.13770176470279694\n",
      "Epoch: 275/2000, Train Loss: 0.12615333497524261, Validation Loss: 0.13755938410758972\n",
      "Epoch: 276/2000, Train Loss: 0.125858873128891, Validation Loss: 0.13739775121212006\n",
      "Epoch: 277/2000, Train Loss: 0.1256173998117447, Validation Loss: 0.13703830540180206\n",
      "Epoch: 278/2000, Train Loss: 0.1254129856824875, Validation Loss: 0.1372205913066864\n",
      "Epoch: 279/2000, Train Loss: 0.12522560358047485, Validation Loss: 0.13666068017482758\n",
      "Epoch: 280/2000, Train Loss: 0.12504631280899048, Validation Loss: 0.13701137900352478\n",
      "Epoch: 281/2000, Train Loss: 0.12485411763191223, Validation Loss: 0.13628320395946503\n",
      "Epoch: 282/2000, Train Loss: 0.12464939802885056, Validation Loss: 0.13664962351322174\n",
      "Epoch: 283/2000, Train Loss: 0.12440527975559235, Validation Loss: 0.13584010303020477\n",
      "Epoch: 284/2000, Train Loss: 0.12413575500249863, Validation Loss: 0.13609333336353302\n",
      "Epoch: 285/2000, Train Loss: 0.12382771819829941, Validation Loss: 0.13537126779556274\n",
      "Epoch: 286/2000, Train Loss: 0.12351330369710922, Validation Loss: 0.13544054329395294\n",
      "Epoch: 287/2000, Train Loss: 0.12320317327976227, Validation Loss: 0.13496628403663635\n",
      "Epoch: 288/2000, Train Loss: 0.12291639298200607, Validation Loss: 0.1348385214805603\n",
      "Epoch: 289/2000, Train Loss: 0.12265542149543762, Validation Loss: 0.1346418857574463\n",
      "Epoch: 290/2000, Train Loss: 0.12241756170988083, Validation Loss: 0.13433809578418732\n",
      "Epoch: 291/2000, Train Loss: 0.12219735980033875, Validation Loss: 0.13437391817569733\n",
      "Epoch: 292/2000, Train Loss: 0.12198977172374725, Validation Loss: 0.13392488658428192\n",
      "Epoch: 293/2000, Train Loss: 0.12179394066333771, Validation Loss: 0.13415732979774475\n",
      "Epoch: 294/2000, Train Loss: 0.12160835415124893, Validation Loss: 0.1335678994655609\n",
      "Epoch: 295/2000, Train Loss: 0.12144078314304352, Validation Loss: 0.13399767875671387\n",
      "Epoch: 296/2000, Train Loss: 0.12128543853759766, Validation Loss: 0.13325342535972595\n",
      "Epoch: 297/2000, Train Loss: 0.12115528434515, Validation Loss: 0.13387517631053925\n",
      "Epoch: 298/2000, Train Loss: 0.12101603299379349, Validation Loss: 0.13295331597328186\n",
      "Epoch: 299/2000, Train Loss: 0.12087385356426239, Validation Loss: 0.1335994303226471\n",
      "Epoch: 300/2000, Train Loss: 0.12064433097839355, Validation Loss: 0.1325192153453827\n",
      "Epoch: 301/2000, Train Loss: 0.12034915387630463, Validation Loss: 0.13288478553295135\n",
      "Epoch: 302/2000, Train Loss: 0.11995236575603485, Validation Loss: 0.13197600841522217\n",
      "Epoch: 303/2000, Train Loss: 0.11955089867115021, Validation Loss: 0.13199271261692047\n",
      "Epoch: 304/2000, Train Loss: 0.11920171231031418, Validation Loss: 0.13169872760772705\n",
      "Epoch: 305/2000, Train Loss: 0.11894863098859787, Validation Loss: 0.13141268491744995\n",
      "Epoch: 306/2000, Train Loss: 0.11877898871898651, Validation Loss: 0.1316622942686081\n",
      "Epoch: 307/2000, Train Loss: 0.11865244060754776, Validation Loss: 0.13107243180274963\n",
      "Epoch: 308/2000, Train Loss: 0.11853215843439102, Validation Loss: 0.13153260946273804\n",
      "Epoch: 309/2000, Train Loss: 0.11837416887283325, Validation Loss: 0.13072320818901062\n",
      "Epoch: 310/2000, Train Loss: 0.11817371845245361, Validation Loss: 0.13110294938087463\n",
      "Epoch: 311/2000, Train Loss: 0.11790559440851212, Validation Loss: 0.13029426336288452\n",
      "Epoch: 312/2000, Train Loss: 0.11760685592889786, Validation Loss: 0.13044323027133942\n",
      "Epoch: 313/2000, Train Loss: 0.11729704588651657, Validation Loss: 0.1299329698085785\n",
      "Epoch: 314/2000, Train Loss: 0.11701560020446777, Validation Loss: 0.12983748316764832\n",
      "Epoch: 315/2000, Train Loss: 0.1167755126953125, Validation Loss: 0.12972773611545563\n",
      "Epoch: 316/2000, Train Loss: 0.11657451093196869, Validation Loss: 0.1293998658657074\n",
      "Epoch: 317/2000, Train Loss: 0.11640014499425888, Validation Loss: 0.12958058714866638\n",
      "Epoch: 318/2000, Train Loss: 0.11623793840408325, Validation Loss: 0.12905506789684296\n",
      "Epoch: 319/2000, Train Loss: 0.1160803809762001, Validation Loss: 0.12939970195293427\n",
      "Epoch: 320/2000, Train Loss: 0.11591532826423645, Validation Loss: 0.1287330538034439\n",
      "Epoch: 321/2000, Train Loss: 0.1157459169626236, Validation Loss: 0.12914548814296722\n",
      "Epoch: 322/2000, Train Loss: 0.1155562624335289, Validation Loss: 0.12840037047863007\n",
      "Epoch: 323/2000, Train Loss: 0.11535628885030746, Validation Loss: 0.12878698110580444\n",
      "Epoch: 324/2000, Train Loss: 0.1151295080780983, Validation Loss: 0.12804502248764038\n",
      "Epoch: 325/2000, Train Loss: 0.1148924008011818, Validation Loss: 0.12832704186439514\n",
      "Epoch: 326/2000, Train Loss: 0.11463871598243713, Validation Loss: 0.127690851688385\n",
      "Epoch: 327/2000, Train Loss: 0.11438655853271484, Validation Loss: 0.12783178687095642\n",
      "Epoch: 328/2000, Train Loss: 0.11413773149251938, Validation Loss: 0.12737196683883667\n",
      "Epoch: 329/2000, Train Loss: 0.1139010414481163, Validation Loss: 0.12736986577510834\n",
      "Epoch: 330/2000, Train Loss: 0.11367607861757278, Validation Loss: 0.1270858347415924\n",
      "Epoch: 331/2000, Train Loss: 0.11346213519573212, Validation Loss: 0.1269567310810089\n",
      "Epoch: 332/2000, Train Loss: 0.1132565587759018, Validation Loss: 0.12681370973587036\n",
      "Epoch: 333/2000, Train Loss: 0.11305717378854752, Validation Loss: 0.1265801191329956\n",
      "Epoch: 334/2000, Train Loss: 0.11286274343729019, Validation Loss: 0.12655910849571228\n",
      "Epoch: 335/2000, Train Loss: 0.11267343163490295, Validation Loss: 0.12622977793216705\n",
      "Epoch: 336/2000, Train Loss: 0.11249145120382309, Validation Loss: 0.1263548880815506\n",
      "Epoch: 337/2000, Train Loss: 0.11232122033834457, Validation Loss: 0.12591058015823364\n",
      "Epoch: 338/2000, Train Loss: 0.11217367649078369, Validation Loss: 0.1262894868850708\n",
      "Epoch: 339/2000, Train Loss: 0.1120639368891716, Validation Loss: 0.12569686770439148\n",
      "Epoch: 340/2000, Train Loss: 0.11202883720397949, Validation Loss: 0.1265878975391388\n",
      "Epoch: 341/2000, Train Loss: 0.11209512501955032, Validation Loss: 0.12582744657993317\n",
      "Epoch: 342/2000, Train Loss: 0.11232815682888031, Validation Loss: 0.1273833066225052\n",
      "Epoch: 343/2000, Train Loss: 0.11257617920637131, Validation Loss: 0.12610428035259247\n",
      "Epoch: 344/2000, Train Loss: 0.11273133009672165, Validation Loss: 0.12704221904277802\n",
      "Epoch: 345/2000, Train Loss: 0.11216988414525986, Validation Loss: 0.12496766448020935\n",
      "Epoch: 346/2000, Train Loss: 0.11122119426727295, Validation Loss: 0.12481807917356491\n",
      "Epoch: 347/2000, Train Loss: 0.11041159927845001, Validation Loss: 0.12487971782684326\n",
      "Epoch: 348/2000, Train Loss: 0.11032283306121826, Validation Loss: 0.12459473311901093\n",
      "Epoch: 349/2000, Train Loss: 0.11067551374435425, Validation Loss: 0.12574519217014313\n",
      "Epoch: 350/2000, Train Loss: 0.11075016111135483, Validation Loss: 0.12430412322282791\n",
      "Epoch: 351/2000, Train Loss: 0.11031613498926163, Validation Loss: 0.12433771789073944\n",
      "Epoch: 352/2000, Train Loss: 0.10962827503681183, Validation Loss: 0.1238776296377182\n",
      "Epoch: 353/2000, Train Loss: 0.10931062698364258, Validation Loss: 0.12363207340240479\n",
      "Epoch: 354/2000, Train Loss: 0.10941454768180847, Validation Loss: 0.12447760254144669\n",
      "Epoch: 355/2000, Train Loss: 0.10948552191257477, Validation Loss: 0.1234498918056488\n",
      "Epoch: 356/2000, Train Loss: 0.1092289462685585, Validation Loss: 0.12361056357622147\n",
      "Epoch: 357/2000, Train Loss: 0.10874386876821518, Validation Loss: 0.12315215915441513\n",
      "Epoch: 358/2000, Train Loss: 0.10843859612941742, Validation Loss: 0.1229308545589447\n",
      "Epoch: 359/2000, Train Loss: 0.1084119975566864, Validation Loss: 0.12352316081523895\n",
      "Epoch: 360/2000, Train Loss: 0.10841519385576248, Validation Loss: 0.12273617833852768\n",
      "Epoch: 361/2000, Train Loss: 0.10823027044534683, Validation Loss: 0.12293864041566849\n",
      "Epoch: 362/2000, Train Loss: 0.10787831991910934, Validation Loss: 0.12247786670923233\n",
      "Epoch: 363/2000, Train Loss: 0.10759681463241577, Validation Loss: 0.1222928836941719\n",
      "Epoch: 364/2000, Train Loss: 0.1074863150715828, Validation Loss: 0.12266592681407928\n",
      "Epoch: 365/2000, Train Loss: 0.10743647068738937, Validation Loss: 0.12206096947193146\n",
      "Epoch: 366/2000, Train Loss: 0.10729998350143433, Validation Loss: 0.12229493260383606\n",
      "Epoch: 367/2000, Train Loss: 0.1070438027381897, Validation Loss: 0.12179654091596603\n",
      "Epoch: 368/2000, Train Loss: 0.10678381472826004, Validation Loss: 0.12168396264314651\n",
      "Epoch: 369/2000, Train Loss: 0.1066112145781517, Validation Loss: 0.12183394283056259\n",
      "Epoch: 370/2000, Train Loss: 0.10651111602783203, Validation Loss: 0.12140973657369614\n",
      "Epoch: 371/2000, Train Loss: 0.1064007431268692, Validation Loss: 0.12166918814182281\n",
      "Epoch: 372/2000, Train Loss: 0.1062217578291893, Validation Loss: 0.12117470800876617\n",
      "Epoch: 373/2000, Train Loss: 0.10600101947784424, Validation Loss: 0.12117302417755127\n",
      "Epoch: 374/2000, Train Loss: 0.1057952418923378, Validation Loss: 0.12107095867395401\n",
      "Epoch: 375/2000, Train Loss: 0.10563889145851135, Validation Loss: 0.12080586701631546\n",
      "Epoch: 376/2000, Train Loss: 0.10551479458808899, Validation Loss: 0.12098657339811325\n",
      "Epoch: 377/2000, Train Loss: 0.10538167506456375, Validation Loss: 0.12055467814207077\n",
      "Epoch: 378/2000, Train Loss: 0.10521748661994934, Validation Loss: 0.12066731601953506\n",
      "Epoch: 379/2000, Train Loss: 0.10502766817808151, Validation Loss: 0.12035694718360901\n",
      "Epoch: 380/2000, Train Loss: 0.10483978688716888, Validation Loss: 0.12027901411056519\n",
      "Epoch: 381/2000, Train Loss: 0.10467254370450974, Validation Loss: 0.12025082111358643\n",
      "Epoch: 382/2000, Train Loss: 0.10452646762132645, Validation Loss: 0.11998794972896576\n",
      "Epoch: 383/2000, Train Loss: 0.10438786447048187, Validation Loss: 0.12010374665260315\n",
      "Epoch: 384/2000, Train Loss: 0.10424142330884933, Validation Loss: 0.11974799633026123\n",
      "Epoch: 385/2000, Train Loss: 0.10408161580562592, Validation Loss: 0.1198258176445961\n",
      "Epoch: 386/2000, Train Loss: 0.103910893201828, Validation Loss: 0.11953752487897873\n",
      "Epoch: 387/2000, Train Loss: 0.1037386804819107, Validation Loss: 0.11949808895587921\n",
      "Epoch: 388/2000, Train Loss: 0.10357227921485901, Validation Loss: 0.1193724051117897\n",
      "Epoch: 389/2000, Train Loss: 0.10341477394104004, Validation Loss: 0.11920727789402008\n",
      "Epoch: 390/2000, Train Loss: 0.10326441377401352, Validation Loss: 0.11921658366918564\n",
      "Epoch: 391/2000, Train Loss: 0.10311707854270935, Validation Loss: 0.11895694583654404\n",
      "Epoch: 392/2000, Train Loss: 0.10296909511089325, Validation Loss: 0.11902416497468948\n",
      "Epoch: 393/2000, Train Loss: 0.10281788557767868, Validation Loss: 0.11872579157352448\n",
      "Epoch: 394/2000, Train Loss: 0.10266343504190445, Validation Loss: 0.11879140883684158\n",
      "Epoch: 395/2000, Train Loss: 0.10250594466924667, Validation Loss: 0.11850582808256149\n",
      "Epoch: 396/2000, Train Loss: 0.10234715789556503, Validation Loss: 0.11853981018066406\n",
      "Epoch: 397/2000, Train Loss: 0.10218796133995056, Validation Loss: 0.11829382926225662\n",
      "Epoch: 398/2000, Train Loss: 0.10202950984239578, Validation Loss: 0.11828776448965073\n",
      "Epoch: 399/2000, Train Loss: 0.10187200456857681, Validation Loss: 0.11808466911315918\n",
      "Epoch: 400/2000, Train Loss: 0.10171566158533096, Validation Loss: 0.11804423481225967\n",
      "Epoch: 401/2000, Train Loss: 0.10156028717756271, Validation Loss: 0.11787506192922592\n",
      "Epoch: 402/2000, Train Loss: 0.10140577703714371, Validation Loss: 0.11781162023544312\n",
      "Epoch: 403/2000, Train Loss: 0.10125192254781723, Validation Loss: 0.11766207963228226\n",
      "Epoch: 404/2000, Train Loss: 0.10109858959913254, Validation Loss: 0.11758619546890259\n",
      "Epoch: 405/2000, Train Loss: 0.10094562917947769, Validation Loss: 0.11744184046983719\n",
      "Epoch: 406/2000, Train Loss: 0.10079304873943329, Validation Loss: 0.11736543476581573\n",
      "Epoch: 407/2000, Train Loss: 0.10064084827899933, Validation Loss: 0.11721514910459518\n",
      "Epoch: 408/2000, Train Loss: 0.10048910230398178, Validation Loss: 0.11715488135814667\n",
      "Epoch: 409/2000, Train Loss: 0.10033798962831497, Validation Loss: 0.11698448657989502\n",
      "Epoch: 410/2000, Train Loss: 0.1001879870891571, Validation Loss: 0.11696558445692062\n",
      "Epoch: 411/2000, Train Loss: 0.10004009306430817, Validation Loss: 0.11674609780311584\n",
      "Epoch: 412/2000, Train Loss: 0.09989674389362335, Validation Loss: 0.11682584136724472\n",
      "Epoch: 413/2000, Train Loss: 0.09976330399513245, Validation Loss: 0.11650633066892624\n",
      "Epoch: 414/2000, Train Loss: 0.0996529832482338, Validation Loss: 0.11686182022094727\n",
      "Epoch: 415/2000, Train Loss: 0.09959553182125092, Validation Loss: 0.11640449613332748\n",
      "Epoch: 416/2000, Train Loss: 0.09966208785772324, Validation Loss: 0.11760615557432175\n",
      "Epoch: 417/2000, Train Loss: 0.09998133033514023, Validation Loss: 0.11720843613147736\n",
      "Epoch: 418/2000, Train Loss: 0.1008007749915123, Validation Loss: 0.12015610933303833\n",
      "Epoch: 419/2000, Train Loss: 0.10198656469583511, Validation Loss: 0.11918237805366516\n",
      "Epoch: 420/2000, Train Loss: 0.10315065830945969, Validation Loss: 0.11997057497501373\n",
      "Epoch: 421/2000, Train Loss: 0.10173633694648743, Validation Loss: 0.11602271348237991\n",
      "Epoch: 422/2000, Train Loss: 0.09926923364400864, Validation Loss: 0.11558756977319717\n",
      "Epoch: 423/2000, Train Loss: 0.09844400733709335, Validation Loss: 0.11797742545604706\n",
      "Epoch: 424/2000, Train Loss: 0.09981893002986908, Validation Loss: 0.11708106100559235\n",
      "Epoch: 425/2000, Train Loss: 0.10044357925653458, Validation Loss: 0.1168285459280014\n",
      "Epoch: 426/2000, Train Loss: 0.09874068200588226, Validation Loss: 0.11549754440784454\n",
      "Epoch: 427/2000, Train Loss: 0.09781063348054886, Validation Loss: 0.11573605239391327\n",
      "Epoch: 428/2000, Train Loss: 0.09870423376560211, Validation Loss: 0.1172960102558136\n",
      "Epoch: 429/2000, Train Loss: 0.09896917641162872, Validation Loss: 0.11513218283653259\n",
      "Epoch: 430/2000, Train Loss: 0.09789783507585526, Validation Loss: 0.11486209183931351\n",
      "Epoch: 431/2000, Train Loss: 0.09730010479688644, Validation Loss: 0.11609300225973129\n",
      "Epoch: 432/2000, Train Loss: 0.0978926420211792, Validation Loss: 0.11516429483890533\n",
      "Epoch: 433/2000, Train Loss: 0.09799528121948242, Validation Loss: 0.11515264213085175\n",
      "Epoch: 434/2000, Train Loss: 0.09710473567247391, Validation Loss: 0.11483936011791229\n",
      "Epoch: 435/2000, Train Loss: 0.09684926271438599, Validation Loss: 0.11467524617910385\n",
      "Epoch: 436/2000, Train Loss: 0.09727029502391815, Validation Loss: 0.11537472903728485\n",
      "Epoch: 437/2000, Train Loss: 0.09704805165529251, Validation Loss: 0.11425057053565979\n",
      "Epoch: 438/2000, Train Loss: 0.09644018858671188, Validation Loss: 0.1141732856631279\n",
      "Epoch: 439/2000, Train Loss: 0.0963984802365303, Validation Loss: 0.11501598358154297\n",
      "Epoch: 440/2000, Train Loss: 0.09659389406442642, Validation Loss: 0.11407562345266342\n",
      "Epoch: 441/2000, Train Loss: 0.09631363302469254, Validation Loss: 0.11406107991933823\n",
      "Epoch: 442/2000, Train Loss: 0.09589719772338867, Validation Loss: 0.1142830029129982\n",
      "Epoch: 443/2000, Train Loss: 0.09591351449489594, Validation Loss: 0.11382639408111572\n",
      "Epoch: 444/2000, Train Loss: 0.09598060697317123, Validation Loss: 0.11411740630865097\n",
      "Epoch: 445/2000, Train Loss: 0.0956815704703331, Validation Loss: 0.1136290654540062\n",
      "Epoch: 446/2000, Train Loss: 0.09539896249771118, Validation Loss: 0.11346378177404404\n",
      "Epoch: 447/2000, Train Loss: 0.09540407359600067, Validation Loss: 0.11394651234149933\n",
      "Epoch: 448/2000, Train Loss: 0.09537280350923538, Validation Loss: 0.11329828947782516\n",
      "Epoch: 449/2000, Train Loss: 0.0951220914721489, Validation Loss: 0.11328940838575363\n",
      "Epoch: 450/2000, Train Loss: 0.09491218626499176, Validation Loss: 0.11350381374359131\n",
      "Epoch: 451/2000, Train Loss: 0.09488575160503387, Validation Loss: 0.11308211088180542\n",
      "Epoch: 452/2000, Train Loss: 0.09482038766145706, Validation Loss: 0.11326048523187637\n",
      "Epoch: 453/2000, Train Loss: 0.09460530430078506, Validation Loss: 0.11299657076597214\n",
      "Epoch: 454/2000, Train Loss: 0.09442903846502304, Validation Loss: 0.11280826479196548\n",
      "Epoch: 455/2000, Train Loss: 0.09437208622694016, Validation Loss: 0.11307915300130844\n",
      "Epoch: 456/2000, Train Loss: 0.09428845345973969, Validation Loss: 0.11263378709554672\n",
      "Epoch: 457/2000, Train Loss: 0.09411141276359558, Validation Loss: 0.11262283474206924\n",
      "Epoch: 458/2000, Train Loss: 0.0939486175775528, Validation Loss: 0.11268554627895355\n",
      "Epoch: 459/2000, Train Loss: 0.0938652828335762, Validation Loss: 0.11238165944814682\n",
      "Epoch: 460/2000, Train Loss: 0.09377989917993546, Validation Loss: 0.11253128945827484\n",
      "Epoch: 461/2000, Train Loss: 0.09362995624542236, Validation Loss: 0.11227972060441971\n",
      "Epoch: 462/2000, Train Loss: 0.09347427636384964, Validation Loss: 0.11216457188129425\n",
      "Epoch: 463/2000, Train Loss: 0.09336915612220764, Validation Loss: 0.11229762434959412\n",
      "Epoch: 464/2000, Train Loss: 0.0932793989777565, Validation Loss: 0.11198404431343079\n",
      "Epoch: 465/2000, Train Loss: 0.09315360337495804, Validation Loss: 0.11202506721019745\n",
      "Epoch: 466/2000, Train Loss: 0.09300701320171356, Validation Loss: 0.11192303895950317\n",
      "Epoch: 467/2000, Train Loss: 0.09288541227579117, Validation Loss: 0.11174788326025009\n",
      "Epoch: 468/2000, Train Loss: 0.0927874743938446, Validation Loss: 0.11185155808925629\n",
      "Epoch: 469/2000, Train Loss: 0.09267684817314148, Validation Loss: 0.1115928366780281\n",
      "Epoch: 470/2000, Train Loss: 0.09254424273967743, Validation Loss: 0.1115708127617836\n",
      "Epoch: 471/2000, Train Loss: 0.09241433441638947, Validation Loss: 0.11152998358011246\n",
      "Epoch: 472/2000, Train Loss: 0.09230374544858932, Validation Loss: 0.11134159564971924\n",
      "Epoch: 473/2000, Train Loss: 0.09219890832901001, Validation Loss: 0.11140758544206619\n",
      "Epoch: 474/2000, Train Loss: 0.09208127856254578, Validation Loss: 0.11120517551898956\n",
      "Epoch: 475/2000, Train Loss: 0.0919540673494339, Validation Loss: 0.11116054654121399\n",
      "Epoch: 476/2000, Train Loss: 0.0918327271938324, Validation Loss: 0.11113076657056808\n",
      "Epoch: 477/2000, Train Loss: 0.09172230213880539, Validation Loss: 0.11095714569091797\n",
      "Epoch: 478/2000, Train Loss: 0.09161311388015747, Validation Loss: 0.11099174618721008\n",
      "Epoch: 479/2000, Train Loss: 0.09149599075317383, Validation Loss: 0.11081676930189133\n",
      "Epoch: 480/2000, Train Loss: 0.09137380123138428, Validation Loss: 0.11076734215021133\n",
      "Epoch: 481/2000, Train Loss: 0.09125494956970215, Validation Loss: 0.11072281748056412\n",
      "Epoch: 482/2000, Train Loss: 0.09114246070384979, Validation Loss: 0.11057810485363007\n",
      "Epoch: 483/2000, Train Loss: 0.09103167057037354, Validation Loss: 0.1105964183807373\n",
      "Epoch: 484/2000, Train Loss: 0.09091713279485703, Validation Loss: 0.11044005304574966\n",
      "Epoch: 485/2000, Train Loss: 0.09079921245574951, Validation Loss: 0.1104019433259964\n",
      "Epoch: 486/2000, Train Loss: 0.09068194776773453, Validation Loss: 0.11032653599977493\n",
      "Epoch: 487/2000, Train Loss: 0.0905681625008583, Validation Loss: 0.11020613461732864\n",
      "Epoch: 488/2000, Train Loss: 0.0904565155506134, Validation Loss: 0.11019489169120789\n",
      "Epoch: 489/2000, Train Loss: 0.09034392237663269, Validation Loss: 0.1100444570183754\n",
      "Epoch: 490/2000, Train Loss: 0.09022898226976395, Validation Loss: 0.11001867800951004\n",
      "Epoch: 491/2000, Train Loss: 0.09011294692754745, Validation Loss: 0.1099126935005188\n",
      "Epoch: 492/2000, Train Loss: 0.08999805152416229, Validation Loss: 0.10983072221279144\n",
      "Epoch: 493/2000, Train Loss: 0.08988530188798904, Validation Loss: 0.10978799313306808\n",
      "Epoch: 494/2000, Train Loss: 0.08977390080690384, Validation Loss: 0.1096622571349144\n",
      "Epoch: 495/2000, Train Loss: 0.08966227620840073, Validation Loss: 0.10964007675647736\n",
      "Epoch: 496/2000, Train Loss: 0.08954957872629166, Validation Loss: 0.10951638966798782\n",
      "Epoch: 497/2000, Train Loss: 0.0894361212849617, Validation Loss: 0.10947033762931824\n",
      "Epoch: 498/2000, Train Loss: 0.08932284265756607, Validation Loss: 0.10938671976327896\n",
      "Epoch: 499/2000, Train Loss: 0.08921057730913162, Validation Loss: 0.10930157452821732\n",
      "Epoch: 500/2000, Train Loss: 0.08909939974546432, Validation Loss: 0.10925637185573578\n",
      "Epoch: 501/2000, Train Loss: 0.0889887809753418, Validation Loss: 0.10914386808872223\n",
      "Epoch: 502/2000, Train Loss: 0.08887803554534912, Validation Loss: 0.10910981148481369\n",
      "Epoch: 503/2000, Train Loss: 0.08876686543226242, Validation Loss: 0.10899657011032104\n",
      "Epoch: 504/2000, Train Loss: 0.08865542709827423, Validation Loss: 0.10894972085952759\n",
      "Epoch: 505/2000, Train Loss: 0.0885440781712532, Validation Loss: 0.10885853320360184\n",
      "Epoch: 506/2000, Train Loss: 0.0884331539273262, Validation Loss: 0.10878905653953552\n",
      "Epoch: 507/2000, Train Loss: 0.08832275867462158, Validation Loss: 0.10872513055801392\n",
      "Epoch: 508/2000, Train Loss: 0.08821278810501099, Validation Loss: 0.10863479226827621\n",
      "Epoch: 509/2000, Train Loss: 0.0881030261516571, Validation Loss: 0.1085880696773529\n",
      "Epoch: 510/2000, Train Loss: 0.08799334615468979, Validation Loss: 0.108486108481884\n",
      "Epoch: 511/2000, Train Loss: 0.08788366615772247, Validation Loss: 0.10844320803880692\n",
      "Epoch: 512/2000, Train Loss: 0.08777400106191635, Validation Loss: 0.10834141075611115\n",
      "Epoch: 513/2000, Train Loss: 0.08766444772481918, Validation Loss: 0.10829335451126099\n",
      "Epoch: 514/2000, Train Loss: 0.08755502849817276, Validation Loss: 0.10820065438747406\n",
      "Epoch: 515/2000, Train Loss: 0.08744578063488007, Validation Loss: 0.10814333707094193\n",
      "Epoch: 516/2000, Train Loss: 0.0873367190361023, Validation Loss: 0.10806278884410858\n",
      "Epoch: 517/2000, Train Loss: 0.08722785115242004, Validation Loss: 0.10799518972635269\n",
      "Epoch: 518/2000, Train Loss: 0.0871191993355751, Validation Loss: 0.10792551189661026\n",
      "Epoch: 519/2000, Train Loss: 0.08701078593730927, Validation Loss: 0.10784843564033508\n",
      "Epoch: 520/2000, Train Loss: 0.08690260350704193, Validation Loss: 0.10778700560331345\n",
      "Epoch: 521/2000, Train Loss: 0.08679458498954773, Validation Loss: 0.10770200937986374\n",
      "Epoch: 522/2000, Train Loss: 0.08668671548366547, Validation Loss: 0.10764680802822113\n",
      "Epoch: 523/2000, Train Loss: 0.08657901734113693, Validation Loss: 0.10755505412817001\n",
      "Epoch: 524/2000, Train Loss: 0.08647150546312332, Validation Loss: 0.10750576108694077\n",
      "Epoch: 525/2000, Train Loss: 0.08636418730020523, Validation Loss: 0.10740762948989868\n",
      "Epoch: 526/2000, Train Loss: 0.08625710010528564, Validation Loss: 0.10736645758152008\n",
      "Epoch: 527/2000, Train Loss: 0.08615022897720337, Validation Loss: 0.1072605550289154\n",
      "Epoch: 528/2000, Train Loss: 0.08604361116886139, Validation Loss: 0.10723207890987396\n",
      "Epoch: 529/2000, Train Loss: 0.08593733608722687, Validation Loss: 0.10711316019296646\n",
      "Epoch: 530/2000, Train Loss: 0.08583150058984756, Validation Loss: 0.10710475593805313\n",
      "Epoch: 531/2000, Train Loss: 0.08572633564472198, Validation Loss: 0.10696201771497726\n",
      "Epoch: 532/2000, Train Loss: 0.08562218397855759, Validation Loss: 0.10698851943016052\n",
      "Epoch: 533/2000, Train Loss: 0.08551964908838272, Validation Loss: 0.10680457949638367\n",
      "Epoch: 534/2000, Train Loss: 0.08541996031999588, Validation Loss: 0.10689909011125565\n",
      "Epoch: 535/2000, Train Loss: 0.08532515168190002, Validation Loss: 0.1066468209028244\n",
      "Epoch: 536/2000, Train Loss: 0.08523932099342346, Validation Loss: 0.10688020288944244\n",
      "Epoch: 537/2000, Train Loss: 0.08516931533813477, Validation Loss: 0.10652001947164536\n",
      "Epoch: 538/2000, Train Loss: 0.08512920141220093, Validation Loss: 0.10704466700553894\n",
      "Epoch: 539/2000, Train Loss: 0.08513915538787842, Validation Loss: 0.10654207319021225\n",
      "Epoch: 540/2000, Train Loss: 0.08524009585380554, Validation Loss: 0.10763926804065704\n",
      "Epoch: 541/2000, Train Loss: 0.0854557603597641, Validation Loss: 0.10697022080421448\n",
      "Epoch: 542/2000, Train Loss: 0.08584195375442505, Validation Loss: 0.10870260000228882\n",
      "Epoch: 543/2000, Train Loss: 0.08620548993349075, Validation Loss: 0.10747700929641724\n",
      "Epoch: 544/2000, Train Loss: 0.08646021038293839, Validation Loss: 0.10854146629571915\n",
      "Epoch: 545/2000, Train Loss: 0.08597781509160995, Validation Loss: 0.10644105076789856\n",
      "Epoch: 546/2000, Train Loss: 0.0850951224565506, Validation Loss: 0.10635833442211151\n",
      "Epoch: 547/2000, Train Loss: 0.08422556519508362, Validation Loss: 0.10615723580121994\n",
      "Epoch: 548/2000, Train Loss: 0.08403890579938889, Validation Loss: 0.10606516152620316\n",
      "Epoch: 549/2000, Train Loss: 0.08442679792642593, Validation Loss: 0.10741084814071655\n",
      "Epoch: 550/2000, Train Loss: 0.08476033806800842, Validation Loss: 0.10623308271169662\n",
      "Epoch: 551/2000, Train Loss: 0.08465705066919327, Validation Loss: 0.1065768301486969\n",
      "Epoch: 552/2000, Train Loss: 0.08405845612287521, Validation Loss: 0.10555744916200638\n",
      "Epoch: 553/2000, Train Loss: 0.08354468643665314, Validation Loss: 0.10547828674316406\n",
      "Epoch: 554/2000, Train Loss: 0.08346248418092728, Validation Loss: 0.10621210932731628\n",
      "Epoch: 555/2000, Train Loss: 0.0836755782365799, Validation Loss: 0.1055944636464119\n",
      "Epoch: 556/2000, Train Loss: 0.083785779774189, Validation Loss: 0.1061779111623764\n",
      "Epoch: 557/2000, Train Loss: 0.08353319764137268, Validation Loss: 0.10526644438505173\n",
      "Epoch: 558/2000, Train Loss: 0.08314595371484756, Validation Loss: 0.10527820885181427\n",
      "Epoch: 559/2000, Train Loss: 0.08292172104120255, Validation Loss: 0.10555677860975266\n",
      "Epoch: 560/2000, Train Loss: 0.08295010775327682, Validation Loss: 0.10517634451389313\n",
      "Epoch: 561/2000, Train Loss: 0.08303628861904144, Validation Loss: 0.10575070232152939\n",
      "Epoch: 562/2000, Train Loss: 0.08295156061649323, Validation Loss: 0.10499802231788635\n",
      "Epoch: 563/2000, Train Loss: 0.08271251618862152, Validation Loss: 0.10510428249835968\n",
      "Epoch: 564/2000, Train Loss: 0.08247213810682297, Validation Loss: 0.10502564907073975\n",
      "Epoch: 565/2000, Train Loss: 0.08237221837043762, Validation Loss: 0.10480216890573502\n",
      "Epoch: 566/2000, Train Loss: 0.0823797881603241, Validation Loss: 0.10525087267160416\n",
      "Epoch: 567/2000, Train Loss: 0.08235900849103928, Validation Loss: 0.10468665510416031\n",
      "Epoch: 568/2000, Train Loss: 0.08223796635866165, Validation Loss: 0.1048956885933876\n",
      "Epoch: 569/2000, Train Loss: 0.08204761147499084, Validation Loss: 0.10459668189287186\n",
      "Epoch: 570/2000, Train Loss: 0.0818915143609047, Validation Loss: 0.10450895875692368\n",
      "Epoch: 571/2000, Train Loss: 0.08181554824113846, Validation Loss: 0.10475435107946396\n",
      "Epoch: 572/2000, Train Loss: 0.08178143948316574, Validation Loss: 0.104390449821949\n",
      "Epoch: 573/2000, Train Loss: 0.08172248303890228, Validation Loss: 0.10466740280389786\n",
      "Epoch: 574/2000, Train Loss: 0.08160510659217834, Validation Loss: 0.10428217053413391\n",
      "Epoch: 575/2000, Train Loss: 0.08146033436059952, Validation Loss: 0.1043190285563469\n",
      "Epoch: 576/2000, Train Loss: 0.08133330196142197, Validation Loss: 0.10428481549024582\n",
      "Epoch: 577/2000, Train Loss: 0.08124695718288422, Validation Loss: 0.1040881797671318\n",
      "Epoch: 578/2000, Train Loss: 0.08118528127670288, Validation Loss: 0.10429754108190536\n",
      "Epoch: 579/2000, Train Loss: 0.08111528307199478, Validation Loss: 0.103961281478405\n",
      "Epoch: 580/2000, Train Loss: 0.08101949095726013, Validation Loss: 0.10411594063043594\n",
      "Epoch: 581/2000, Train Loss: 0.08090230077505112, Validation Loss: 0.10388532280921936\n",
      "Epoch: 582/2000, Train Loss: 0.08078577369451523, Validation Loss: 0.10387048125267029\n",
      "Epoch: 583/2000, Train Loss: 0.08068566769361496, Validation Loss: 0.10388513654470444\n",
      "Epoch: 584/2000, Train Loss: 0.0806032121181488, Validation Loss: 0.10370385646820068\n",
      "Epoch: 585/2000, Train Loss: 0.08052744716405869, Validation Loss: 0.10384847223758698\n",
      "Epoch: 586/2000, Train Loss: 0.0804453119635582, Validation Loss: 0.10358451306819916\n",
      "Epoch: 587/2000, Train Loss: 0.08035160601139069, Validation Loss: 0.10369394719600677\n",
      "Epoch: 588/2000, Train Loss: 0.0802486315369606, Validation Loss: 0.1034930944442749\n",
      "Epoch: 589/2000, Train Loss: 0.08014466613531113, Validation Loss: 0.10349224507808685\n",
      "Epoch: 590/2000, Train Loss: 0.08004626631736755, Validation Loss: 0.10344033688306808\n",
      "Epoch: 591/2000, Train Loss: 0.07995570451021194, Validation Loss: 0.1033250093460083\n",
      "Epoch: 592/2000, Train Loss: 0.07987066358327866, Validation Loss: 0.10338924080133438\n",
      "Epoch: 593/2000, Train Loss: 0.0797867402434349, Validation Loss: 0.10319830477237701\n",
      "Epoch: 594/2000, Train Loss: 0.07970038056373596, Validation Loss: 0.10329452902078629\n",
      "Epoch: 595/2000, Train Loss: 0.07960980385541916, Validation Loss: 0.10309307277202606\n",
      "Epoch: 596/2000, Train Loss: 0.07951581478118896, Validation Loss: 0.10315576195716858\n",
      "Epoch: 597/2000, Train Loss: 0.07941997051239014, Validation Loss: 0.10300210863351822\n",
      "Epoch: 598/2000, Train Loss: 0.07932446897029877, Validation Loss: 0.10300183296203613\n",
      "Epoch: 599/2000, Train Loss: 0.0792306438088417, Validation Loss: 0.1029212549328804\n",
      "Epoch: 600/2000, Train Loss: 0.07913900911808014, Validation Loss: 0.10285598039627075\n",
      "Epoch: 601/2000, Train Loss: 0.07904932647943497, Validation Loss: 0.10284214466810226\n",
      "Epoch: 602/2000, Train Loss: 0.07896099239587784, Validation Loss: 0.10272490978240967\n",
      "Epoch: 603/2000, Train Loss: 0.07887335121631622, Validation Loss: 0.10275761783123016\n",
      "Epoch: 604/2000, Train Loss: 0.07878583669662476, Validation Loss: 0.10260520875453949\n",
      "Epoch: 605/2000, Train Loss: 0.07869821786880493, Validation Loss: 0.10266527533531189\n",
      "Epoch: 606/2000, Train Loss: 0.07861033082008362, Validation Loss: 0.1024903804063797\n",
      "Epoch: 607/2000, Train Loss: 0.07852233946323395, Validation Loss: 0.10256757587194443\n",
      "Epoch: 608/2000, Train Loss: 0.07843436300754547, Validation Loss: 0.10237572342157364\n",
      "Epoch: 609/2000, Train Loss: 0.07834675163030624, Validation Loss: 0.10247086733579636\n",
      "Epoch: 610/2000, Train Loss: 0.0782596692442894, Validation Loss: 0.10225945711135864\n",
      "Epoch: 611/2000, Train Loss: 0.07817371189594269, Validation Loss: 0.1023838222026825\n",
      "Epoch: 612/2000, Train Loss: 0.07808926701545715, Validation Loss: 0.10214189440011978\n",
      "Epoch: 613/2000, Train Loss: 0.07800745218992233, Validation Loss: 0.10231923311948776\n",
      "Epoch: 614/2000, Train Loss: 0.07792928069829941, Validation Loss: 0.10202699154615402\n",
      "Epoch: 615/2000, Train Loss: 0.07785724103450775, Validation Loss: 0.10229937732219696\n",
      "Epoch: 616/2000, Train Loss: 0.07779378443956375, Validation Loss: 0.10192796587944031\n",
      "Epoch: 617/2000, Train Loss: 0.0777444913983345, Validation Loss: 0.10236338526010513\n",
      "Epoch: 618/2000, Train Loss: 0.07771339267492294, Validation Loss: 0.10187835246324539\n",
      "Epoch: 619/2000, Train Loss: 0.07771173119544983, Validation Loss: 0.10256505012512207\n",
      "Epoch: 620/2000, Train Loss: 0.0777377337217331, Validation Loss: 0.10192929953336716\n",
      "Epoch: 621/2000, Train Loss: 0.07780804485082626, Validation Loss: 0.10290282964706421\n",
      "Epoch: 622/2000, Train Loss: 0.07788094878196716, Validation Loss: 0.10205380618572235\n",
      "Epoch: 623/2000, Train Loss: 0.07797011733055115, Validation Loss: 0.10311182588338852\n",
      "Epoch: 624/2000, Train Loss: 0.07794319093227386, Validation Loss: 0.1019739881157875\n",
      "Epoch: 625/2000, Train Loss: 0.07783163338899612, Validation Loss: 0.10267718136310577\n",
      "Epoch: 626/2000, Train Loss: 0.07751653343439102, Validation Loss: 0.10153447836637497\n",
      "Epoch: 627/2000, Train Loss: 0.07714513689279556, Validation Loss: 0.10176687687635422\n",
      "Epoch: 628/2000, Train Loss: 0.0767921581864357, Validation Loss: 0.10136502981185913\n",
      "Epoch: 629/2000, Train Loss: 0.07658391445875168, Validation Loss: 0.10128482431173325\n",
      "Epoch: 630/2000, Train Loss: 0.0765359103679657, Validation Loss: 0.1017264798283577\n",
      "Epoch: 631/2000, Train Loss: 0.07658891379833221, Validation Loss: 0.10127667337656021\n",
      "Epoch: 632/2000, Train Loss: 0.07666190713644028, Validation Loss: 0.10199175029993057\n",
      "Epoch: 633/2000, Train Loss: 0.0766645148396492, Validation Loss: 0.10120713710784912\n",
      "Epoch: 634/2000, Train Loss: 0.0765833631157875, Validation Loss: 0.10171572118997574\n",
      "Epoch: 635/2000, Train Loss: 0.0763910636305809, Validation Loss: 0.10096967965364456\n",
      "Epoch: 636/2000, Train Loss: 0.07616526633501053, Validation Loss: 0.10115475952625275\n",
      "Epoch: 637/2000, Train Loss: 0.07595893740653992, Validation Loss: 0.10092968493700027\n",
      "Epoch: 638/2000, Train Loss: 0.07582671195268631, Validation Loss: 0.10083749145269394\n",
      "Epoch: 639/2000, Train Loss: 0.07577113807201385, Validation Loss: 0.101141057908535\n",
      "Epoch: 640/2000, Train Loss: 0.07575896382331848, Validation Loss: 0.10076586902141571\n",
      "Epoch: 641/2000, Train Loss: 0.07574938982725143, Validation Loss: 0.10124403983354568\n",
      "Epoch: 642/2000, Train Loss: 0.07570337504148483, Validation Loss: 0.10067879408597946\n",
      "Epoch: 643/2000, Train Loss: 0.07561734318733215, Validation Loss: 0.10105292499065399\n",
      "Epoch: 644/2000, Train Loss: 0.07548695057630539, Validation Loss: 0.10054158419370651\n",
      "Epoch: 645/2000, Train Loss: 0.07534241676330566, Validation Loss: 0.10071256011724472\n",
      "Epoch: 646/2000, Train Loss: 0.07520310580730438, Validation Loss: 0.10048796981573105\n",
      "Epoch: 647/2000, Train Loss: 0.07508954405784607, Validation Loss: 0.10045018047094345\n",
      "Epoch: 648/2000, Train Loss: 0.07500586658716202, Validation Loss: 0.10054188966751099\n",
      "Epoch: 649/2000, Train Loss: 0.07494483143091202, Validation Loss: 0.10031277686357498\n",
      "Epoch: 650/2000, Train Loss: 0.07489375025033951, Validation Loss: 0.10058626532554626\n",
      "Epoch: 651/2000, Train Loss: 0.07483917474746704, Validation Loss: 0.10022206604480743\n",
      "Epoch: 652/2000, Train Loss: 0.07477497309446335, Validation Loss: 0.10053122043609619\n",
      "Epoch: 653/2000, Train Loss: 0.0746949091553688, Validation Loss: 0.10012856125831604\n",
      "Epoch: 654/2000, Train Loss: 0.07460379600524902, Validation Loss: 0.10037734359502792\n",
      "Epoch: 655/2000, Train Loss: 0.07450259476900101, Validation Loss: 0.10003849118947983\n",
      "Epoch: 656/2000, Train Loss: 0.07439956068992615, Validation Loss: 0.10018163174390793\n",
      "Epoch: 657/2000, Train Loss: 0.07429817318916321, Validation Loss: 0.09997357428073883\n",
      "Epoch: 658/2000, Train Loss: 0.07420278340578079, Validation Loss: 0.10000207275152206\n",
      "Epoch: 659/2000, Train Loss: 0.07411429286003113, Validation Loss: 0.09993711113929749\n",
      "Epoch: 660/2000, Train Loss: 0.07403229176998138, Validation Loss: 0.09986092895269394\n",
      "Epoch: 661/2000, Train Loss: 0.07395534962415695, Validation Loss: 0.09991349279880524\n",
      "Epoch: 662/2000, Train Loss: 0.07388179749250412, Validation Loss: 0.09974928945302963\n",
      "Epoch: 663/2000, Train Loss: 0.07381026446819305, Validation Loss: 0.09988445788621902\n",
      "Epoch: 664/2000, Train Loss: 0.07373946905136108, Validation Loss: 0.09964995831251144\n",
      "Epoch: 665/2000, Train Loss: 0.0736691877245903, Validation Loss: 0.09984253346920013\n",
      "Epoch: 666/2000, Train Loss: 0.07359858602285385, Validation Loss: 0.09955460578203201\n",
      "Epoch: 667/2000, Train Loss: 0.07352878898382187, Validation Loss: 0.09979556500911713\n",
      "Epoch: 668/2000, Train Loss: 0.07345890998840332, Validation Loss: 0.09946474432945251\n",
      "Epoch: 669/2000, Train Loss: 0.07339110225439072, Validation Loss: 0.09975767135620117\n",
      "Epoch: 670/2000, Train Loss: 0.07332395762205124, Validation Loss: 0.09938329458236694\n",
      "Epoch: 671/2000, Train Loss: 0.07326095551252365, Validation Loss: 0.09974014759063721\n",
      "Epoch: 672/2000, Train Loss: 0.0731998160481453, Validation Loss: 0.09931131452322006\n",
      "Epoch: 673/2000, Train Loss: 0.07314594835042953, Validation Loss: 0.09975171834230423\n",
      "Epoch: 674/2000, Train Loss: 0.07309523224830627, Validation Loss: 0.09925254434347153\n",
      "Epoch: 675/2000, Train Loss: 0.07305625826120377, Validation Loss: 0.09980268776416779\n",
      "Epoch: 676/2000, Train Loss: 0.07302066683769226, Validation Loss: 0.09921631962060928\n",
      "Epoch: 677/2000, Train Loss: 0.07300175726413727, Validation Loss: 0.09989646077156067\n",
      "Epoch: 678/2000, Train Loss: 0.07298097759485245, Validation Loss: 0.09920578449964523\n",
      "Epoch: 679/2000, Train Loss: 0.07297707349061966, Validation Loss: 0.09999654442071915\n",
      "Epoch: 680/2000, Train Loss: 0.07295171171426773, Validation Loss: 0.09918948262929916\n",
      "Epoch: 681/2000, Train Loss: 0.07292955368757248, Validation Loss: 0.09999220818281174\n",
      "Epoch: 682/2000, Train Loss: 0.07285135984420776, Validation Loss: 0.09909462183713913\n",
      "Epoch: 683/2000, Train Loss: 0.07275288552045822, Validation Loss: 0.0997529923915863\n",
      "Epoch: 684/2000, Train Loss: 0.07258478552103043, Validation Loss: 0.09890168160200119\n",
      "Epoch: 685/2000, Train Loss: 0.07239978015422821, Validation Loss: 0.09931188821792603\n",
      "Epoch: 686/2000, Train Loss: 0.07219386100769043, Validation Loss: 0.0987429991364479\n",
      "Epoch: 687/2000, Train Loss: 0.07201287895441055, Validation Loss: 0.0988953560590744\n",
      "Epoch: 688/2000, Train Loss: 0.0718693658709526, Validation Loss: 0.09874607622623444\n",
      "Epoch: 689/2000, Train Loss: 0.07177156955003738, Validation Loss: 0.09865351766347885\n",
      "Epoch: 690/2000, Train Loss: 0.07171221822500229, Validation Loss: 0.09886220842599869\n",
      "Epoch: 691/2000, Train Loss: 0.07167770713567734, Validation Loss: 0.09854734688997269\n",
      "Epoch: 692/2000, Train Loss: 0.07165498286485672, Validation Loss: 0.09897145628929138\n",
      "Epoch: 693/2000, Train Loss: 0.07162905484437943, Validation Loss: 0.09848769009113312\n",
      "Epoch: 694/2000, Train Loss: 0.07159697264432907, Validation Loss: 0.0989958643913269\n",
      "Epoch: 695/2000, Train Loss: 0.07154355198144913, Validation Loss: 0.09841997176408768\n",
      "Epoch: 696/2000, Train Loss: 0.07147733122110367, Validation Loss: 0.09890501201152802\n",
      "Epoch: 697/2000, Train Loss: 0.07138553261756897, Validation Loss: 0.09833014756441116\n",
      "Epoch: 698/2000, Train Loss: 0.07128413021564484, Validation Loss: 0.09871652722358704\n",
      "Epoch: 699/2000, Train Loss: 0.07116834074258804, Validation Loss: 0.09823911637067795\n",
      "Epoch: 700/2000, Train Loss: 0.07105347514152527, Validation Loss: 0.09848985075950623\n",
      "Epoch: 701/2000, Train Loss: 0.07094060629606247, Validation Loss: 0.09817595034837723\n",
      "Epoch: 702/2000, Train Loss: 0.07083722949028015, Validation Loss: 0.09828326106071472\n",
      "Epoch: 703/2000, Train Loss: 0.07074379920959473, Validation Loss: 0.09814614802598953\n",
      "Epoch: 704/2000, Train Loss: 0.0706605613231659, Validation Loss: 0.0981195941567421\n",
      "Epoch: 705/2000, Train Loss: 0.07058579474687576, Validation Loss: 0.09813471138477325\n",
      "Epoch: 706/2000, Train Loss: 0.0705174133181572, Validation Loss: 0.0979938805103302\n",
      "Epoch: 707/2000, Train Loss: 0.0704534724354744, Validation Loss: 0.09812813997268677\n",
      "Epoch: 708/2000, Train Loss: 0.07039236277341843, Validation Loss: 0.09789475053548813\n",
      "Epoch: 709/2000, Train Loss: 0.07033353298902512, Validation Loss: 0.09812403470277786\n",
      "Epoch: 710/2000, Train Loss: 0.07027588784694672, Validation Loss: 0.09781339019536972\n",
      "Epoch: 711/2000, Train Loss: 0.07022083550691605, Validation Loss: 0.09812748432159424\n",
      "Epoch: 712/2000, Train Loss: 0.07016696780920029, Validation Loss: 0.09774427860975266\n",
      "Epoch: 713/2000, Train Loss: 0.0701180100440979, Validation Loss: 0.09814754873514175\n",
      "Epoch: 714/2000, Train Loss: 0.07007118314504623, Validation Loss: 0.09768687188625336\n",
      "Epoch: 715/2000, Train Loss: 0.07003337889909744, Validation Loss: 0.09819714725017548\n",
      "Epoch: 716/2000, Train Loss: 0.06999856233596802, Validation Loss: 0.09764821082353592\n",
      "Epoch: 717/2000, Train Loss: 0.06997822970151901, Validation Loss: 0.09828846901655197\n",
      "Epoch: 718/2000, Train Loss: 0.06995885819196701, Validation Loss: 0.09763757139444351\n",
      "Epoch: 719/2000, Train Loss: 0.06995825469493866, Validation Loss: 0.09841260313987732\n",
      "Epoch: 720/2000, Train Loss: 0.06994732469320297, Validation Loss: 0.0976458340883255\n",
      "Epoch: 721/2000, Train Loss: 0.069951131939888, Validation Loss: 0.09850253909826279\n",
      "Epoch: 722/2000, Train Loss: 0.06991659849882126, Validation Loss: 0.09761878103017807\n",
      "Epoch: 723/2000, Train Loss: 0.06987743079662323, Validation Loss: 0.09842251986265182\n",
      "Epoch: 724/2000, Train Loss: 0.06976698338985443, Validation Loss: 0.09748519957065582\n",
      "Epoch: 725/2000, Train Loss: 0.06963346898555756, Validation Loss: 0.09808267652988434\n",
      "Epoch: 726/2000, Train Loss: 0.06943942606449127, Validation Loss: 0.09728547930717468\n",
      "Epoch: 727/2000, Train Loss: 0.06924355030059814, Validation Loss: 0.09761248528957367\n",
      "Epoch: 728/2000, Train Loss: 0.0690535232424736, Validation Loss: 0.09718894213438034\n",
      "Epoch: 729/2000, Train Loss: 0.06890365481376648, Validation Loss: 0.09725382924079895\n",
      "Epoch: 730/2000, Train Loss: 0.06880008429288864, Validation Loss: 0.09726497530937195\n",
      "Epoch: 731/2000, Train Loss: 0.06873898208141327, Validation Loss: 0.0970822125673294\n",
      "Epoch: 732/2000, Train Loss: 0.0687074139714241, Validation Loss: 0.09741217643022537\n",
      "Epoch: 733/2000, Train Loss: 0.06868943572044373, Validation Loss: 0.09701583534479141\n",
      "Epoch: 734/2000, Train Loss: 0.06867467612028122, Validation Loss: 0.0975121259689331\n",
      "Epoch: 735/2000, Train Loss: 0.06864737719297409, Validation Loss: 0.09696809202432632\n",
      "Epoch: 736/2000, Train Loss: 0.06860988587141037, Validation Loss: 0.09750201553106308\n",
      "Epoch: 737/2000, Train Loss: 0.06854618340730667, Validation Loss: 0.09689843654632568\n",
      "Epoch: 738/2000, Train Loss: 0.06846950203180313, Validation Loss: 0.09737059473991394\n",
      "Epoch: 739/2000, Train Loss: 0.06836899369955063, Validation Loss: 0.09681079536676407\n",
      "Epoch: 740/2000, Train Loss: 0.06826256215572357, Validation Loss: 0.09715932607650757\n",
      "Epoch: 741/2000, Train Loss: 0.06814832240343094, Validation Loss: 0.09673760831356049\n",
      "Epoch: 742/2000, Train Loss: 0.0680396631360054, Validation Loss: 0.0969376415014267\n",
      "Epoch: 743/2000, Train Loss: 0.06793814897537231, Validation Loss: 0.09670176357030869\n",
      "Epoch: 744/2000, Train Loss: 0.06784795224666595, Validation Loss: 0.09675296396017075\n",
      "Epoch: 745/2000, Train Loss: 0.06776850670576096, Validation Loss: 0.09669613838195801\n",
      "Epoch: 746/2000, Train Loss: 0.06769838184118271, Validation Loss: 0.096613310277462\n",
      "Epoch: 747/2000, Train Loss: 0.06763530522584915, Validation Loss: 0.09670119732618332\n",
      "Epoch: 748/2000, Train Loss: 0.06757703423500061, Validation Loss: 0.09650762379169464\n",
      "Epoch: 749/2000, Train Loss: 0.06752195954322815, Validation Loss: 0.09670591354370117\n",
      "Epoch: 750/2000, Train Loss: 0.06746846437454224, Validation Loss: 0.09642433375120163\n",
      "Epoch: 751/2000, Train Loss: 0.06741676479578018, Validation Loss: 0.09671039879322052\n",
      "Epoch: 752/2000, Train Loss: 0.06736533343791962, Validation Loss: 0.09635554254055023\n",
      "Epoch: 753/2000, Train Loss: 0.06731658428907394, Validation Loss: 0.09672027081251144\n",
      "Epoch: 754/2000, Train Loss: 0.06726811081171036, Validation Loss: 0.09629635512828827\n",
      "Epoch: 755/2000, Train Loss: 0.06722482293844223, Validation Loss: 0.09674359858036041\n",
      "Epoch: 756/2000, Train Loss: 0.0671822726726532, Validation Loss: 0.09624657034873962\n",
      "Epoch: 757/2000, Train Loss: 0.06714858114719391, Validation Loss: 0.09679022431373596\n",
      "Epoch: 758/2000, Train Loss: 0.06711532920598984, Validation Loss: 0.09621134400367737\n",
      "Epoch: 759/2000, Train Loss: 0.06709502637386322, Validation Loss: 0.09686535596847534\n",
      "Epoch: 760/2000, Train Loss: 0.0670715793967247, Validation Loss: 0.09619442373514175\n",
      "Epoch: 761/2000, Train Loss: 0.06706321239471436, Validation Loss: 0.09695298224687576\n",
      "Epoch: 762/2000, Train Loss: 0.06704063713550568, Validation Loss: 0.09618350863456726\n",
      "Epoch: 763/2000, Train Loss: 0.06702882796525955, Validation Loss: 0.09699531644582748\n",
      "Epoch: 764/2000, Train Loss: 0.06698198616504669, Validation Loss: 0.0961390882730484\n",
      "Epoch: 765/2000, Train Loss: 0.0669327899813652, Validation Loss: 0.0968996211886406\n",
      "Epoch: 766/2000, Train Loss: 0.0668293684720993, Validation Loss: 0.09602143615484238\n",
      "Epoch: 767/2000, Train Loss: 0.06671323627233505, Validation Loss: 0.09661620110273361\n",
      "Epoch: 768/2000, Train Loss: 0.06655300408601761, Validation Loss: 0.09586221724748611\n",
      "Epoch: 769/2000, Train Loss: 0.0663924515247345, Validation Loss: 0.09623068571090698\n",
      "Epoch: 770/2000, Train Loss: 0.0662304013967514, Validation Loss: 0.0957661047577858\n",
      "Epoch: 771/2000, Train Loss: 0.06609312444925308, Validation Loss: 0.09590175747871399\n",
      "Epoch: 772/2000, Train Loss: 0.06598472595214844, Validation Loss: 0.0957871526479721\n",
      "Epoch: 773/2000, Train Loss: 0.06590676307678223, Validation Loss: 0.09570202231407166\n",
      "Epoch: 774/2000, Train Loss: 0.06585335731506348, Validation Loss: 0.09587766230106354\n",
      "Epoch: 775/2000, Train Loss: 0.06581619381904602, Validation Loss: 0.09559884667396545\n",
      "Epoch: 776/2000, Train Loss: 0.06578802317380905, Validation Loss: 0.09596731513738632\n",
      "Epoch: 777/2000, Train Loss: 0.06576064974069595, Validation Loss: 0.09553848206996918\n",
      "Epoch: 778/2000, Train Loss: 0.06573319435119629, Validation Loss: 0.09601623564958572\n",
      "Epoch: 779/2000, Train Loss: 0.06569641083478928, Validation Loss: 0.09548716992139816\n",
      "Epoch: 780/2000, Train Loss: 0.0656564012169838, Validation Loss: 0.09600740671157837\n",
      "Epoch: 781/2000, Train Loss: 0.0656014084815979, Validation Loss: 0.09542825818061829\n",
      "Epoch: 782/2000, Train Loss: 0.06554263830184937, Validation Loss: 0.09593509882688522\n",
      "Epoch: 783/2000, Train Loss: 0.06546803563833237, Validation Loss: 0.09535720944404602\n",
      "Epoch: 784/2000, Train Loss: 0.06539104133844376, Validation Loss: 0.09580696374177933\n",
      "Epoch: 785/2000, Train Loss: 0.0653025284409523, Validation Loss: 0.09528045356273651\n",
      "Epoch: 786/2000, Train Loss: 0.0652146190404892, Validation Loss: 0.09564521163702011\n",
      "Epoch: 787/2000, Train Loss: 0.06512225419282913, Validation Loss: 0.09520938247442245\n",
      "Epoch: 788/2000, Train Loss: 0.06503354758024216, Validation Loss: 0.09547749161720276\n",
      "Epoch: 789/2000, Train Loss: 0.0649462342262268, Validation Loss: 0.09515145421028137\n",
      "Epoch: 790/2000, Train Loss: 0.06486395746469498, Validation Loss: 0.09532465040683746\n",
      "Epoch: 791/2000, Train Loss: 0.06478549540042877, Validation Loss: 0.09510647505521774\n",
      "Epoch: 792/2000, Train Loss: 0.06471150368452072, Validation Loss: 0.09519533812999725\n",
      "Epoch: 793/2000, Train Loss: 0.06464101374149323, Validation Loss: 0.09506934136152267\n",
      "Epoch: 794/2000, Train Loss: 0.0645734965801239, Validation Loss: 0.0950879454612732\n",
      "Epoch: 795/2000, Train Loss: 0.06450819969177246, Validation Loss: 0.09503445774316788\n",
      "Epoch: 796/2000, Train Loss: 0.06444454938173294, Validation Loss: 0.09499569237232208\n",
      "Epoch: 797/2000, Train Loss: 0.06438209861516953, Validation Loss: 0.09499888122081757\n",
      "Epoch: 798/2000, Train Loss: 0.06432054191827774, Validation Loss: 0.0949113667011261\n",
      "Epoch: 799/2000, Train Loss: 0.06425977498292923, Validation Loss: 0.09496398270130157\n",
      "Epoch: 800/2000, Train Loss: 0.06419981271028519, Validation Loss: 0.09483002871274948\n",
      "Epoch: 801/2000, Train Loss: 0.06414090096950531, Validation Loss: 0.09493589401245117\n",
      "Epoch: 802/2000, Train Loss: 0.06408336013555527, Validation Loss: 0.0947493463754654\n",
      "Epoch: 803/2000, Train Loss: 0.0640280544757843, Validation Loss: 0.09492592513561249\n",
      "Epoch: 804/2000, Train Loss: 0.06397580355405807, Validation Loss: 0.0946703627705574\n",
      "Epoch: 805/2000, Train Loss: 0.06392893940210342, Validation Loss: 0.09495515376329422\n",
      "Epoch: 806/2000, Train Loss: 0.0638892874121666, Validation Loss: 0.09460284560918808\n",
      "Epoch: 807/2000, Train Loss: 0.06386319547891617, Validation Loss: 0.09506738930940628\n",
      "Epoch: 808/2000, Train Loss: 0.06385433673858643, Validation Loss: 0.09458222985267639\n",
      "Epoch: 809/2000, Train Loss: 0.06387990713119507, Validation Loss: 0.09535512328147888\n",
      "Epoch: 810/2000, Train Loss: 0.06394389271736145, Validation Loss: 0.09470529854297638\n",
      "Epoch: 811/2000, Train Loss: 0.06409021466970444, Validation Loss: 0.09597835689783096\n",
      "Epoch: 812/2000, Train Loss: 0.06429855525493622, Validation Loss: 0.09513934701681137\n",
      "Epoch: 813/2000, Train Loss: 0.06465843319892883, Validation Loss: 0.09698614478111267\n",
      "Epoch: 814/2000, Train Loss: 0.06499169766902924, Validation Loss: 0.09577538818120956\n",
      "Epoch: 815/2000, Train Loss: 0.06540251523256302, Validation Loss: 0.09749401360750198\n",
      "Epoch: 816/2000, Train Loss: 0.06532185524702072, Validation Loss: 0.09544647485017776\n",
      "Epoch: 817/2000, Train Loss: 0.06495419144630432, Validation Loss: 0.0959264487028122\n",
      "Epoch: 818/2000, Train Loss: 0.0640273243188858, Validation Loss: 0.09429055452346802\n",
      "Epoch: 819/2000, Train Loss: 0.06325080990791321, Validation Loss: 0.09428337961435318\n",
      "Epoch: 820/2000, Train Loss: 0.0629909336566925, Validation Loss: 0.09503616392612457\n",
      "Epoch: 821/2000, Train Loss: 0.0632624626159668, Validation Loss: 0.09460469335317612\n",
      "Epoch: 822/2000, Train Loss: 0.06369365751743317, Validation Loss: 0.09593730419874191\n",
      "Epoch: 823/2000, Train Loss: 0.0638008862733841, Validation Loss: 0.09453793615102768\n",
      "Epoch: 824/2000, Train Loss: 0.0635550394654274, Validation Loss: 0.09494189918041229\n",
      "Epoch: 825/2000, Train Loss: 0.0630299299955368, Validation Loss: 0.09409891813993454\n",
      "Epoch: 826/2000, Train Loss: 0.06265759468078613, Validation Loss: 0.09404963999986649\n",
      "Epoch: 827/2000, Train Loss: 0.06262026727199554, Validation Loss: 0.0947512686252594\n",
      "Epoch: 828/2000, Train Loss: 0.06280691176652908, Validation Loss: 0.09415742009878159\n",
      "Epoch: 829/2000, Train Loss: 0.06295951455831528, Validation Loss: 0.09495484083890915\n",
      "Epoch: 830/2000, Train Loss: 0.06285982578992844, Validation Loss: 0.09397370368242264\n",
      "Epoch: 831/2000, Train Loss: 0.06259634345769882, Validation Loss: 0.0941891223192215\n",
      "Epoch: 832/2000, Train Loss: 0.06232885271310806, Validation Loss: 0.0940278097987175\n",
      "Epoch: 833/2000, Train Loss: 0.062226802110672, Validation Loss: 0.0938510149717331\n",
      "Epoch: 834/2000, Train Loss: 0.062279339879751205, Validation Loss: 0.0944809764623642\n",
      "Epoch: 835/2000, Train Loss: 0.062349215149879456, Validation Loss: 0.09385304898023605\n",
      "Epoch: 836/2000, Train Loss: 0.06232648342847824, Validation Loss: 0.0943310409784317\n",
      "Epoch: 837/2000, Train Loss: 0.06217658519744873, Validation Loss: 0.09374605864286423\n",
      "Epoch: 838/2000, Train Loss: 0.06199761480093002, Validation Loss: 0.09382721036672592\n",
      "Epoch: 839/2000, Train Loss: 0.06187763065099716, Validation Loss: 0.09390437602996826\n",
      "Epoch: 840/2000, Train Loss: 0.061848461627960205, Validation Loss: 0.09363783150911331\n",
      "Epoch: 841/2000, Train Loss: 0.06186413764953613, Validation Loss: 0.09408906102180481\n",
      "Epoch: 842/2000, Train Loss: 0.061851102858781815, Validation Loss: 0.09358083456754684\n",
      "Epoch: 843/2000, Train Loss: 0.061779771000146866, Validation Loss: 0.0938858762383461\n",
      "Epoch: 844/2000, Train Loss: 0.06166188046336174, Validation Loss: 0.09356310218572617\n",
      "Epoch: 845/2000, Train Loss: 0.061550624668598175, Validation Loss: 0.09358272701501846\n",
      "Epoch: 846/2000, Train Loss: 0.061479587107896805, Validation Loss: 0.09370037913322449\n",
      "Epoch: 847/2000, Train Loss: 0.06144797429442406, Validation Loss: 0.0934501588344574\n",
      "Epoch: 848/2000, Train Loss: 0.06142757833003998, Validation Loss: 0.09376754611730576\n",
      "Epoch: 849/2000, Train Loss: 0.061387158930301666, Validation Loss: 0.09338747709989548\n",
      "Epoch: 850/2000, Train Loss: 0.061318881809711456, Validation Loss: 0.09360651671886444\n",
      "Epoch: 851/2000, Train Loss: 0.061231765896081924, Validation Loss: 0.09337331354618073\n",
      "Epoch: 852/2000, Train Loss: 0.06114867702126503, Validation Loss: 0.09339050203561783\n",
      "Epoch: 853/2000, Train Loss: 0.06108368933200836, Validation Loss: 0.09344061464071274\n",
      "Epoch: 854/2000, Train Loss: 0.06103719398379326, Validation Loss: 0.09326519817113876\n",
      "Epoch: 855/2000, Train Loss: 0.06099820137023926, Validation Loss: 0.09347663074731827\n",
      "Epoch: 856/2000, Train Loss: 0.06095331534743309, Validation Loss: 0.09320364892482758\n",
      "Epoch: 857/2000, Train Loss: 0.06089659407734871, Validation Loss: 0.09338942170143127\n",
      "Epoch: 858/2000, Train Loss: 0.06082897260785103, Validation Loss: 0.09317757189273834\n",
      "Epoch: 859/2000, Train Loss: 0.06075873598456383, Validation Loss: 0.0932365357875824\n",
      "Epoch: 860/2000, Train Loss: 0.060693010687828064, Validation Loss: 0.09319040924310684\n",
      "Epoch: 861/2000, Train Loss: 0.06063548102974892, Validation Loss: 0.09310806542634964\n",
      "Epoch: 862/2000, Train Loss: 0.06058475747704506, Validation Loss: 0.09320560097694397\n",
      "Epoch: 863/2000, Train Loss: 0.060536351054906845, Validation Loss: 0.0930262953042984\n",
      "Epoch: 864/2000, Train Loss: 0.060485851019620895, Validation Loss: 0.09317020326852798\n",
      "Epoch: 865/2000, Train Loss: 0.06043057516217232, Validation Loss: 0.09297589957714081\n",
      "Epoch: 866/2000, Train Loss: 0.060371167957782745, Validation Loss: 0.09307831525802612\n",
      "Epoch: 867/2000, Train Loss: 0.06030955910682678, Validation Loss: 0.09294909238815308\n",
      "Epoch: 868/2000, Train Loss: 0.060248587280511856, Validation Loss: 0.0929683968424797\n",
      "Epoch: 869/2000, Train Loss: 0.06019006296992302, Validation Loss: 0.09293999522924423\n",
      "Epoch: 870/2000, Train Loss: 0.060134463012218475, Validation Loss: 0.09287524968385696\n",
      "Epoch: 871/2000, Train Loss: 0.06008107587695122, Validation Loss: 0.09292849898338318\n",
      "Epoch: 872/2000, Train Loss: 0.06002851948142052, Validation Loss: 0.09280439466238022\n",
      "Epoch: 873/2000, Train Loss: 0.0599755235016346, Validation Loss: 0.09289155900478363\n",
      "Epoch: 874/2000, Train Loss: 0.05992121621966362, Validation Loss: 0.09274763613939285\n",
      "Epoch: 875/2000, Train Loss: 0.059865519404411316, Validation Loss: 0.09282565861940384\n",
      "Epoch: 876/2000, Train Loss: 0.059808675199747086, Validation Loss: 0.09270261228084564\n",
      "Epoch: 877/2000, Train Loss: 0.05975139141082764, Validation Loss: 0.09274628758430481\n",
      "Epoch: 878/2000, Train Loss: 0.05969420447945595, Validation Loss: 0.09266908466815948\n",
      "Epoch: 879/2000, Train Loss: 0.05963760241866112, Validation Loss: 0.09266787022352219\n",
      "Epoch: 880/2000, Train Loss: 0.059581778943538666, Validation Loss: 0.09263992309570312\n",
      "Epoch: 881/2000, Train Loss: 0.0595267079770565, Validation Loss: 0.09259511530399323\n",
      "Epoch: 882/2000, Train Loss: 0.0594722181558609, Validation Loss: 0.0926068052649498\n",
      "Epoch: 883/2000, Train Loss: 0.05941805616021156, Validation Loss: 0.09252943098545074\n",
      "Epoch: 884/2000, Train Loss: 0.05936400219798088, Validation Loss: 0.09256713837385178\n",
      "Epoch: 885/2000, Train Loss: 0.05930986627936363, Validation Loss: 0.09247138351202011\n",
      "Epoch: 886/2000, Train Loss: 0.05925557762384415, Validation Loss: 0.09252121299505234\n",
      "Epoch: 887/2000, Train Loss: 0.05920107290148735, Validation Loss: 0.09241849929094315\n",
      "Epoch: 888/2000, Train Loss: 0.05914640799164772, Validation Loss: 0.09246936440467834\n",
      "Epoch: 889/2000, Train Loss: 0.05909160152077675, Validation Loss: 0.09236784279346466\n",
      "Epoch: 890/2000, Train Loss: 0.059036727994680405, Validation Loss: 0.0924140214920044\n",
      "Epoch: 891/2000, Train Loss: 0.05898182466626167, Validation Loss: 0.0923188254237175\n",
      "Epoch: 892/2000, Train Loss: 0.05892696604132652, Validation Loss: 0.0923587903380394\n",
      "Epoch: 893/2000, Train Loss: 0.058872152119874954, Validation Loss: 0.09227102249860764\n",
      "Epoch: 894/2000, Train Loss: 0.058817435055971146, Validation Loss: 0.09230510145425797\n",
      "Epoch: 895/2000, Train Loss: 0.0587627999484539, Validation Loss: 0.09222272038459778\n",
      "Epoch: 896/2000, Train Loss: 0.058708272874355316, Validation Loss: 0.09225305169820786\n",
      "Epoch: 897/2000, Train Loss: 0.05865384265780449, Validation Loss: 0.0921727865934372\n",
      "Epoch: 898/2000, Train Loss: 0.05859953165054321, Validation Loss: 0.09220337867736816\n",
      "Epoch: 899/2000, Train Loss: 0.05854532867670059, Validation Loss: 0.09212101250886917\n",
      "Epoch: 900/2000, Train Loss: 0.05849127471446991, Validation Loss: 0.09215685725212097\n",
      "Epoch: 901/2000, Train Loss: 0.05843736603856087, Validation Loss: 0.09206673502922058\n",
      "Epoch: 902/2000, Train Loss: 0.058383673429489136, Validation Loss: 0.09211453050374985\n",
      "Epoch: 903/2000, Train Loss: 0.0583302341401577, Validation Loss: 0.09200941771268845\n",
      "Epoch: 904/2000, Train Loss: 0.05827717483043671, Validation Loss: 0.0920790433883667\n",
      "Epoch: 905/2000, Train Loss: 0.058224644511938095, Validation Loss: 0.09194859117269516\n",
      "Epoch: 906/2000, Train Loss: 0.05817291885614395, Validation Loss: 0.09205490350723267\n",
      "Epoch: 907/2000, Train Loss: 0.05812235176563263, Validation Loss: 0.09188313037157059\n",
      "Epoch: 908/2000, Train Loss: 0.05807371065020561, Validation Loss: 0.092050701379776\n",
      "Epoch: 909/2000, Train Loss: 0.05802782624959946, Validation Loss: 0.09181376546621323\n",
      "Epoch: 910/2000, Train Loss: 0.057986848056316376, Validation Loss: 0.0920867919921875\n",
      "Epoch: 911/2000, Train Loss: 0.057952847331762314, Validation Loss: 0.09175083786249161\n",
      "Epoch: 912/2000, Train Loss: 0.05793190747499466, Validation Loss: 0.09221095591783524\n",
      "Epoch: 913/2000, Train Loss: 0.05792896822094917, Validation Loss: 0.09173266589641571\n",
      "Epoch: 914/2000, Train Loss: 0.057961829006671906, Validation Loss: 0.09253200143575668\n",
      "Epoch: 915/2000, Train Loss: 0.05803924426436424, Validation Loss: 0.09187552332878113\n",
      "Epoch: 916/2000, Train Loss: 0.05821182578802109, Validation Loss: 0.09327251464128494\n",
      "Epoch: 917/2000, Train Loss: 0.0584728978574276, Validation Loss: 0.0924370288848877\n",
      "Epoch: 918/2000, Train Loss: 0.058943577110767365, Validation Loss: 0.09464710205793381\n",
      "Epoch: 919/2000, Train Loss: 0.05944611877202988, Validation Loss: 0.09348084032535553\n",
      "Epoch: 920/2000, Train Loss: 0.060150302946567535, Validation Loss: 0.09574376791715622\n",
      "Epoch: 921/2000, Train Loss: 0.06025531515479088, Validation Loss: 0.09339088946580887\n",
      "Epoch: 922/2000, Train Loss: 0.05999383702874184, Validation Loss: 0.09387914091348648\n",
      "Epoch: 923/2000, Train Loss: 0.05871763452887535, Validation Loss: 0.09157069772481918\n",
      "Epoch: 924/2000, Train Loss: 0.057565849274396896, Validation Loss: 0.09150480479001999\n",
      "Epoch: 925/2000, Train Loss: 0.05720028653740883, Validation Loss: 0.09263820201158524\n",
      "Epoch: 926/2000, Train Loss: 0.05769002065062523, Validation Loss: 0.09217526763677597\n",
      "Epoch: 927/2000, Train Loss: 0.058356720954179764, Validation Loss: 0.09370091557502747\n",
      "Epoch: 928/2000, Train Loss: 0.058360692113637924, Validation Loss: 0.0918300449848175\n",
      "Epoch: 929/2000, Train Loss: 0.05782220885157585, Validation Loss: 0.09198521822690964\n",
      "Epoch: 930/2000, Train Loss: 0.05710696056485176, Validation Loss: 0.09157480299472809\n",
      "Epoch: 931/2000, Train Loss: 0.0568854846060276, Validation Loss: 0.09142392873764038\n",
      "Epoch: 932/2000, Train Loss: 0.0571724958717823, Validation Loss: 0.09268645197153091\n",
      "Epoch: 933/2000, Train Loss: 0.05747072771191597, Validation Loss: 0.09157583862543106\n",
      "Epoch: 934/2000, Train Loss: 0.05743132531642914, Validation Loss: 0.0920880064368248\n",
      "Epoch: 935/2000, Train Loss: 0.05701039358973503, Validation Loss: 0.0912378579378128\n",
      "Epoch: 936/2000, Train Loss: 0.056654222309589386, Validation Loss: 0.09120818227529526\n",
      "Epoch: 937/2000, Train Loss: 0.056616902351379395, Validation Loss: 0.09191495180130005\n",
      "Epoch: 938/2000, Train Loss: 0.05679631605744362, Validation Loss: 0.09128862619400024\n",
      "Epoch: 939/2000, Train Loss: 0.05690556764602661, Validation Loss: 0.09197379648685455\n",
      "Epoch: 940/2000, Train Loss: 0.056753575801849365, Validation Loss: 0.09112962335348129\n",
      "Epoch: 941/2000, Train Loss: 0.056490708142519, Validation Loss: 0.09123995155096054\n",
      "Epoch: 942/2000, Train Loss: 0.05632110685110092, Validation Loss: 0.09142839908599854\n",
      "Epoch: 943/2000, Train Loss: 0.05633513256907463, Validation Loss: 0.0910855308175087\n",
      "Epoch: 944/2000, Train Loss: 0.056421536952257156, Validation Loss: 0.09171010553836823\n",
      "Epoch: 945/2000, Train Loss: 0.05641324818134308, Validation Loss: 0.09101112186908722\n",
      "Epoch: 946/2000, Train Loss: 0.05628495290875435, Validation Loss: 0.09125424176454544\n",
      "Epoch: 947/2000, Train Loss: 0.05611617863178253, Validation Loss: 0.09106769412755966\n",
      "Epoch: 948/2000, Train Loss: 0.05602491274476051, Validation Loss: 0.09093641489744186\n",
      "Epoch: 949/2000, Train Loss: 0.05602717027068138, Validation Loss: 0.09136517345905304\n",
      "Epoch: 950/2000, Train Loss: 0.05604659020900726, Validation Loss: 0.09089389443397522\n",
      "Epoch: 951/2000, Train Loss: 0.05601068213582039, Validation Loss: 0.09123612940311432\n",
      "Epoch: 952/2000, Train Loss: 0.055907171219587326, Validation Loss: 0.09087686985731125\n",
      "Epoch: 953/2000, Train Loss: 0.055797334760427475, Validation Loss: 0.09090042114257812\n",
      "Epoch: 954/2000, Train Loss: 0.055731844156980515, Validation Loss: 0.09104649722576141\n",
      "Epoch: 955/2000, Train Loss: 0.05571302771568298, Validation Loss: 0.09077803045511246\n",
      "Epoch: 956/2000, Train Loss: 0.05570172891020775, Validation Loss: 0.09111294895410538\n",
      "Epoch: 957/2000, Train Loss: 0.05565878003835678, Validation Loss: 0.09074262529611588\n",
      "Epoch: 958/2000, Train Loss: 0.05558373034000397, Validation Loss: 0.09090510755777359\n",
      "Epoch: 959/2000, Train Loss: 0.05550079420208931, Validation Loss: 0.0907944068312645\n",
      "Epoch: 960/2000, Train Loss: 0.055438533425331116, Validation Loss: 0.09071334451436996\n",
      "Epoch: 961/2000, Train Loss: 0.05540185421705246, Validation Loss: 0.09090530127286911\n",
      "Epoch: 962/2000, Train Loss: 0.05537383630871773, Validation Loss: 0.09064459055662155\n",
      "Epoch: 963/2000, Train Loss: 0.05533485487103462, Validation Loss: 0.09086944907903671\n",
      "Epoch: 964/2000, Train Loss: 0.055277079343795776, Validation Loss: 0.0906335785984993\n",
      "Epoch: 965/2000, Train Loss: 0.05521057918667793, Validation Loss: 0.09070635586977005\n",
      "Epoch: 966/2000, Train Loss: 0.055149152874946594, Validation Loss: 0.09068410098552704\n",
      "Epoch: 967/2000, Train Loss: 0.055100612342357635, Validation Loss: 0.09057852625846863\n",
      "Epoch: 968/2000, Train Loss: 0.05506150797009468, Validation Loss: 0.0907275378704071\n",
      "Epoch: 969/2000, Train Loss: 0.055022239685058594, Validation Loss: 0.09051871299743652\n",
      "Epoch: 970/2000, Train Loss: 0.054975736886262894, Validation Loss: 0.09066884964704514\n",
      "Epoch: 971/2000, Train Loss: 0.054921094328165054, Validation Loss: 0.0905032679438591\n",
      "Epoch: 972/2000, Train Loss: 0.054863836616277695, Validation Loss: 0.09054552763700485\n",
      "Epoch: 973/2000, Train Loss: 0.054809775203466415, Validation Loss: 0.09052626043558121\n",
      "Epoch: 974/2000, Train Loss: 0.054761629551649094, Validation Loss: 0.090446338057518\n",
      "Epoch: 975/2000, Train Loss: 0.05471772328019142, Validation Loss: 0.09054483473300934\n",
      "Epoch: 976/2000, Train Loss: 0.05467407405376434, Validation Loss: 0.09039469808340073\n",
      "Epoch: 977/2000, Train Loss: 0.054627545177936554, Validation Loss: 0.0905081182718277\n",
      "Epoch: 978/2000, Train Loss: 0.054577264934778214, Validation Loss: 0.09037528187036514\n",
      "Epoch: 979/2000, Train Loss: 0.054525043815374374, Validation Loss: 0.09042420983314514\n",
      "Epoch: 980/2000, Train Loss: 0.054473280906677246, Validation Loss: 0.09037604928016663\n",
      "Epoch: 981/2000, Train Loss: 0.0544237457215786, Validation Loss: 0.09033887088298798\n",
      "Epoch: 982/2000, Train Loss: 0.05437657982110977, Validation Loss: 0.09037812799215317\n",
      "Epoch: 983/2000, Train Loss: 0.05433065816760063, Validation Loss: 0.09027872234582901\n",
      "Epoch: 984/2000, Train Loss: 0.05428449437022209, Validation Loss: 0.09035652875900269\n",
      "Epoch: 985/2000, Train Loss: 0.05423703044652939, Validation Loss: 0.09024321287870407\n",
      "Epoch: 986/2000, Train Loss: 0.054188162088394165, Validation Loss: 0.09030486643314362\n",
      "Epoch: 987/2000, Train Loss: 0.054138392210006714, Validation Loss: 0.09022543579339981\n",
      "Epoch: 988/2000, Train Loss: 0.054088592529296875, Validation Loss: 0.09024029970169067\n",
      "Epoch: 989/2000, Train Loss: 0.05403940752148628, Validation Loss: 0.09021785110235214\n",
      "Epoch: 990/2000, Train Loss: 0.05399106815457344, Validation Loss: 0.09018179029226303\n",
      "Epoch: 991/2000, Train Loss: 0.053943414241075516, Validation Loss: 0.09020823985338211\n",
      "Epoch: 992/2000, Train Loss: 0.05389605835080147, Validation Loss: 0.09013472497463226\n",
      "Epoch: 993/2000, Train Loss: 0.05384858325123787, Validation Loss: 0.09018527716398239\n",
      "Epoch: 994/2000, Train Loss: 0.05380076542496681, Validation Loss: 0.09009695053100586\n",
      "Epoch: 995/2000, Train Loss: 0.05375254526734352, Validation Loss: 0.09014855325222015\n",
      "Epoch: 996/2000, Train Loss: 0.053703997284173965, Validation Loss: 0.09006612747907639\n",
      "Epoch: 997/2000, Train Loss: 0.05365527421236038, Validation Loss: 0.0901036262512207\n",
      "Epoch: 998/2000, Train Loss: 0.05360652133822441, Validation Loss: 0.09003766626119614\n",
      "Epoch: 999/2000, Train Loss: 0.05355784296989441, Validation Loss: 0.0900556743144989\n",
      "Epoch: 1000/2000, Train Loss: 0.053509291261434555, Validation Loss: 0.09000936150550842\n",
      "Epoch: 1001/2000, Train Loss: 0.05346085503697395, Validation Loss: 0.09001033008098602\n",
      "Epoch: 1002/2000, Train Loss: 0.0534125417470932, Validation Loss: 0.08998052775859833\n",
      "Epoch: 1003/2000, Train Loss: 0.053364310413599014, Validation Loss: 0.0899670273065567\n",
      "Epoch: 1004/2000, Train Loss: 0.0533161498606205, Validation Loss: 0.0899490937590599\n",
      "Epoch: 1005/2000, Train Loss: 0.05326805263757706, Validation Loss: 0.089924655854702\n",
      "Epoch: 1006/2000, Train Loss: 0.05322003737092018, Validation Loss: 0.08991727232933044\n",
      "Epoch: 1007/2000, Train Loss: 0.05317212641239166, Validation Loss: 0.08988232910633087\n",
      "Epoch: 1008/2000, Train Loss: 0.05312435328960419, Validation Loss: 0.08988723158836365\n",
      "Epoch: 1009/2000, Train Loss: 0.05307675898075104, Validation Loss: 0.08983835577964783\n",
      "Epoch: 1010/2000, Train Loss: 0.05302939563989639, Validation Loss: 0.0898628756403923\n",
      "Epoch: 1011/2000, Train Loss: 0.05298236012458801, Validation Loss: 0.08979007601737976\n",
      "Epoch: 1012/2000, Train Loss: 0.05293586477637291, Validation Loss: 0.0898495465517044\n",
      "Epoch: 1013/2000, Train Loss: 0.052890144288539886, Validation Loss: 0.08973504602909088\n",
      "Epoch: 1014/2000, Train Loss: 0.05284593999385834, Validation Loss: 0.08985606580972672\n",
      "Epoch: 1015/2000, Train Loss: 0.052803684026002884, Validation Loss: 0.08967449516057968\n",
      "Epoch: 1016/2000, Train Loss: 0.05276523903012276, Validation Loss: 0.08989410847425461\n",
      "Epoch: 1017/2000, Train Loss: 0.052730776369571686, Validation Loss: 0.08961714804172516\n",
      "Epoch: 1018/2000, Train Loss: 0.052703890949487686, Validation Loss: 0.08997683227062225\n",
      "Epoch: 1019/2000, Train Loss: 0.05268341675400734, Validation Loss: 0.08957765996456146\n",
      "Epoch: 1020/2000, Train Loss: 0.05267604812979698, Validation Loss: 0.09011872112751007\n",
      "Epoch: 1021/2000, Train Loss: 0.05267654359340668, Validation Loss: 0.08957520872354507\n",
      "Epoch: 1022/2000, Train Loss: 0.052696872502565384, Validation Loss: 0.09032943099737167\n",
      "Epoch: 1023/2000, Train Loss: 0.05272199586033821, Validation Loss: 0.08962476998567581\n",
      "Epoch: 1024/2000, Train Loss: 0.05277373269200325, Validation Loss: 0.0905938521027565\n",
      "Epoch: 1025/2000, Train Loss: 0.052815135568380356, Validation Loss: 0.08971487730741501\n",
      "Epoch: 1026/2000, Train Loss: 0.052881620824337006, Validation Loss: 0.09082832932472229\n",
      "Epoch: 1027/2000, Train Loss: 0.05289950594305992, Validation Loss: 0.08977387845516205\n",
      "Epoch: 1028/2000, Train Loss: 0.05292097106575966, Validation Loss: 0.09085185080766678\n",
      "Epoch: 1029/2000, Train Loss: 0.052843332290649414, Validation Loss: 0.0896875411272049\n",
      "Epoch: 1030/2000, Train Loss: 0.05274038761854172, Validation Loss: 0.0905090793967247\n",
      "Epoch: 1031/2000, Train Loss: 0.052540626376867294, Validation Loss: 0.08947509527206421\n",
      "Epoch: 1032/2000, Train Loss: 0.052333440631628036, Validation Loss: 0.08993697166442871\n",
      "Epoch: 1033/2000, Train Loss: 0.05212104693055153, Validation Loss: 0.08937988430261612\n",
      "Epoch: 1034/2000, Train Loss: 0.05196109041571617, Validation Loss: 0.08949580788612366\n",
      "Epoch: 1035/2000, Train Loss: 0.05186561122536659, Validation Loss: 0.089542917907238\n",
      "Epoch: 1036/2000, Train Loss: 0.05183202773332596, Validation Loss: 0.08933161199092865\n",
      "Epoch: 1037/2000, Train Loss: 0.051840927451848984, Validation Loss: 0.08981368690729141\n",
      "Epoch: 1038/2000, Train Loss: 0.05186599865555763, Validation Loss: 0.089316725730896\n",
      "Epoch: 1039/2000, Train Loss: 0.05189042165875435, Validation Loss: 0.08998234570026398\n",
      "Epoch: 1040/2000, Train Loss: 0.051887787878513336, Validation Loss: 0.08930303156375885\n",
      "Epoch: 1041/2000, Train Loss: 0.05186426639556885, Validation Loss: 0.0899466872215271\n",
      "Epoch: 1042/2000, Train Loss: 0.05179924890398979, Validation Loss: 0.0892433226108551\n",
      "Epoch: 1043/2000, Train Loss: 0.051716625690460205, Validation Loss: 0.08973515033721924\n",
      "Epoch: 1044/2000, Train Loss: 0.051611125469207764, Validation Loss: 0.08918676525354385\n",
      "Epoch: 1045/2000, Train Loss: 0.05150765925645828, Validation Loss: 0.08947165310382843\n",
      "Epoch: 1046/2000, Train Loss: 0.051411524415016174, Validation Loss: 0.08920054137706757\n",
      "Epoch: 1047/2000, Train Loss: 0.05133352428674698, Validation Loss: 0.08926964551210403\n",
      "Epoch: 1048/2000, Train Loss: 0.05127475783228874, Validation Loss: 0.08928552269935608\n",
      "Epoch: 1049/2000, Train Loss: 0.05123293772339821, Validation Loss: 0.08915460109710693\n",
      "Epoch: 1050/2000, Train Loss: 0.051202964037656784, Validation Loss: 0.08938544243574142\n",
      "Epoch: 1051/2000, Train Loss: 0.05117879435420036, Validation Loss: 0.089092917740345\n",
      "Epoch: 1052/2000, Train Loss: 0.051156170666217804, Validation Loss: 0.08944910019636154\n",
      "Epoch: 1053/2000, Train Loss: 0.05112973228096962, Validation Loss: 0.08905115723609924\n",
      "Epoch: 1054/2000, Train Loss: 0.05109989643096924, Validation Loss: 0.08946005254983902\n",
      "Epoch: 1055/2000, Train Loss: 0.05106199160218239, Validation Loss: 0.08901701867580414\n",
      "Epoch: 1056/2000, Train Loss: 0.05102026090025902, Validation Loss: 0.0894257053732872\n",
      "Epoch: 1057/2000, Train Loss: 0.05097071826457977, Validation Loss: 0.08898857980966568\n",
      "Epoch: 1058/2000, Train Loss: 0.050919100642204285, Validation Loss: 0.0893598198890686\n",
      "Epoch: 1059/2000, Train Loss: 0.050862547010183334, Validation Loss: 0.08896568417549133\n",
      "Epoch: 1060/2000, Train Loss: 0.05080614238977432, Validation Loss: 0.08927752822637558\n",
      "Epoch: 1061/2000, Train Loss: 0.05074816197156906, Validation Loss: 0.08894769102334976\n",
      "Epoch: 1062/2000, Train Loss: 0.050691910088062286, Validation Loss: 0.08919137716293335\n",
      "Epoch: 1063/2000, Train Loss: 0.05063634365797043, Validation Loss: 0.08893153816461563\n",
      "Epoch: 1064/2000, Train Loss: 0.05058297514915466, Validation Loss: 0.0891106128692627\n",
      "Epoch: 1065/2000, Train Loss: 0.0505310595035553, Validation Loss: 0.08891613036394119\n",
      "Epoch: 1066/2000, Train Loss: 0.050480980426073074, Validation Loss: 0.08904166519641876\n",
      "Epoch: 1067/2000, Train Loss: 0.050432220101356506, Validation Loss: 0.08890119940042496\n",
      "Epoch: 1068/2000, Train Loss: 0.05038468912243843, Validation Loss: 0.0889856219291687\n",
      "Epoch: 1069/2000, Train Loss: 0.050338033586740494, Validation Loss: 0.08888504654169083\n",
      "Epoch: 1070/2000, Train Loss: 0.05029207095503807, Validation Loss: 0.08893956243991852\n",
      "Epoch: 1071/2000, Train Loss: 0.05024658888578415, Validation Loss: 0.08886539191007614\n",
      "Epoch: 1072/2000, Train Loss: 0.05020149052143097, Validation Loss: 0.08889997750520706\n",
      "Epoch: 1073/2000, Train Loss: 0.05015664920210838, Validation Loss: 0.08884190768003464\n",
      "Epoch: 1074/2000, Train Loss: 0.05011199787259102, Validation Loss: 0.08886580914258957\n",
      "Epoch: 1075/2000, Train Loss: 0.0500674694776535, Validation Loss: 0.08881552517414093\n",
      "Epoch: 1076/2000, Train Loss: 0.050023045390844345, Validation Loss: 0.08883534371852875\n",
      "Epoch: 1077/2000, Train Loss: 0.04997871443629265, Validation Loss: 0.08878491818904877\n",
      "Epoch: 1078/2000, Train Loss: 0.049934469163417816, Validation Loss: 0.08880694210529327\n",
      "Epoch: 1079/2000, Train Loss: 0.049890317022800446, Validation Loss: 0.08875026553869247\n",
      "Epoch: 1080/2000, Train Loss: 0.049846287816762924, Validation Loss: 0.08878324180841446\n",
      "Epoch: 1081/2000, Train Loss: 0.049802426248788834, Validation Loss: 0.08871446549892426\n",
      "Epoch: 1082/2000, Train Loss: 0.04975879564881325, Validation Loss: 0.08876907080411911\n",
      "Epoch: 1083/2000, Train Loss: 0.049715518951416016, Validation Loss: 0.08867696672677994\n",
      "Epoch: 1084/2000, Train Loss: 0.04967283084988594, Validation Loss: 0.08876615017652512\n",
      "Epoch: 1085/2000, Train Loss: 0.04963104426860809, Validation Loss: 0.0886320099234581\n",
      "Epoch: 1086/2000, Train Loss: 0.04959079995751381, Validation Loss: 0.08878073841333389\n",
      "Epoch: 1087/2000, Train Loss: 0.04955298453569412, Validation Loss: 0.08857850730419159\n",
      "Epoch: 1088/2000, Train Loss: 0.04951956868171692, Validation Loss: 0.08883693069219589\n",
      "Epoch: 1089/2000, Train Loss: 0.049493059515953064, Validation Loss: 0.088528111577034\n",
      "Epoch: 1090/2000, Train Loss: 0.049479659646749496, Validation Loss: 0.0889919325709343\n",
      "Epoch: 1091/2000, Train Loss: 0.04948634281754494, Validation Loss: 0.08852431923151016\n",
      "Epoch: 1092/2000, Train Loss: 0.04953330010175705, Validation Loss: 0.08938711881637573\n",
      "Epoch: 1093/2000, Train Loss: 0.049637094140052795, Validation Loss: 0.0887262374162674\n",
      "Epoch: 1094/2000, Train Loss: 0.04986325278878212, Validation Loss: 0.09036698192358017\n",
      "Epoch: 1095/2000, Train Loss: 0.05022638291120529, Validation Loss: 0.08957535773515701\n",
      "Epoch: 1096/2000, Train Loss: 0.05091189220547676, Validation Loss: 0.09244777262210846\n",
      "Epoch: 1097/2000, Train Loss: 0.0517263300716877, Validation Loss: 0.09146251529455185\n",
      "Epoch: 1098/2000, Train Loss: 0.052984122186899185, Validation Loss: 0.09458567202091217\n",
      "Epoch: 1099/2000, Train Loss: 0.05336964502930641, Validation Loss: 0.09172791987657547\n",
      "Epoch: 1100/2000, Train Loss: 0.05320552736520767, Validation Loss: 0.09177679568529129\n",
      "Epoch: 1101/2000, Train Loss: 0.05108392611145973, Validation Loss: 0.08849651366472244\n",
      "Epoch: 1102/2000, Train Loss: 0.04925717040896416, Validation Loss: 0.08842385560274124\n",
      "Epoch: 1103/2000, Train Loss: 0.049069277942180634, Validation Loss: 0.09077761322259903\n",
      "Epoch: 1104/2000, Train Loss: 0.05021819844841957, Validation Loss: 0.08998892456293106\n",
      "Epoch: 1105/2000, Train Loss: 0.05109785869717598, Validation Loss: 0.09108927845954895\n",
      "Epoch: 1106/2000, Train Loss: 0.05033846199512482, Validation Loss: 0.08854853361845016\n",
      "Epoch: 1107/2000, Train Loss: 0.049118392169475555, Validation Loss: 0.0884479507803917\n",
      "Epoch: 1108/2000, Train Loss: 0.04869173467159271, Validation Loss: 0.08980534225702286\n",
      "Epoch: 1109/2000, Train Loss: 0.04930848255753517, Validation Loss: 0.08912112563848495\n",
      "Epoch: 1110/2000, Train Loss: 0.049901772290468216, Validation Loss: 0.09014338999986649\n",
      "Epoch: 1111/2000, Train Loss: 0.049498267471790314, Validation Loss: 0.0883411318063736\n",
      "Epoch: 1112/2000, Train Loss: 0.04873817414045334, Validation Loss: 0.08831551671028137\n",
      "Epoch: 1113/2000, Train Loss: 0.04847609996795654, Validation Loss: 0.08928616344928741\n",
      "Epoch: 1114/2000, Train Loss: 0.04885222762823105, Validation Loss: 0.088599793612957\n",
      "Epoch: 1115/2000, Train Loss: 0.04917217418551445, Validation Loss: 0.08941949158906937\n",
      "Epoch: 1116/2000, Train Loss: 0.048882681876420975, Validation Loss: 0.08820564299821854\n",
      "Epoch: 1117/2000, Train Loss: 0.048407845199108124, Validation Loss: 0.08822879195213318\n",
      "Epoch: 1118/2000, Train Loss: 0.04827788099646568, Validation Loss: 0.08899989724159241\n",
      "Epoch: 1119/2000, Train Loss: 0.04850844293832779, Validation Loss: 0.08835722506046295\n",
      "Epoch: 1120/2000, Train Loss: 0.04865390434861183, Validation Loss: 0.0890001654624939\n",
      "Epoch: 1121/2000, Train Loss: 0.0484350323677063, Validation Loss: 0.0881793275475502\n",
      "Epoch: 1122/2000, Train Loss: 0.048140592873096466, Validation Loss: 0.08816873282194138\n",
      "Epoch: 1123/2000, Train Loss: 0.04807651787996292, Validation Loss: 0.08873405307531357\n",
      "Epoch: 1124/2000, Train Loss: 0.048207152634859085, Validation Loss: 0.08814509212970734\n",
      "Epoch: 1125/2000, Train Loss: 0.048262543976306915, Validation Loss: 0.0886504277586937\n",
      "Epoch: 1126/2000, Train Loss: 0.04810566455125809, Validation Loss: 0.08807923644781113\n",
      "Epoch: 1127/2000, Train Loss: 0.04791560396552086, Validation Loss: 0.08807306736707687\n",
      "Epoch: 1128/2000, Train Loss: 0.047867875546216965, Validation Loss: 0.08849387615919113\n",
      "Epoch: 1129/2000, Train Loss: 0.04793104901909828, Validation Loss: 0.08801297843456268\n",
      "Epoch: 1130/2000, Train Loss: 0.04794462397694588, Validation Loss: 0.08842453360557556\n",
      "Epoch: 1131/2000, Train Loss: 0.04783494025468826, Validation Loss: 0.08800549060106277\n",
      "Epoch: 1132/2000, Train Loss: 0.04770534858107567, Validation Loss: 0.08800719678401947\n",
      "Epoch: 1133/2000, Train Loss: 0.04765830934047699, Validation Loss: 0.0883060097694397\n",
      "Epoch: 1134/2000, Train Loss: 0.047678638249635696, Validation Loss: 0.08793403208255768\n",
      "Epoch: 1135/2000, Train Loss: 0.04767478257417679, Validation Loss: 0.08828260749578476\n",
      "Epoch: 1136/2000, Train Loss: 0.04759927839040756, Validation Loss: 0.0879441499710083\n",
      "Epoch: 1137/2000, Train Loss: 0.047503914684057236, Validation Loss: 0.08796658366918564\n",
      "Epoch: 1138/2000, Train Loss: 0.04745117574930191, Validation Loss: 0.08814626932144165\n",
      "Epoch: 1139/2000, Train Loss: 0.04744335263967514, Validation Loss: 0.08786924928426743\n",
      "Epoch: 1140/2000, Train Loss: 0.047431156039237976, Validation Loss: 0.08816539496183395\n",
      "Epoch: 1141/2000, Train Loss: 0.04737906903028488, Validation Loss: 0.08787901699542999\n",
      "Epoch: 1142/2000, Train Loss: 0.04730618745088577, Validation Loss: 0.08793874830007553\n",
      "Epoch: 1143/2000, Train Loss: 0.04724982753396034, Validation Loss: 0.08801610767841339\n",
      "Epoch: 1144/2000, Train Loss: 0.04722273722290993, Validation Loss: 0.08782801032066345\n",
      "Epoch: 1145/2000, Train Loss: 0.047203246504068375, Validation Loss: 0.08807104080915451\n",
      "Epoch: 1146/2000, Train Loss: 0.047165509313344955, Validation Loss: 0.08782431483268738\n",
      "Epoch: 1147/2000, Train Loss: 0.04710955172777176, Validation Loss: 0.08792156726121902\n",
      "Epoch: 1148/2000, Train Loss: 0.047053951770067215, Validation Loss: 0.08790472149848938\n",
      "Epoch: 1149/2000, Train Loss: 0.047013960778713226, Validation Loss: 0.08779297769069672\n",
      "Epoch: 1150/2000, Train Loss: 0.04698565974831581, Validation Loss: 0.08796362578868866\n",
      "Epoch: 1151/2000, Train Loss: 0.04695412144064903, Validation Loss: 0.08775369077920914\n",
      "Epoch: 1152/2000, Train Loss: 0.0469110943377018, Validation Loss: 0.08787615597248077\n",
      "Epoch: 1153/2000, Train Loss: 0.04686105623841286, Validation Loss: 0.08778617531061172\n",
      "Epoch: 1154/2000, Train Loss: 0.04681485518813133, Validation Loss: 0.0877537801861763\n",
      "Epoch: 1155/2000, Train Loss: 0.04677740857005119, Validation Loss: 0.0878443717956543\n",
      "Epoch: 1156/2000, Train Loss: 0.046744395047426224, Validation Loss: 0.08769379556179047\n",
      "Epoch: 1157/2000, Train Loss: 0.04670839011669159, Validation Loss: 0.08781994879245758\n",
      "Epoch: 1158/2000, Train Loss: 0.04666614904999733, Validation Loss: 0.08769156038761139\n",
      "Epoch: 1159/2000, Train Loss: 0.04662097617983818, Validation Loss: 0.08772498369216919\n",
      "Epoch: 1160/2000, Train Loss: 0.04657803475856781, Validation Loss: 0.08772701025009155\n",
      "Epoch: 1161/2000, Train Loss: 0.04653957113623619, Validation Loss: 0.0876464694738388\n",
      "Epoch: 1162/2000, Train Loss: 0.04650354012846947, Validation Loss: 0.08773741126060486\n",
      "Epoch: 1163/2000, Train Loss: 0.04646624997258186, Validation Loss: 0.0876113772392273\n",
      "Epoch: 1164/2000, Train Loss: 0.04642591252923012, Validation Loss: 0.08768351376056671\n",
      "Epoch: 1165/2000, Train Loss: 0.046383537352085114, Validation Loss: 0.087612584233284\n",
      "Epoch: 1166/2000, Train Loss: 0.046341609209775925, Validation Loss: 0.08760446310043335\n",
      "Epoch: 1167/2000, Train Loss: 0.04630172252655029, Validation Loss: 0.08762585371732712\n",
      "Epoch: 1168/2000, Train Loss: 0.04626357555389404, Validation Loss: 0.08754600584506989\n",
      "Epoch: 1169/2000, Train Loss: 0.046225644648075104, Validation Loss: 0.08761357516050339\n",
      "Epoch: 1170/2000, Train Loss: 0.04618655890226364, Validation Loss: 0.08751919120550156\n",
      "Epoch: 1171/2000, Train Loss: 0.046146050095558167, Validation Loss: 0.08756737411022186\n",
      "Epoch: 1172/2000, Train Loss: 0.046104852110147476, Validation Loss: 0.08751717954874039\n",
      "Epoch: 1173/2000, Train Loss: 0.046063974499702454, Validation Loss: 0.08751143515110016\n",
      "Epoch: 1174/2000, Train Loss: 0.04602396860718727, Validation Loss: 0.08752141147851944\n",
      "Epoch: 1175/2000, Train Loss: 0.045984696596860886, Validation Loss: 0.087466761469841\n",
      "Epoch: 1176/2000, Train Loss: 0.04594562575221062, Validation Loss: 0.08751264959573746\n",
      "Epoch: 1177/2000, Train Loss: 0.0459061935544014, Validation Loss: 0.0874403715133667\n",
      "Epoch: 1178/2000, Train Loss: 0.04586618393659592, Validation Loss: 0.08748707175254822\n",
      "Epoch: 1179/2000, Train Loss: 0.045825663954019547, Validation Loss: 0.0874289870262146\n",
      "Epoch: 1180/2000, Train Loss: 0.04578491672873497, Validation Loss: 0.08745196461677551\n",
      "Epoch: 1181/2000, Train Loss: 0.045744214206933975, Validation Loss: 0.08742455393075943\n",
      "Epoch: 1182/2000, Train Loss: 0.045703716576099396, Validation Loss: 0.08741733431816101\n",
      "Epoch: 1183/2000, Train Loss: 0.045663464814424515, Validation Loss: 0.08742111921310425\n",
      "Epoch: 1184/2000, Train Loss: 0.04562339186668396, Validation Loss: 0.08738754689693451\n",
      "Epoch: 1185/2000, Train Loss: 0.045583415776491165, Validation Loss: 0.08741333335638046\n",
      "Epoch: 1186/2000, Train Loss: 0.045543476939201355, Validation Loss: 0.08736056834459305\n",
      "Epoch: 1187/2000, Train Loss: 0.04550352692604065, Validation Loss: 0.0874011218547821\n",
      "Epoch: 1188/2000, Train Loss: 0.04546358063817024, Validation Loss: 0.08733399957418442\n",
      "Epoch: 1189/2000, Train Loss: 0.045423686504364014, Validation Loss: 0.08738761395215988\n",
      "Epoch: 1190/2000, Train Loss: 0.04538394510746002, Validation Loss: 0.08730410784482956\n",
      "Epoch: 1191/2000, Train Loss: 0.04534444212913513, Validation Loss: 0.08737904578447342\n",
      "Epoch: 1192/2000, Train Loss: 0.045305460691452026, Validation Loss: 0.08726947754621506\n",
      "Epoch: 1193/2000, Train Loss: 0.04526720568537712, Validation Loss: 0.08738330751657486\n",
      "Epoch: 1194/2000, Train Loss: 0.045230600982904434, Validation Loss: 0.08722627907991409\n",
      "Epoch: 1195/2000, Train Loss: 0.045195892453193665, Validation Loss: 0.08741259574890137\n",
      "Epoch: 1196/2000, Train Loss: 0.04516526311635971, Validation Loss: 0.08718035370111465\n",
      "Epoch: 1197/2000, Train Loss: 0.045138150453567505, Validation Loss: 0.08747781813144684\n",
      "Epoch: 1198/2000, Train Loss: 0.045117832720279694, Validation Loss: 0.08714333176612854\n",
      "Epoch: 1199/2000, Train Loss: 0.045100145041942596, Validation Loss: 0.08756238222122192\n",
      "Epoch: 1200/2000, Train Loss: 0.04508553817868233, Validation Loss: 0.08712218701839447\n",
      "Epoch: 1201/2000, Train Loss: 0.045066915452480316, Validation Loss: 0.08761025965213776\n",
      "Epoch: 1202/2000, Train Loss: 0.04504098370671272, Validation Loss: 0.08710043877363205\n",
      "Epoch: 1203/2000, Train Loss: 0.04500391706824303, Validation Loss: 0.0875672847032547\n",
      "Epoch: 1204/2000, Train Loss: 0.04495280981063843, Validation Loss: 0.08707129210233688\n",
      "Epoch: 1205/2000, Train Loss: 0.04489203169941902, Validation Loss: 0.0874362587928772\n",
      "Epoch: 1206/2000, Train Loss: 0.04482379928231239, Validation Loss: 0.08705392479896545\n",
      "Epoch: 1207/2000, Train Loss: 0.044755835086107254, Validation Loss: 0.08727481216192245\n",
      "Epoch: 1208/2000, Train Loss: 0.04469194635748863, Validation Loss: 0.08706976473331451\n",
      "Epoch: 1209/2000, Train Loss: 0.0446358323097229, Validation Loss: 0.08714032173156738\n",
      "Epoch: 1210/2000, Train Loss: 0.04458809643983841, Validation Loss: 0.08711642771959305\n",
      "Epoch: 1211/2000, Train Loss: 0.04454787075519562, Validation Loss: 0.08705129474401474\n",
      "Epoch: 1212/2000, Train Loss: 0.044513311237096786, Validation Loss: 0.08717235922813416\n",
      "Epoch: 1213/2000, Train Loss: 0.04448242485523224, Validation Loss: 0.08699574321508408\n",
      "Epoch: 1214/2000, Train Loss: 0.044453613460063934, Validation Loss: 0.08722089976072311\n",
      "Epoch: 1215/2000, Train Loss: 0.04442558065056801, Validation Loss: 0.08695923537015915\n",
      "Epoch: 1216/2000, Train Loss: 0.044397879391908646, Validation Loss: 0.0872592180967331\n",
      "Epoch: 1217/2000, Train Loss: 0.04436967894434929, Validation Loss: 0.08693384379148483\n",
      "Epoch: 1218/2000, Train Loss: 0.04434139281511307, Validation Loss: 0.08728838711977005\n",
      "Epoch: 1219/2000, Train Loss: 0.04431192949414253, Validation Loss: 0.0869140625\n",
      "Epoch: 1220/2000, Train Loss: 0.04428233578801155, Validation Loss: 0.08730758726596832\n",
      "Epoch: 1221/2000, Train Loss: 0.04425076022744179, Validation Loss: 0.08689709007740021\n",
      "Epoch: 1222/2000, Train Loss: 0.044218920171260834, Validation Loss: 0.08731429278850555\n",
      "Epoch: 1223/2000, Train Loss: 0.04418417438864708, Validation Loss: 0.08688090741634369\n",
      "Epoch: 1224/2000, Train Loss: 0.04414905607700348, Validation Loss: 0.08730505406856537\n",
      "Epoch: 1225/2000, Train Loss: 0.04411042481660843, Validation Loss: 0.08686328679323196\n",
      "Epoch: 1226/2000, Train Loss: 0.0440714992582798, Validation Loss: 0.0872785896062851\n",
      "Epoch: 1227/2000, Train Loss: 0.04402903839945793, Validation Loss: 0.08684457093477249\n",
      "Epoch: 1228/2000, Train Loss: 0.04398660734295845, Validation Loss: 0.08723786473274231\n",
      "Epoch: 1229/2000, Train Loss: 0.04394131526350975, Validation Loss: 0.08682636171579361\n",
      "Epoch: 1230/2000, Train Loss: 0.04389661177992821, Validation Loss: 0.08718844503164291\n",
      "Epoch: 1231/2000, Train Loss: 0.04385014623403549, Validation Loss: 0.08680970966815948\n",
      "Epoch: 1232/2000, Train Loss: 0.043804820626974106, Validation Loss: 0.0871371179819107\n",
      "Epoch: 1233/2000, Train Loss: 0.0437588095664978, Validation Loss: 0.08679544180631638\n",
      "Epoch: 1234/2000, Train Loss: 0.04371429979801178, Validation Loss: 0.08709022402763367\n",
      "Epoch: 1235/2000, Train Loss: 0.04366987198591232, Validation Loss: 0.08678290247917175\n",
      "Epoch: 1236/2000, Train Loss: 0.0436270609498024, Validation Loss: 0.0870516300201416\n",
      "Epoch: 1237/2000, Train Loss: 0.043584756553173065, Validation Loss: 0.08676997572183609\n",
      "Epoch: 1238/2000, Train Loss: 0.04354405030608177, Validation Loss: 0.08702369034290314\n",
      "Epoch: 1239/2000, Train Loss: 0.04350404813885689, Validation Loss: 0.08675510436296463\n",
      "Epoch: 1240/2000, Train Loss: 0.04346567764878273, Validation Loss: 0.08700882643461227\n",
      "Epoch: 1241/2000, Train Loss: 0.043428197503089905, Validation Loss: 0.08673754334449768\n",
      "Epoch: 1242/2000, Train Loss: 0.04339262470602989, Validation Loss: 0.08700992912054062\n",
      "Epoch: 1243/2000, Train Loss: 0.043358348309993744, Validation Loss: 0.08671725541353226\n",
      "Epoch: 1244/2000, Train Loss: 0.04332678020000458, Validation Loss: 0.08703204989433289\n",
      "Epoch: 1245/2000, Train Loss: 0.04329735040664673, Validation Loss: 0.08669654279947281\n",
      "Epoch: 1246/2000, Train Loss: 0.043272390961647034, Validation Loss: 0.08708501607179642\n",
      "Epoch: 1247/2000, Train Loss: 0.0432511605322361, Validation Loss: 0.08668173849582672\n",
      "Epoch: 1248/2000, Train Loss: 0.043237850069999695, Validation Loss: 0.08718468993902206\n",
      "Epoch: 1249/2000, Train Loss: 0.04323077201843262, Validation Loss: 0.08668524026870728\n",
      "Epoch: 1250/2000, Train Loss: 0.04323768988251686, Validation Loss: 0.08735288679599762\n",
      "Epoch: 1251/2000, Train Loss: 0.043253254145383835, Validation Loss: 0.08672752231359482\n",
      "Epoch: 1252/2000, Train Loss: 0.04329153150320053, Validation Loss: 0.08760710060596466\n",
      "Epoch: 1253/2000, Train Loss: 0.04333551973104477, Validation Loss: 0.08682670444250107\n",
      "Epoch: 1254/2000, Train Loss: 0.04340919107198715, Validation Loss: 0.08792155981063843\n",
      "Epoch: 1255/2000, Train Loss: 0.04346686601638794, Validation Loss: 0.08695820719003677\n",
      "Epoch: 1256/2000, Train Loss: 0.043546147644519806, Validation Loss: 0.08815886080265045\n",
      "Epoch: 1257/2000, Train Loss: 0.04355601966381073, Validation Loss: 0.08700548112392426\n",
      "Epoch: 1258/2000, Train Loss: 0.04355372115969658, Validation Loss: 0.08807049691677094\n",
      "Epoch: 1259/2000, Train Loss: 0.043429452925920486, Validation Loss: 0.0868447944521904\n",
      "Epoch: 1260/2000, Train Loss: 0.04327020421624184, Validation Loss: 0.08755304664373398\n",
      "Epoch: 1261/2000, Train Loss: 0.043031610548496246, Validation Loss: 0.0866154134273529\n",
      "Epoch: 1262/2000, Train Loss: 0.04281039535999298, Validation Loss: 0.08693082630634308\n",
      "Epoch: 1263/2000, Train Loss: 0.042631398886442184, Validation Loss: 0.0866474136710167\n",
      "Epoch: 1264/2000, Train Loss: 0.042532242834568024, Validation Loss: 0.08660653978586197\n",
      "Epoch: 1265/2000, Train Loss: 0.04250940680503845, Validation Loss: 0.08695606887340546\n",
      "Epoch: 1266/2000, Train Loss: 0.042538080364465714, Validation Loss: 0.08657284080982208\n",
      "Epoch: 1267/2000, Train Loss: 0.042587727308273315, Validation Loss: 0.08725392818450928\n",
      "Epoch: 1268/2000, Train Loss: 0.04262262582778931, Validation Loss: 0.0866021141409874\n",
      "Epoch: 1269/2000, Train Loss: 0.04263525456190109, Validation Loss: 0.08731935918331146\n",
      "Epoch: 1270/2000, Train Loss: 0.04259814694523811, Validation Loss: 0.08656647801399231\n",
      "Epoch: 1271/2000, Train Loss: 0.042533330619335175, Validation Loss: 0.08713077753782272\n",
      "Epoch: 1272/2000, Train Loss: 0.04243231192231178, Validation Loss: 0.08650343120098114\n",
      "Epoch: 1273/2000, Train Loss: 0.042326997965574265, Validation Loss: 0.08683528006076813\n",
      "Epoch: 1274/2000, Train Loss: 0.04222571849822998, Validation Loss: 0.08651895821094513\n",
      "Epoch: 1275/2000, Train Loss: 0.04214596375823021, Validation Loss: 0.08660563826560974\n",
      "Epoch: 1276/2000, Train Loss: 0.04209137335419655, Validation Loss: 0.08663921058177948\n",
      "Epoch: 1277/2000, Train Loss: 0.04205985739827156, Validation Loss: 0.0864967480301857\n",
      "Epoch: 1278/2000, Train Loss: 0.042044177651405334, Validation Loss: 0.08678761124610901\n",
      "Epoch: 1279/2000, Train Loss: 0.0420350506901741, Validation Loss: 0.08646045625209808\n",
      "Epoch: 1280/2000, Train Loss: 0.04202576354146004, Validation Loss: 0.08687832951545715\n",
      "Epoch: 1281/2000, Train Loss: 0.04200838506221771, Validation Loss: 0.08644061535596848\n",
      "Epoch: 1282/2000, Train Loss: 0.04198359325528145, Validation Loss: 0.08687933534383774\n",
      "Epoch: 1283/2000, Train Loss: 0.04194633662700653, Validation Loss: 0.08641878515481949\n",
      "Epoch: 1284/2000, Train Loss: 0.04190271347761154, Validation Loss: 0.08680817484855652\n",
      "Epoch: 1285/2000, Train Loss: 0.04185052588582039, Validation Loss: 0.08640382438898087\n",
      "Epoch: 1286/2000, Train Loss: 0.04179658368229866, Validation Loss: 0.08670254796743393\n",
      "Epoch: 1287/2000, Train Loss: 0.04174088314175606, Validation Loss: 0.08640748262405396\n",
      "Epoch: 1288/2000, Train Loss: 0.0416877456009388, Validation Loss: 0.08659645169973373\n",
      "Epoch: 1289/2000, Train Loss: 0.04163765534758568, Validation Loss: 0.08643089979887009\n",
      "Epoch: 1290/2000, Train Loss: 0.04159192368388176, Validation Loss: 0.0865086168050766\n",
      "Epoch: 1291/2000, Train Loss: 0.041550323367118835, Validation Loss: 0.08646506816148758\n",
      "Epoch: 1292/2000, Train Loss: 0.04151236638426781, Validation Loss: 0.08644244074821472\n",
      "Epoch: 1293/2000, Train Loss: 0.04147724434733391, Validation Loss: 0.08649939298629761\n",
      "Epoch: 1294/2000, Train Loss: 0.04144413769245148, Validation Loss: 0.08639378100633621\n",
      "Epoch: 1295/2000, Train Loss: 0.0414123609662056, Validation Loss: 0.08652902394533157\n",
      "Epoch: 1296/2000, Train Loss: 0.0413813441991806, Validation Loss: 0.08635812997817993\n",
      "Epoch: 1297/2000, Train Loss: 0.04135090485215187, Validation Loss: 0.08655509352684021\n",
      "Epoch: 1298/2000, Train Loss: 0.04132063314318657, Validation Loss: 0.08633162826299667\n",
      "Epoch: 1299/2000, Train Loss: 0.04129093885421753, Validation Loss: 0.08658108860254288\n",
      "Epoch: 1300/2000, Train Loss: 0.04126136004924774, Validation Loss: 0.0863104984164238\n",
      "Epoch: 1301/2000, Train Loss: 0.041232943534851074, Validation Loss: 0.08661144971847534\n",
      "Epoch: 1302/2000, Train Loss: 0.04120497778058052, Validation Loss: 0.08629196882247925\n",
      "Epoch: 1303/2000, Train Loss: 0.04117930680513382, Validation Loss: 0.08665252476930618\n",
      "Epoch: 1304/2000, Train Loss: 0.04115474224090576, Validation Loss: 0.08627563714981079\n",
      "Epoch: 1305/2000, Train Loss: 0.04113434627652168, Validation Loss: 0.08671359717845917\n",
      "Epoch: 1306/2000, Train Loss: 0.041116081178188324, Validation Loss: 0.08626490831375122\n",
      "Epoch: 1307/2000, Train Loss: 0.04110512509942055, Validation Loss: 0.08680874109268188\n",
      "Epoch: 1308/2000, Train Loss: 0.041097674518823624, Validation Loss: 0.08626940101385117\n",
      "Epoch: 1309/2000, Train Loss: 0.04110276326537132, Validation Loss: 0.08695755898952484\n",
      "Epoch: 1310/2000, Train Loss: 0.04111245274543762, Validation Loss: 0.0863054022192955\n",
      "Epoch: 1311/2000, Train Loss: 0.041142549365758896, Validation Loss: 0.08717909455299377\n",
      "Epoch: 1312/2000, Train Loss: 0.041175343096256256, Validation Loss: 0.08639021962881088\n",
      "Epoch: 1313/2000, Train Loss: 0.041237615048885345, Validation Loss: 0.08747094869613647\n",
      "Epoch: 1314/2000, Train Loss: 0.041289955377578735, Validation Loss: 0.08652115613222122\n",
      "Epoch: 1315/2000, Train Loss: 0.04137463495135307, Validation Loss: 0.08776261657476425\n",
      "Epoch: 1316/2000, Train Loss: 0.04141338914632797, Validation Loss: 0.08663228154182434\n",
      "Epoch: 1317/2000, Train Loss: 0.04146689921617508, Validation Loss: 0.08786632865667343\n",
      "Epoch: 1318/2000, Train Loss: 0.041417159140110016, Validation Loss: 0.08658646792173386\n",
      "Epoch: 1319/2000, Train Loss: 0.041346631944179535, Validation Loss: 0.08756440132856369\n",
      "Epoch: 1320/2000, Train Loss: 0.04115457087755203, Validation Loss: 0.08634834736585617\n",
      "Epoch: 1321/2000, Train Loss: 0.04094434529542923, Validation Loss: 0.08692522346973419\n",
      "Epoch: 1322/2000, Train Loss: 0.04070299491286278, Validation Loss: 0.08618279546499252\n",
      "Epoch: 1323/2000, Train Loss: 0.04051174223423004, Validation Loss: 0.08637450635433197\n",
      "Epoch: 1324/2000, Train Loss: 0.040391601622104645, Validation Loss: 0.08634362369775772\n",
      "Epoch: 1325/2000, Train Loss: 0.040350962430238724, Validation Loss: 0.08617469668388367\n",
      "Epoch: 1326/2000, Train Loss: 0.040370237082242966, Validation Loss: 0.0866929218173027\n",
      "Epoch: 1327/2000, Train Loss: 0.0404171422123909, Validation Loss: 0.086190365254879\n",
      "Epoch: 1328/2000, Train Loss: 0.04046674072742462, Validation Loss: 0.08693845570087433\n",
      "Epoch: 1329/2000, Train Loss: 0.04048576205968857, Validation Loss: 0.08620882034301758\n",
      "Epoch: 1330/2000, Train Loss: 0.0404786579310894, Validation Loss: 0.08692721277475357\n",
      "Epoch: 1331/2000, Train Loss: 0.04042147099971771, Validation Loss: 0.08615811169147491\n",
      "Epoch: 1332/2000, Train Loss: 0.04034220799803734, Validation Loss: 0.08669336140155792\n",
      "Epoch: 1333/2000, Train Loss: 0.04023626446723938, Validation Loss: 0.08610646426677704\n",
      "Epoch: 1334/2000, Train Loss: 0.0401335135102272, Validation Loss: 0.08639966696500778\n",
      "Epoch: 1335/2000, Train Loss: 0.040041957050561905, Validation Loss: 0.08614862710237503\n",
      "Epoch: 1336/2000, Train Loss: 0.03997403010725975, Validation Loss: 0.08619411289691925\n",
      "Epoch: 1337/2000, Train Loss: 0.039930641651153564, Validation Loss: 0.08628440648317337\n",
      "Epoch: 1338/2000, Train Loss: 0.03990742936730385, Validation Loss: 0.08610352873802185\n",
      "Epoch: 1339/2000, Train Loss: 0.03989659622311592, Validation Loss: 0.08642949908971786\n",
      "Epoch: 1340/2000, Train Loss: 0.039889149367809296, Validation Loss: 0.08607348054647446\n",
      "Epoch: 1341/2000, Train Loss: 0.039879728108644485, Validation Loss: 0.08650817722082138\n",
      "Epoch: 1342/2000, Train Loss: 0.039861127734184265, Validation Loss: 0.0860554650425911\n",
      "Epoch: 1343/2000, Train Loss: 0.03983519971370697, Validation Loss: 0.08650011569261551\n",
      "Epoch: 1344/2000, Train Loss: 0.039797376841306686, Validation Loss: 0.08603796362876892\n",
      "Epoch: 1345/2000, Train Loss: 0.039753980934619904, Validation Loss: 0.08642706274986267\n",
      "Epoch: 1346/2000, Train Loss: 0.03970310091972351, Validation Loss: 0.08603022247552872\n",
      "Epoch: 1347/2000, Train Loss: 0.03965115174651146, Validation Loss: 0.08632455766201019\n",
      "Epoch: 1348/2000, Train Loss: 0.03959818184375763, Validation Loss: 0.0860409289598465\n",
      "Epoch: 1349/2000, Train Loss: 0.039548035711050034, Validation Loss: 0.0862232968211174\n",
      "Epoch: 1350/2000, Train Loss: 0.03950110450387001, Validation Loss: 0.08606929332017899\n",
      "Epoch: 1351/2000, Train Loss: 0.03945840150117874, Validation Loss: 0.08614004403352737\n",
      "Epoch: 1352/2000, Train Loss: 0.03941961005330086, Validation Loss: 0.08610635250806808\n",
      "Epoch: 1353/2000, Train Loss: 0.03938419371843338, Validation Loss: 0.08607767522335052\n",
      "Epoch: 1354/2000, Train Loss: 0.03935137391090393, Validation Loss: 0.08614268898963928\n",
      "Epoch: 1355/2000, Train Loss: 0.03932035714387894, Validation Loss: 0.08603282272815704\n",
      "Epoch: 1356/2000, Train Loss: 0.039290525019168854, Validation Loss: 0.08617475628852844\n",
      "Epoch: 1357/2000, Train Loss: 0.03926132246851921, Validation Loss: 0.08600165694952011\n",
      "Epoch: 1358/2000, Train Loss: 0.03923260420560837, Validation Loss: 0.08620360493659973\n",
      "Epoch: 1359/2000, Train Loss: 0.0392039529979229, Validation Loss: 0.08597982674837112\n",
      "Epoch: 1360/2000, Train Loss: 0.03917577117681503, Validation Loss: 0.08623146265745163\n",
      "Epoch: 1361/2000, Train Loss: 0.03914758563041687, Validation Loss: 0.0859626978635788\n",
      "Epoch: 1362/2000, Train Loss: 0.03912036493420601, Validation Loss: 0.08626174926757812\n",
      "Epoch: 1363/2000, Train Loss: 0.03909340873360634, Validation Loss: 0.08594708889722824\n",
      "Epoch: 1364/2000, Train Loss: 0.03906835988163948, Validation Loss: 0.08629989624023438\n",
      "Epoch: 1365/2000, Train Loss: 0.0390441007912159, Validation Loss: 0.08593229204416275\n",
      "Epoch: 1366/2000, Train Loss: 0.03902328386902809, Validation Loss: 0.08635422587394714\n",
      "Epoch: 1367/2000, Train Loss: 0.03900407999753952, Validation Loss: 0.08592115342617035\n",
      "Epoch: 1368/2000, Train Loss: 0.03899086266756058, Validation Loss: 0.0864376649260521\n",
      "Epoch: 1369/2000, Train Loss: 0.03898043930530548, Validation Loss: 0.08592189103364944\n",
      "Epoch: 1370/2000, Train Loss: 0.03898026794195175, Validation Loss: 0.08656829595565796\n",
      "Epoch: 1371/2000, Train Loss: 0.03898414969444275, Validation Loss: 0.08594824373722076\n",
      "Epoch: 1372/2000, Train Loss: 0.039005097001791, Validation Loss: 0.08676620572805405\n",
      "Epoch: 1373/2000, Train Loss: 0.03903000056743622, Validation Loss: 0.08601753413677216\n",
      "Epoch: 1374/2000, Train Loss: 0.03908136859536171, Validation Loss: 0.08704293519258499\n",
      "Epoch: 1375/2000, Train Loss: 0.03913046419620514, Validation Loss: 0.08614107966423035\n",
      "Epoch: 1376/2000, Train Loss: 0.039214473217725754, Validation Loss: 0.08737090229988098\n",
      "Epoch: 1377/2000, Train Loss: 0.039273083209991455, Validation Loss: 0.08629176020622253\n",
      "Epoch: 1378/2000, Train Loss: 0.039362408220767975, Validation Loss: 0.08762407302856445\n",
      "Epoch: 1379/2000, Train Loss: 0.03937521204352379, Validation Loss: 0.08635804057121277\n",
      "Epoch: 1380/2000, Train Loss: 0.039388563483953476, Validation Loss: 0.08756104111671448\n",
      "Epoch: 1381/2000, Train Loss: 0.039270706474781036, Validation Loss: 0.08620350807905197\n",
      "Epoch: 1382/2000, Train Loss: 0.039123035967350006, Validation Loss: 0.08703853189945221\n",
      "Epoch: 1383/2000, Train Loss: 0.03887491673231125, Validation Loss: 0.08593204617500305\n",
      "Epoch: 1384/2000, Train Loss: 0.03863651305437088, Validation Loss: 0.08634010702371597\n",
      "Epoch: 1385/2000, Train Loss: 0.03842576965689659, Validation Loss: 0.0859055146574974\n",
      "Epoch: 1386/2000, Train Loss: 0.03829650208353996, Validation Loss: 0.08592895418405533\n",
      "Epoch: 1387/2000, Train Loss: 0.03825308382511139, Validation Loss: 0.08621681481599808\n",
      "Epoch: 1388/2000, Train Loss: 0.038275886327028275, Validation Loss: 0.0858682170510292\n",
      "Epoch: 1389/2000, Train Loss: 0.03833267465233803, Validation Loss: 0.08657625317573547\n",
      "Epoch: 1390/2000, Train Loss: 0.03838479891419411, Validation Loss: 0.08591564744710922\n",
      "Epoch: 1391/2000, Train Loss: 0.0384189747273922, Validation Loss: 0.08671160787343979\n",
      "Epoch: 1392/2000, Train Loss: 0.03840257227420807, Validation Loss: 0.0858992487192154\n",
      "Epoch: 1393/2000, Train Loss: 0.03835561126470566, Validation Loss: 0.08655931800603867\n",
      "Epoch: 1394/2000, Train Loss: 0.03826257213950157, Validation Loss: 0.08583029359579086\n",
      "Epoch: 1395/2000, Train Loss: 0.03815857693552971, Validation Loss: 0.08624923974275589\n",
      "Epoch: 1396/2000, Train Loss: 0.03804952651262283, Validation Loss: 0.085825115442276\n",
      "Epoch: 1397/2000, Train Loss: 0.037959419190883636, Validation Loss: 0.08597869426012039\n",
      "Epoch: 1398/2000, Train Loss: 0.037894949316978455, Validation Loss: 0.08594222366809845\n",
      "Epoch: 1399/2000, Train Loss: 0.03785775229334831, Validation Loss: 0.08584167063236237\n",
      "Epoch: 1400/2000, Train Loss: 0.03784165158867836, Validation Loss: 0.0861140638589859\n",
      "Epoch: 1401/2000, Train Loss: 0.037836797535419464, Validation Loss: 0.0858006700873375\n",
      "Epoch: 1402/2000, Train Loss: 0.03783438727259636, Validation Loss: 0.08623512834310532\n",
      "Epoch: 1403/2000, Train Loss: 0.03782481327652931, Validation Loss: 0.08578670769929886\n",
      "Epoch: 1404/2000, Train Loss: 0.03780704364180565, Validation Loss: 0.08625373244285583\n",
      "Epoch: 1405/2000, Train Loss: 0.0377751924097538, Validation Loss: 0.08577117323875427\n",
      "Epoch: 1406/2000, Train Loss: 0.037735357880592346, Validation Loss: 0.0861840471625328\n",
      "Epoch: 1407/2000, Train Loss: 0.037685517221689224, Validation Loss: 0.0857636108994484\n",
      "Epoch: 1408/2000, Train Loss: 0.03763330727815628, Validation Loss: 0.08607170730829239\n",
      "Epoch: 1409/2000, Train Loss: 0.03757940232753754, Validation Loss: 0.08577962219715118\n",
      "Epoch: 1410/2000, Train Loss: 0.03752869740128517, Validation Loss: 0.0859595388174057\n",
      "Epoch: 1411/2000, Train Loss: 0.03748215362429619, Validation Loss: 0.08582056313753128\n",
      "Epoch: 1412/2000, Train Loss: 0.037441037595272064, Validation Loss: 0.08587100356817245\n",
      "Epoch: 1413/2000, Train Loss: 0.03740503266453743, Validation Loss: 0.08587426692247391\n",
      "Epoch: 1414/2000, Train Loss: 0.03737330064177513, Validation Loss: 0.08580999821424484\n",
      "Epoch: 1415/2000, Train Loss: 0.03734464943408966, Validation Loss: 0.08592559397220612\n",
      "Epoch: 1416/2000, Train Loss: 0.037317853420972824, Validation Loss: 0.08577033877372742\n",
      "Epoch: 1417/2000, Train Loss: 0.037291985005140305, Validation Loss: 0.08596614003181458\n",
      "Epoch: 1418/2000, Train Loss: 0.03726610913872719, Validation Loss: 0.08574577420949936\n",
      "Epoch: 1419/2000, Train Loss: 0.03724009916186333, Validation Loss: 0.08599566668272018\n",
      "Epoch: 1420/2000, Train Loss: 0.03721319139003754, Validation Loss: 0.08573178201913834\n",
      "Epoch: 1421/2000, Train Loss: 0.037186019122600555, Validation Loss: 0.08601678162813187\n",
      "Epoch: 1422/2000, Train Loss: 0.03715776652097702, Validation Loss: 0.08572346717119217\n",
      "Epoch: 1423/2000, Train Loss: 0.03712961822748184, Validation Loss: 0.0860319659113884\n",
      "Epoch: 1424/2000, Train Loss: 0.037100568413734436, Validation Loss: 0.08571625500917435\n",
      "Epoch: 1425/2000, Train Loss: 0.03707217797636986, Validation Loss: 0.08604458719491959\n",
      "Epoch: 1426/2000, Train Loss: 0.03704320266842842, Validation Loss: 0.08570776134729385\n",
      "Epoch: 1427/2000, Train Loss: 0.03701552748680115, Validation Loss: 0.08605927973985672\n",
      "Epoch: 1428/2000, Train Loss: 0.03698763996362686, Validation Loss: 0.08569753170013428\n",
      "Epoch: 1429/2000, Train Loss: 0.036961838603019714, Validation Loss: 0.08608201146125793\n",
      "Epoch: 1430/2000, Train Loss: 0.03693624958395958, Validation Loss: 0.08568723499774933\n",
      "Epoch: 1431/2000, Train Loss: 0.03691384196281433, Validation Loss: 0.08612063527107239\n",
      "Epoch: 1432/2000, Train Loss: 0.036892157047986984, Validation Loss: 0.08568057417869568\n",
      "Epoch: 1433/2000, Train Loss: 0.03687535971403122, Validation Loss: 0.08618448674678802\n",
      "Epoch: 1434/2000, Train Loss: 0.036859914660453796, Validation Loss: 0.08568231761455536\n",
      "Epoch: 1435/2000, Train Loss: 0.03685208037495613, Validation Loss: 0.08628340065479279\n",
      "Epoch: 1436/2000, Train Loss: 0.03684622794389725, Validation Loss: 0.08569861203432083\n",
      "Epoch: 1437/2000, Train Loss: 0.036852262914180756, Validation Loss: 0.0864286720752716\n",
      "Epoch: 1438/2000, Train Loss: 0.036860305815935135, Validation Loss: 0.08573934435844421\n",
      "Epoch: 1439/2000, Train Loss: 0.03688647970557213, Validation Loss: 0.08663099259138107\n",
      "Epoch: 1440/2000, Train Loss: 0.036912087351083755, Validation Loss: 0.08581596612930298\n",
      "Epoch: 1441/2000, Train Loss: 0.03696304187178612, Validation Loss: 0.08688714355230331\n",
      "Epoch: 1442/2000, Train Loss: 0.037003204226493835, Validation Loss: 0.0859275534749031\n",
      "Epoch: 1443/2000, Train Loss: 0.03707210347056389, Validation Loss: 0.08714782446622849\n",
      "Epoch: 1444/2000, Train Loss: 0.0371042899787426, Validation Loss: 0.08603175729513168\n",
      "Epoch: 1445/2000, Train Loss: 0.03715530037879944, Validation Loss: 0.08728066086769104\n",
      "Epoch: 1446/2000, Train Loss: 0.03712758794426918, Validation Loss: 0.08603133261203766\n",
      "Epoch: 1447/2000, Train Loss: 0.03709321469068527, Validation Loss: 0.08710737526416779\n",
      "Epoch: 1448/2000, Train Loss: 0.0369553305208683, Validation Loss: 0.08586619049310684\n",
      "Epoch: 1449/2000, Train Loss: 0.03679990395903587, Validation Loss: 0.08660277724266052\n",
      "Epoch: 1450/2000, Train Loss: 0.036588650196790695, Validation Loss: 0.08567921072244644\n",
      "Epoch: 1451/2000, Train Loss: 0.036398500204086304, Validation Loss: 0.08604046702384949\n",
      "Epoch: 1452/2000, Train Loss: 0.03624241054058075, Validation Loss: 0.08571324497461319\n",
      "Epoch: 1453/2000, Train Loss: 0.03614848852157593, Validation Loss: 0.08572085946798325\n",
      "Epoch: 1454/2000, Train Loss: 0.03611483424901962, Validation Loss: 0.08597838878631592\n",
      "Epoch: 1455/2000, Train Loss: 0.0361260324716568, Validation Loss: 0.0856541246175766\n",
      "Epoch: 1456/2000, Train Loss: 0.03616100177168846, Validation Loss: 0.08626939356327057\n",
      "Epoch: 1457/2000, Train Loss: 0.03619525209069252, Validation Loss: 0.08567880839109421\n",
      "Epoch: 1458/2000, Train Loss: 0.03622045740485191, Validation Loss: 0.08641263842582703\n",
      "Epoch: 1459/2000, Train Loss: 0.03621431067585945, Validation Loss: 0.08567949384450912\n",
      "Epoch: 1460/2000, Train Loss: 0.03618905693292618, Validation Loss: 0.086356021463871\n",
      "Epoch: 1461/2000, Train Loss: 0.03612874448299408, Validation Loss: 0.08564163744449615\n",
      "Epoch: 1462/2000, Train Loss: 0.03605645149946213, Validation Loss: 0.08615315705537796\n",
      "Epoch: 1463/2000, Train Loss: 0.035968612879514694, Validation Loss: 0.08561912178993225\n",
      "Epoch: 1464/2000, Train Loss: 0.035885248333215714, Validation Loss: 0.08591772615909576\n",
      "Epoch: 1465/2000, Train Loss: 0.03581026941537857, Validation Loss: 0.08566535264253616\n",
      "Epoch: 1466/2000, Train Loss: 0.0357515811920166, Validation Loss: 0.08574359118938446\n",
      "Epoch: 1467/2000, Train Loss: 0.03570965304970741, Validation Loss: 0.08577500283718109\n",
      "Epoch: 1468/2000, Train Loss: 0.035682253539562225, Validation Loss: 0.08565156906843185\n",
      "Epoch: 1469/2000, Train Loss: 0.03566492348909378, Validation Loss: 0.08589623868465424\n",
      "Epoch: 1470/2000, Train Loss: 0.03565246984362602, Validation Loss: 0.08561278134584427\n",
      "Epoch: 1471/2000, Train Loss: 0.03564111143350601, Validation Loss: 0.08598292618989944\n",
      "Epoch: 1472/2000, Train Loss: 0.0356263592839241, Validation Loss: 0.08559694141149521\n",
      "Epoch: 1473/2000, Train Loss: 0.03560832515358925, Validation Loss: 0.08601875603199005\n",
      "Epoch: 1474/2000, Train Loss: 0.03558339178562164, Validation Loss: 0.08559007942676544\n",
      "Epoch: 1475/2000, Train Loss: 0.03555483743548393, Validation Loss: 0.08600809425115585\n",
      "Epoch: 1476/2000, Train Loss: 0.035519789904356, Validation Loss: 0.08558820188045502\n",
      "Epoch: 1477/2000, Train Loss: 0.035482678562402725, Validation Loss: 0.08596372604370117\n",
      "Epoch: 1478/2000, Train Loss: 0.035441599786281586, Validation Loss: 0.0855913907289505\n",
      "Epoch: 1479/2000, Train Loss: 0.035400453954935074, Validation Loss: 0.08590182662010193\n",
      "Epoch: 1480/2000, Train Loss: 0.0353582464158535, Validation Loss: 0.0856008306145668\n",
      "Epoch: 1481/2000, Train Loss: 0.03531741723418236, Validation Loss: 0.08583729714155197\n",
      "Epoch: 1482/2000, Train Loss: 0.03527745231986046, Validation Loss: 0.08561597764492035\n",
      "Epoch: 1483/2000, Train Loss: 0.035239383578300476, Validation Loss: 0.08577989041805267\n",
      "Epoch: 1484/2000, Train Loss: 0.03520284593105316, Validation Loss: 0.08563483506441116\n",
      "Epoch: 1485/2000, Train Loss: 0.035167984664440155, Validation Loss: 0.0857343077659607\n",
      "Epoch: 1486/2000, Train Loss: 0.03513446822762489, Validation Loss: 0.0856555625796318\n",
      "Epoch: 1487/2000, Train Loss: 0.03510211408138275, Validation Loss: 0.0857010930776596\n",
      "Epoch: 1488/2000, Train Loss: 0.03507065027952194, Validation Loss: 0.08567607402801514\n",
      "Epoch: 1489/2000, Train Loss: 0.03503987193107605, Validation Loss: 0.08567721396684647\n",
      "Epoch: 1490/2000, Train Loss: 0.03500958904623985, Validation Loss: 0.0856943354010582\n",
      "Epoch: 1491/2000, Train Loss: 0.0349796786904335, Validation Loss: 0.08565832674503326\n",
      "Epoch: 1492/2000, Train Loss: 0.034950047731399536, Validation Loss: 0.08570992946624756\n",
      "Epoch: 1493/2000, Train Loss: 0.034920647740364075, Validation Loss: 0.08564122021198273\n",
      "Epoch: 1494/2000, Train Loss: 0.03489149361848831, Validation Loss: 0.08572458475828171\n",
      "Epoch: 1495/2000, Train Loss: 0.03486260026693344, Validation Loss: 0.08562401682138443\n",
      "Epoch: 1496/2000, Train Loss: 0.034834057092666626, Validation Loss: 0.08574175089597702\n",
      "Epoch: 1497/2000, Train Loss: 0.03480594605207443, Validation Loss: 0.08560596406459808\n",
      "Epoch: 1498/2000, Train Loss: 0.03477851301431656, Validation Loss: 0.08576677739620209\n",
      "Epoch: 1499/2000, Train Loss: 0.034751906991004944, Validation Loss: 0.08558721840381622\n",
      "Epoch: 1500/2000, Train Loss: 0.034726690500974655, Validation Loss: 0.08580734580755234\n",
      "Epoch: 1501/2000, Train Loss: 0.03470313921570778, Validation Loss: 0.08556854724884033\n",
      "Epoch: 1502/2000, Train Loss: 0.03468251973390579, Validation Loss: 0.08587486296892166\n",
      "Epoch: 1503/2000, Train Loss: 0.03466536104679108, Validation Loss: 0.08555306494235992\n",
      "Epoch: 1504/2000, Train Loss: 0.03465457633137703, Validation Loss: 0.08599025011062622\n",
      "Epoch: 1505/2000, Train Loss: 0.03465103730559349, Validation Loss: 0.0855531394481659\n",
      "Epoch: 1506/2000, Train Loss: 0.034661732614040375, Validation Loss: 0.08619542419910431\n",
      "Epoch: 1507/2000, Train Loss: 0.03468743711709976, Validation Loss: 0.08560414612293243\n",
      "Epoch: 1508/2000, Train Loss: 0.034745216369628906, Validation Loss: 0.08657149970531464\n",
      "Epoch: 1509/2000, Train Loss: 0.03483191877603531, Validation Loss: 0.08578924834728241\n",
      "Epoch: 1510/2000, Train Loss: 0.03498870134353638, Validation Loss: 0.08725335448980331\n",
      "Epoch: 1511/2000, Train Loss: 0.03518860787153244, Validation Loss: 0.08625484257936478\n",
      "Epoch: 1512/2000, Train Loss: 0.035523321479558945, Validation Loss: 0.08835666626691818\n",
      "Epoch: 1513/2000, Train Loss: 0.03586333245038986, Validation Loss: 0.0870678722858429\n",
      "Epoch: 1514/2000, Train Loss: 0.036375366151332855, Validation Loss: 0.089536152780056\n",
      "Epoch: 1515/2000, Train Loss: 0.03663213178515434, Validation Loss: 0.08759339898824692\n",
      "Epoch: 1516/2000, Train Loss: 0.03686775267124176, Validation Loss: 0.08926431834697723\n",
      "Epoch: 1517/2000, Train Loss: 0.03637749329209328, Validation Loss: 0.08653007447719574\n",
      "Epoch: 1518/2000, Train Loss: 0.035642657428979874, Validation Loss: 0.0868065282702446\n",
      "Epoch: 1519/2000, Train Loss: 0.034672629088163376, Validation Loss: 0.08561652898788452\n",
      "Epoch: 1520/2000, Train Loss: 0.03415263444185257, Validation Loss: 0.08559944480657578\n",
      "Epoch: 1521/2000, Train Loss: 0.0342588908970356, Validation Loss: 0.0870598554611206\n",
      "Epoch: 1522/2000, Train Loss: 0.03471800684928894, Validation Loss: 0.08625443279743195\n",
      "Epoch: 1523/2000, Train Loss: 0.035125065594911575, Validation Loss: 0.0877440944314003\n",
      "Epoch: 1524/2000, Train Loss: 0.03507321700453758, Validation Loss: 0.08599782735109329\n",
      "Epoch: 1525/2000, Train Loss: 0.03471512719988823, Validation Loss: 0.08640297502279282\n",
      "Epoch: 1526/2000, Train Loss: 0.034204546362161636, Validation Loss: 0.0856630727648735\n",
      "Epoch: 1527/2000, Train Loss: 0.03392894193530083, Validation Loss: 0.08557607233524323\n",
      "Epoch: 1528/2000, Train Loss: 0.033990465104579926, Validation Loss: 0.08654335141181946\n",
      "Epoch: 1529/2000, Train Loss: 0.03422585874795914, Validation Loss: 0.08579898625612259\n",
      "Epoch: 1530/2000, Train Loss: 0.03439963981509209, Validation Loss: 0.086822509765625\n",
      "Epoch: 1531/2000, Train Loss: 0.034321870654821396, Validation Loss: 0.08566664904356003\n",
      "Epoch: 1532/2000, Train Loss: 0.03408760949969292, Validation Loss: 0.08600720763206482\n",
      "Epoch: 1533/2000, Train Loss: 0.033829882740974426, Validation Loss: 0.08571038395166397\n",
      "Epoch: 1534/2000, Train Loss: 0.033720605075359344, Validation Loss: 0.08557509630918503\n",
      "Epoch: 1535/2000, Train Loss: 0.0337752029299736, Validation Loss: 0.08628186583518982\n",
      "Epoch: 1536/2000, Train Loss: 0.03388215973973274, Validation Loss: 0.08564044535160065\n",
      "Epoch: 1537/2000, Train Loss: 0.0339253731071949, Validation Loss: 0.08632112294435501\n",
      "Epoch: 1538/2000, Train Loss: 0.03384062275290489, Validation Loss: 0.08558561652898788\n",
      "Epoch: 1539/2000, Train Loss: 0.03369511291384697, Validation Loss: 0.08581003546714783\n",
      "Epoch: 1540/2000, Train Loss: 0.033569108694791794, Validation Loss: 0.08572717010974884\n",
      "Epoch: 1541/2000, Train Loss: 0.03352569043636322, Validation Loss: 0.0855536088347435\n",
      "Epoch: 1542/2000, Train Loss: 0.033552564680576324, Validation Loss: 0.08606274425983429\n",
      "Epoch: 1543/2000, Train Loss: 0.033588264137506485, Validation Loss: 0.08556167781352997\n",
      "Epoch: 1544/2000, Train Loss: 0.03358276188373566, Validation Loss: 0.08604447543621063\n",
      "Epoch: 1545/2000, Train Loss: 0.03351784497499466, Validation Loss: 0.08557285368442535\n",
      "Epoch: 1546/2000, Train Loss: 0.03342990204691887, Validation Loss: 0.08573746681213379\n",
      "Epoch: 1547/2000, Train Loss: 0.03335823118686676, Validation Loss: 0.08571002632379532\n",
      "Epoch: 1548/2000, Train Loss: 0.03332706168293953, Validation Loss: 0.08556382358074188\n",
      "Epoch: 1549/2000, Train Loss: 0.03332722559571266, Validation Loss: 0.0859132632613182\n",
      "Epoch: 1550/2000, Train Loss: 0.033329978585243225, Validation Loss: 0.0855521559715271\n",
      "Epoch: 1551/2000, Train Loss: 0.03331286832690239, Validation Loss: 0.08590742200613022\n",
      "Epoch: 1552/2000, Train Loss: 0.03326834365725517, Validation Loss: 0.08557524532079697\n",
      "Epoch: 1553/2000, Train Loss: 0.0332115963101387, Validation Loss: 0.08571918308734894\n",
      "Epoch: 1554/2000, Train Loss: 0.033160433173179626, Validation Loss: 0.08566731214523315\n",
      "Epoch: 1555/2000, Train Loss: 0.033127158880233765, Validation Loss: 0.08558446168899536\n",
      "Epoch: 1556/2000, Train Loss: 0.03310998156666756, Validation Loss: 0.08580470830202103\n",
      "Epoch: 1557/2000, Train Loss: 0.033097557723522186, Validation Loss: 0.08556657284498215\n",
      "Epoch: 1558/2000, Train Loss: 0.03307873010635376, Validation Loss: 0.08584022521972656\n",
      "Epoch: 1559/2000, Train Loss: 0.033047646284103394, Validation Loss: 0.08559093624353409\n",
      "Epoch: 1560/2000, Train Loss: 0.03300802782177925, Validation Loss: 0.08574415743350983\n",
      "Epoch: 1561/2000, Train Loss: 0.03296685963869095, Validation Loss: 0.0856456533074379\n",
      "Epoch: 1562/2000, Train Loss: 0.032931435853242874, Validation Loss: 0.08563774824142456\n",
      "Epoch: 1563/2000, Train Loss: 0.03290403261780739, Validation Loss: 0.08573345094919205\n",
      "Epoch: 1564/2000, Train Loss: 0.032882124185562134, Validation Loss: 0.08559650927782059\n",
      "Epoch: 1565/2000, Train Loss: 0.03286094218492508, Validation Loss: 0.08578303456306458\n",
      "Epoch: 1566/2000, Train Loss: 0.0328361876308918, Validation Loss: 0.08559413999319077\n",
      "Epoch: 1567/2000, Train Loss: 0.032806649804115295, Validation Loss: 0.08574732393026352\n",
      "Epoch: 1568/2000, Train Loss: 0.032773490995168686, Validation Loss: 0.0856141448020935\n",
      "Epoch: 1569/2000, Train Loss: 0.03273986279964447, Validation Loss: 0.08567579835653305\n",
      "Epoch: 1570/2000, Train Loss: 0.03270827978849411, Validation Loss: 0.08566609770059586\n",
      "Epoch: 1571/2000, Train Loss: 0.03267986327409744, Validation Loss: 0.0856286808848381\n",
      "Epoch: 1572/2000, Train Loss: 0.03265403211116791, Validation Loss: 0.0857200175523758\n",
      "Epoch: 1573/2000, Train Loss: 0.03262912482023239, Validation Loss: 0.08560959994792938\n",
      "Epoch: 1574/2000, Train Loss: 0.03260346129536629, Validation Loss: 0.0857335552573204\n",
      "Epoch: 1575/2000, Train Loss: 0.03257601335644722, Validation Loss: 0.08560976386070251\n",
      "Epoch: 1576/2000, Train Loss: 0.032546911388635635, Validation Loss: 0.08570999652147293\n",
      "Epoch: 1577/2000, Train Loss: 0.032516829669475555, Validation Loss: 0.08563297986984253\n",
      "Epoch: 1578/2000, Train Loss: 0.032486818730831146, Validation Loss: 0.08567588776350021\n",
      "Epoch: 1579/2000, Train Loss: 0.032457660883665085, Validation Loss: 0.08566822856664658\n",
      "Epoch: 1580/2000, Train Loss: 0.03242965042591095, Validation Loss: 0.08564557880163193\n",
      "Epoch: 1581/2000, Train Loss: 0.03240257129073143, Validation Loss: 0.08569583296775818\n",
      "Epoch: 1582/2000, Train Loss: 0.03237592428922653, Validation Loss: 0.08562862128019333\n",
      "Epoch: 1583/2000, Train Loss: 0.03234916925430298, Validation Loss: 0.08570946007966995\n",
      "Epoch: 1584/2000, Train Loss: 0.03232192248106003, Validation Loss: 0.08563082665205002\n",
      "Epoch: 1585/2000, Train Loss: 0.03229409083724022, Validation Loss: 0.08570796996355057\n",
      "Epoch: 1586/2000, Train Loss: 0.03226577863097191, Validation Loss: 0.08564431965351105\n",
      "Epoch: 1587/2000, Train Loss: 0.032237257808446884, Validation Loss: 0.08569198101758957\n",
      "Epoch: 1588/2000, Train Loss: 0.032208800315856934, Validation Loss: 0.0856611356139183\n",
      "Epoch: 1589/2000, Train Loss: 0.03218061849474907, Validation Loss: 0.08567293733358383\n",
      "Epoch: 1590/2000, Train Loss: 0.03215278685092926, Validation Loss: 0.08568110316991806\n",
      "Epoch: 1591/2000, Train Loss: 0.032125264406204224, Validation Loss: 0.08566185086965561\n",
      "Epoch: 1592/2000, Train Loss: 0.0320979468524456, Validation Loss: 0.08569872379302979\n",
      "Epoch: 1593/2000, Train Loss: 0.032070696353912354, Validation Loss: 0.08565694838762283\n",
      "Epoch: 1594/2000, Train Loss: 0.032043393701314926, Validation Loss: 0.08570587635040283\n",
      "Epoch: 1595/2000, Train Loss: 0.03201596811413765, Validation Loss: 0.08565589785575867\n",
      "Epoch: 1596/2000, Train Loss: 0.03198839724063873, Validation Loss: 0.08570496737957001\n",
      "Epoch: 1597/2000, Train Loss: 0.03196069970726967, Validation Loss: 0.08566177636384964\n",
      "Epoch: 1598/2000, Train Loss: 0.031932931393384933, Validation Loss: 0.08570165932178497\n",
      "Epoch: 1599/2000, Train Loss: 0.03190514072775841, Validation Loss: 0.08567248284816742\n",
      "Epoch: 1600/2000, Train Loss: 0.03187738358974457, Validation Loss: 0.08569582551717758\n",
      "Epoch: 1601/2000, Train Loss: 0.0318496897816658, Validation Loss: 0.08568250387907028\n",
      "Epoch: 1602/2000, Train Loss: 0.031822070479393005, Validation Loss: 0.08568870276212692\n",
      "Epoch: 1603/2000, Train Loss: 0.03179453685879707, Validation Loss: 0.0856919065117836\n",
      "Epoch: 1604/2000, Train Loss: 0.031767070293426514, Validation Loss: 0.08568456023931503\n",
      "Epoch: 1605/2000, Train Loss: 0.031739652156829834, Validation Loss: 0.08570126444101334\n",
      "Epoch: 1606/2000, Train Loss: 0.03171226754784584, Validation Loss: 0.08568332344293594\n",
      "Epoch: 1607/2000, Train Loss: 0.03168489784002304, Validation Loss: 0.08570797741413116\n",
      "Epoch: 1608/2000, Train Loss: 0.03165752813220024, Validation Loss: 0.08568288385868073\n",
      "Epoch: 1609/2000, Train Loss: 0.03163015469908714, Validation Loss: 0.08571183681488037\n",
      "Epoch: 1610/2000, Train Loss: 0.03160277009010315, Validation Loss: 0.0856841430068016\n",
      "Epoch: 1611/2000, Train Loss: 0.031575385481119156, Validation Loss: 0.08571504801511765\n",
      "Epoch: 1612/2000, Train Loss: 0.03154798969626427, Validation Loss: 0.08568738400936127\n",
      "Epoch: 1613/2000, Train Loss: 0.03152059391140938, Validation Loss: 0.08571757376194\n",
      "Epoch: 1614/2000, Train Loss: 0.03149319812655449, Validation Loss: 0.08569084107875824\n",
      "Epoch: 1615/2000, Train Loss: 0.0314658097922802, Validation Loss: 0.08571920543909073\n",
      "Epoch: 1616/2000, Train Loss: 0.0314384289085865, Validation Loss: 0.08569447696208954\n",
      "Epoch: 1617/2000, Train Loss: 0.031411062926054, Validation Loss: 0.0857214406132698\n",
      "Epoch: 1618/2000, Train Loss: 0.031383708119392395, Validation Loss: 0.08569879829883575\n",
      "Epoch: 1619/2000, Train Loss: 0.031356360763311386, Validation Loss: 0.08572431653738022\n",
      "Epoch: 1620/2000, Train Loss: 0.03132903575897217, Validation Loss: 0.08570253103971481\n",
      "Epoch: 1621/2000, Train Loss: 0.031301725655794144, Validation Loss: 0.08572706580162048\n",
      "Epoch: 1622/2000, Train Loss: 0.031274426728487015, Validation Loss: 0.08570531755685806\n",
      "Epoch: 1623/2000, Train Loss: 0.031247150152921677, Validation Loss: 0.08573058992624283\n",
      "Epoch: 1624/2000, Train Loss: 0.03121989034116268, Validation Loss: 0.08570805191993713\n",
      "Epoch: 1625/2000, Train Loss: 0.03119264915585518, Validation Loss: 0.08573555201292038\n",
      "Epoch: 1626/2000, Train Loss: 0.031165430322289467, Validation Loss: 0.08571020513772964\n",
      "Epoch: 1627/2000, Train Loss: 0.031138239428400993, Validation Loss: 0.08574122935533524\n",
      "Epoch: 1628/2000, Train Loss: 0.03111107461154461, Validation Loss: 0.08571073412895203\n",
      "Epoch: 1629/2000, Train Loss: 0.031083950772881508, Validation Loss: 0.08574806153774261\n",
      "Epoch: 1630/2000, Train Loss: 0.031056871637701988, Validation Loss: 0.0857100710272789\n",
      "Epoch: 1631/2000, Train Loss: 0.031029853969812393, Validation Loss: 0.0857575535774231\n",
      "Epoch: 1632/2000, Train Loss: 0.031002918258309364, Validation Loss: 0.08570800721645355\n",
      "Epoch: 1633/2000, Train Loss: 0.030976099893450737, Validation Loss: 0.08577046543359756\n",
      "Epoch: 1634/2000, Train Loss: 0.030949445441365242, Validation Loss: 0.0857030600309372\n",
      "Epoch: 1635/2000, Train Loss: 0.03092302568256855, Validation Loss: 0.08578842133283615\n",
      "Epoch: 1636/2000, Train Loss: 0.030896946787834167, Validation Loss: 0.08569446206092834\n",
      "Epoch: 1637/2000, Train Loss: 0.03087138757109642, Validation Loss: 0.0858156681060791\n",
      "Epoch: 1638/2000, Train Loss: 0.030846571549773216, Validation Loss: 0.08568176627159119\n",
      "Epoch: 1639/2000, Train Loss: 0.03082297556102276, Validation Loss: 0.0858595222234726\n",
      "Epoch: 1640/2000, Train Loss: 0.030801108106970787, Validation Loss: 0.08566491305828094\n",
      "Epoch: 1641/2000, Train Loss: 0.030782168731093407, Validation Loss: 0.08593422174453735\n",
      "Epoch: 1642/2000, Train Loss: 0.030767375603318214, Validation Loss: 0.08564858883619308\n",
      "Epoch: 1643/2000, Train Loss: 0.03075995296239853, Validation Loss: 0.08607122302055359\n",
      "Epoch: 1644/2000, Train Loss: 0.030762676149606705, Validation Loss: 0.08565328270196915\n",
      "Epoch: 1645/2000, Train Loss: 0.030784498900175095, Validation Loss: 0.08634070307016373\n",
      "Epoch: 1646/2000, Train Loss: 0.03083108738064766, Validation Loss: 0.08574382215738297\n",
      "Epoch: 1647/2000, Train Loss: 0.03092784248292446, Validation Loss: 0.08689874410629272\n",
      "Epoch: 1648/2000, Train Loss: 0.03108164109289646, Validation Loss: 0.08609864115715027\n",
      "Epoch: 1649/2000, Train Loss: 0.03136371076107025, Validation Loss: 0.08805685490369797\n",
      "Epoch: 1650/2000, Train Loss: 0.03175073862075806, Validation Loss: 0.08709161728620529\n",
      "Epoch: 1651/2000, Train Loss: 0.03242355212569237, Validation Loss: 0.09016270935535431\n",
      "Epoch: 1652/2000, Train Loss: 0.03313712030649185, Validation Loss: 0.08892548084259033\n",
      "Epoch: 1653/2000, Train Loss: 0.034229375422000885, Validation Loss: 0.0923045203089714\n",
      "Epoch: 1654/2000, Train Loss: 0.034627918154001236, Validation Loss: 0.08952219784259796\n",
      "Epoch: 1655/2000, Train Loss: 0.0347541943192482, Validation Loss: 0.09028174728155136\n",
      "Epoch: 1656/2000, Train Loss: 0.03312147781252861, Validation Loss: 0.08622817695140839\n",
      "Epoch: 1657/2000, Train Loss: 0.03128759190440178, Validation Loss: 0.08584039658308029\n",
      "Epoch: 1658/2000, Train Loss: 0.03035278618335724, Validation Loss: 0.08725923299789429\n",
      "Epoch: 1659/2000, Train Loss: 0.031004415825009346, Validation Loss: 0.08709936589002609\n",
      "Epoch: 1660/2000, Train Loss: 0.03215726092457771, Validation Loss: 0.08931828290224075\n",
      "Epoch: 1661/2000, Train Loss: 0.03224338963627815, Validation Loss: 0.08659423142671585\n",
      "Epoch: 1662/2000, Train Loss: 0.031425051391124725, Validation Loss: 0.08650094270706177\n",
      "Epoch: 1663/2000, Train Loss: 0.03038492053747177, Validation Loss: 0.08630244433879852\n",
      "Epoch: 1664/2000, Train Loss: 0.030272362753748894, Validation Loss: 0.08620396256446838\n",
      "Epoch: 1665/2000, Train Loss: 0.03091573156416416, Validation Loss: 0.08809742331504822\n",
      "Epoch: 1666/2000, Train Loss: 0.0312890000641346, Validation Loss: 0.0862850546836853\n",
      "Epoch: 1667/2000, Train Loss: 0.03101460449397564, Validation Loss: 0.08658161759376526\n",
      "Epoch: 1668/2000, Train Loss: 0.030329346656799316, Validation Loss: 0.08593752235174179\n",
      "Epoch: 1669/2000, Train Loss: 0.030041340738534927, Validation Loss: 0.08586204051971436\n",
      "Epoch: 1670/2000, Train Loss: 0.030308807268738747, Validation Loss: 0.08724869042634964\n",
      "Epoch: 1671/2000, Train Loss: 0.03062460571527481, Validation Loss: 0.08606336265802383\n",
      "Epoch: 1672/2000, Train Loss: 0.03057660534977913, Validation Loss: 0.08654565364122391\n",
      "Epoch: 1673/2000, Train Loss: 0.030177082866430283, Validation Loss: 0.08586055040359497\n",
      "Epoch: 1674/2000, Train Loss: 0.029914477840065956, Validation Loss: 0.0858035683631897\n",
      "Epoch: 1675/2000, Train Loss: 0.030000101774930954, Validation Loss: 0.08675383776426315\n",
      "Epoch: 1676/2000, Train Loss: 0.03020256944000721, Validation Loss: 0.08592573553323746\n",
      "Epoch: 1677/2000, Train Loss: 0.03021889179944992, Validation Loss: 0.0864376649260521\n",
      "Epoch: 1678/2000, Train Loss: 0.029994485899806023, Validation Loss: 0.08581873029470444\n",
      "Epoch: 1679/2000, Train Loss: 0.029797442257404327, Validation Loss: 0.0857858657836914\n",
      "Epoch: 1680/2000, Train Loss: 0.02979816496372223, Validation Loss: 0.08640898019075394\n",
      "Epoch: 1681/2000, Train Loss: 0.02991003356873989, Validation Loss: 0.08582291007041931\n",
      "Epoch: 1682/2000, Train Loss: 0.02994139865040779, Validation Loss: 0.08633587509393692\n",
      "Epoch: 1683/2000, Train Loss: 0.02982022985816002, Validation Loss: 0.08581650257110596\n",
      "Epoch: 1684/2000, Train Loss: 0.02967974543571472, Validation Loss: 0.08583156764507294\n",
      "Epoch: 1685/2000, Train Loss: 0.029642054811120033, Validation Loss: 0.08620039373636246\n",
      "Epoch: 1686/2000, Train Loss: 0.029692234471440315, Validation Loss: 0.08578059077262878\n",
      "Epoch: 1687/2000, Train Loss: 0.029720868915319443, Validation Loss: 0.08624523878097534\n",
      "Epoch: 1688/2000, Train Loss: 0.02966020070016384, Validation Loss: 0.08580753207206726\n",
      "Epoch: 1689/2000, Train Loss: 0.029562756419181824, Validation Loss: 0.0858951136469841\n",
      "Epoch: 1690/2000, Train Loss: 0.029507486149668694, Validation Loss: 0.0860745906829834\n",
      "Epoch: 1691/2000, Train Loss: 0.029514437541365623, Validation Loss: 0.08579475432634354\n",
      "Epoch: 1692/2000, Train Loss: 0.02953174151480198, Validation Loss: 0.0861799344420433\n",
      "Epoch: 1693/2000, Train Loss: 0.02950531244277954, Validation Loss: 0.08579883724451065\n",
      "Epoch: 1694/2000, Train Loss: 0.029442347586154938, Validation Loss: 0.08593595027923584\n",
      "Epoch: 1695/2000, Train Loss: 0.029385684058070183, Validation Loss: 0.08597151935100555\n",
      "Epoch: 1696/2000, Train Loss: 0.029365122318267822, Validation Loss: 0.08582013100385666\n",
      "Epoch: 1697/2000, Train Loss: 0.029366206377744675, Validation Loss: 0.08613099157810211\n",
      "Epoch: 1698/2000, Train Loss: 0.02935427613556385, Validation Loss: 0.0858350619673729\n",
      "Epoch: 1699/2000, Train Loss: 0.02931646630167961, Validation Loss: 0.08601099252700806\n",
      "Epoch: 1700/2000, Train Loss: 0.02926838956773281, Validation Loss: 0.08593837171792984\n",
      "Epoch: 1701/2000, Train Loss: 0.02923446148633957, Validation Loss: 0.0858762115240097\n",
      "Epoch: 1702/2000, Train Loss: 0.029219526797533035, Validation Loss: 0.08607956767082214\n",
      "Epoch: 1703/2000, Train Loss: 0.029208319261670113, Validation Loss: 0.08585987985134125\n",
      "Epoch: 1704/2000, Train Loss: 0.02918536402285099, Validation Loss: 0.0860479325056076\n",
      "Epoch: 1705/2000, Train Loss: 0.029149556532502174, Validation Loss: 0.08591250330209732\n",
      "Epoch: 1706/2000, Train Loss: 0.029113413766026497, Validation Loss: 0.08592468500137329\n",
      "Epoch: 1707/2000, Train Loss: 0.02908710017800331, Validation Loss: 0.08601433038711548\n",
      "Epoch: 1708/2000, Train Loss: 0.02906942553818226, Validation Loss: 0.08587353676557541\n",
      "Epoch: 1709/2000, Train Loss: 0.02905123680830002, Validation Loss: 0.08604322373867035\n",
      "Epoch: 1710/2000, Train Loss: 0.029025636613368988, Validation Loss: 0.08589757233858109\n",
      "Epoch: 1711/2000, Train Loss: 0.028994446620345116, Validation Loss: 0.08597204834222794\n",
      "Epoch: 1712/2000, Train Loss: 0.028964217752218246, Validation Loss: 0.08597173541784286\n",
      "Epoch: 1713/2000, Train Loss: 0.028939511626958847, Validation Loss: 0.0859101265668869\n",
      "Epoch: 1714/2000, Train Loss: 0.0289189163595438, Validation Loss: 0.08602920919656754\n",
      "Epoch: 1715/2000, Train Loss: 0.028897587209939957, Validation Loss: 0.08590657263994217\n",
      "Epoch: 1716/2000, Train Loss: 0.028872404247522354, Validation Loss: 0.08601097762584686\n",
      "Epoch: 1717/2000, Train Loss: 0.028844285756349564, Validation Loss: 0.0859522596001625\n",
      "Epoch: 1718/2000, Train Loss: 0.0288167092949152, Validation Loss: 0.08595993369817734\n",
      "Epoch: 1719/2000, Train Loss: 0.028791986405849457, Validation Loss: 0.0860116183757782\n",
      "Epoch: 1720/2000, Train Loss: 0.028769509866833687, Validation Loss: 0.08593533933162689\n",
      "Epoch: 1721/2000, Train Loss: 0.028746899217367172, Validation Loss: 0.08603091537952423\n",
      "Epoch: 1722/2000, Train Loss: 0.028722364455461502, Validation Loss: 0.08595218509435654\n",
      "Epoch: 1723/2000, Train Loss: 0.028696175664663315, Validation Loss: 0.08600665628910065\n",
      "Epoch: 1724/2000, Train Loss: 0.0286699328571558, Validation Loss: 0.08599866181612015\n",
      "Epoch: 1725/2000, Train Loss: 0.02864501066505909, Validation Loss: 0.08597946912050247\n",
      "Epoch: 1726/2000, Train Loss: 0.028621423989534378, Validation Loss: 0.08603763580322266\n",
      "Epoch: 1727/2000, Train Loss: 0.02859812043607235, Validation Loss: 0.08597441762685776\n",
      "Epoch: 1728/2000, Train Loss: 0.028574060648679733, Validation Loss: 0.08604057133197784\n",
      "Epoch: 1729/2000, Train Loss: 0.02854902297258377, Validation Loss: 0.08599531650543213\n",
      "Epoch: 1730/2000, Train Loss: 0.028523623943328857, Validation Loss: 0.08602069318294525\n",
      "Epoch: 1731/2000, Train Loss: 0.028498657047748566, Validation Loss: 0.08603014051914215\n",
      "Epoch: 1732/2000, Train Loss: 0.028474435210227966, Validation Loss: 0.08600559830665588\n",
      "Epoch: 1733/2000, Train Loss: 0.028450656682252884, Validation Loss: 0.08605356514453888\n",
      "Epoch: 1734/2000, Train Loss: 0.028426751494407654, Validation Loss: 0.08600804954767227\n",
      "Epoch: 1735/2000, Train Loss: 0.02840237319469452, Validation Loss: 0.0860537737607956\n",
      "Epoch: 1736/2000, Train Loss: 0.028377611190080643, Validation Loss: 0.08602859824895859\n",
      "Epoch: 1737/2000, Train Loss: 0.028352828696370125, Validation Loss: 0.08604320138692856\n",
      "Epoch: 1738/2000, Train Loss: 0.028328340500593185, Validation Loss: 0.08605635911226273\n",
      "Epoch: 1739/2000, Train Loss: 0.028304193168878555, Validation Loss: 0.08603759855031967\n",
      "Epoch: 1740/2000, Train Loss: 0.02828020416200161, Validation Loss: 0.0860743299126625\n",
      "Epoch: 1741/2000, Train Loss: 0.028256112709641457, Validation Loss: 0.08604341000318527\n",
      "Epoch: 1742/2000, Train Loss: 0.028231799602508545, Validation Loss: 0.08607668429613113\n",
      "Epoch: 1743/2000, Train Loss: 0.028207330033183098, Validation Loss: 0.08606036752462387\n",
      "Epoch: 1744/2000, Train Loss: 0.02818286418914795, Validation Loss: 0.08607196062803268\n",
      "Epoch: 1745/2000, Train Loss: 0.028158538043498993, Validation Loss: 0.08608134835958481\n",
      "Epoch: 1746/2000, Train Loss: 0.028134381398558617, Validation Loss: 0.08606994897127151\n",
      "Epoch: 1747/2000, Train Loss: 0.028110302984714508, Validation Loss: 0.08609608560800552\n",
      "Epoch: 1748/2000, Train Loss: 0.02808620035648346, Validation Loss: 0.08607526868581772\n",
      "Epoch: 1749/2000, Train Loss: 0.0280620064586401, Validation Loss: 0.08610108494758606\n",
      "Epoch: 1750/2000, Train Loss: 0.02803773246705532, Validation Loss: 0.08608847856521606\n",
      "Epoch: 1751/2000, Train Loss: 0.0280134454369545, Validation Loss: 0.08610112965106964\n",
      "Epoch: 1752/2000, Train Loss: 0.027989210560917854, Validation Loss: 0.08610551804304123\n",
      "Epoch: 1753/2000, Train Loss: 0.027965055778622627, Validation Loss: 0.08610228449106216\n",
      "Epoch: 1754/2000, Train Loss: 0.027940960600972176, Validation Loss: 0.08612015098333359\n",
      "Epoch: 1755/2000, Train Loss: 0.027916880324482918, Validation Loss: 0.08610820770263672\n",
      "Epoch: 1756/2000, Train Loss: 0.02789277769625187, Validation Loss: 0.08612941205501556\n",
      "Epoch: 1757/2000, Train Loss: 0.027868637815117836, Validation Loss: 0.08611979335546494\n",
      "Epoch: 1758/2000, Train Loss: 0.027844475582242012, Validation Loss: 0.08613467216491699\n",
      "Epoch: 1759/2000, Train Loss: 0.027820320799946785, Validation Loss: 0.08613468706607819\n",
      "Epoch: 1760/2000, Train Loss: 0.027796197682619095, Validation Loss: 0.08613879978656769\n",
      "\n",
      "Early stopping at epoch 1760 due to validation loss increasing for 3 consecutive epochs.\n",
      "\n",
      "Accuracy: 0.9735\n",
      "\n",
      "Class 0 - Precision: 0.9776, Recall: 0.9695, F1 Score: 0.9735\n",
      "Class 1 - Precision: 0.9693, Recall: 0.9775, F1 Score: 0.9734\n",
      "\n",
      "Confusion Matrix:\n",
      "[[3145   99]\n",
      " [  72 3127]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "optimizer_bi_lstm = optim.Adam(bi_lstm.parameters(), lr=0.001)\n",
    "bi_lstm, bi_lstm_train_losses, bi_lstm_val_losses = train_model(bi_lstm, optimizer_bi_lstm, criterion, X_train, y_train, X_val, y_val)\n",
    "\n",
    "accuracy, report, conf_matrix = evaluate_model(bi_lstm, X_test, y_test)\n",
    "print_evaluation_metrics(accuracy, report, conf_matrix, bi_lstm_train_losses, bi_lstm_val_losses, \"Bi-LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1b9ab1148babcb51",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-16T02:37:40.978950400Z",
     "start_time": "2023-08-16T02:37:40.935631500Z"
    }
   },
   "outputs": [],
   "source": [
    "# Saving the weights of the best model locally\n",
    "base_dir = \"Models\"\n",
    "sub_dir = \"Torches\"\n",
    "save_path = os.path.join(base_dir, sub_dir, 'bi_lstm_weights_1.pth')\n",
    "\n",
    "torch.save(bi_lstm.state_dict(), save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
